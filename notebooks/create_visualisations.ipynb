{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory found: c:\\Users\\todor\\Repositories\\ir-project/results/cleaned\n"
     ]
    }
   ],
   "source": [
    "# Get the directory containing the notebook\n",
    "notebook_dir = Path(os.getcwd())\n",
    "# Get the parent directory (project root)\n",
    "project_root = str(notebook_dir.parent)\n",
    "# Add to Python path if not already there\n",
    "\n",
    "results_path = project_root + \"/results/cleaned\"\n",
    "\n",
    "if not os.path.exists(results_path):\n",
    "    print(f\"Directory not found: {results_path}\")\n",
    "else:\n",
    "    print(f\"Directory found: {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [\"pinecone\", \"openai\", \"hybrid\", \"auto_merge\"]\n",
    "datasets = [\"nq\", \"hotpotqa\", \"sse_single\", \"sse_multi\"]\n",
    "metrics_columns = [\"context_precision\", \"context_recall\", \"faithfulness\", \"factual_correctness(mode=f1)\", \"context_entity_recall\", \"answer_relevancy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_tables = {dataset: {} for dataset in datasets}\n",
    "for dataset in datasets:\n",
    "    for agent in agents:\n",
    "        path = f\"../results/cleaned/{dataset}/{agent}/evaluation_results.csv\"\n",
    "        if os.path.exists(path):\n",
    "            metrics_tables[dataset][agent] = pd.read_csv(path)[metrics_columns]\n",
    "        else:\n",
    "            print(f\"File {path} does not exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pinecone</th>\n",
       "      <th>openai</th>\n",
       "      <th>hybrid</th>\n",
       "      <th>auto_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>context_precision</th>\n",
       "      <td>0.621667</td>\n",
       "      <td>0.705000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.726667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context_recall</th>\n",
       "      <td>0.703333</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faithfulness</th>\n",
       "      <td>0.724726</td>\n",
       "      <td>0.849737</td>\n",
       "      <td>0.690213</td>\n",
       "      <td>0.854840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "      <td>0.148400</td>\n",
       "      <td>0.096800</td>\n",
       "      <td>0.175200</td>\n",
       "      <td>0.101600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context_entity_recall</th>\n",
       "      <td>0.485417</td>\n",
       "      <td>0.545977</td>\n",
       "      <td>0.464815</td>\n",
       "      <td>0.582979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_relevancy</th>\n",
       "      <td>0.783246</td>\n",
       "      <td>0.848667</td>\n",
       "      <td>0.786651</td>\n",
       "      <td>0.834428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              pinecone    openai    hybrid  auto_merge\n",
       "context_precision             0.621667  0.705000  0.620000    0.726667\n",
       "context_recall                0.703333  0.820000  0.600000    0.800000\n",
       "faithfulness                  0.724726  0.849737  0.690213    0.854840\n",
       "factual_correctness(mode=f1)  0.148400  0.096800  0.175200    0.101600\n",
       "context_entity_recall         0.485417  0.545977  0.464815    0.582979\n",
       "answer_relevancy              0.783246  0.848667  0.786651    0.834428"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get averages of metrics\n",
    "metrics_averages = {}\n",
    "for dataset in datasets:\n",
    "    dataset_metrics = {}\n",
    "    for agent in agents:\n",
    "        dataset_metrics[agent] = metrics_tables[dataset][agent].mean()\n",
    "    metrics_averages[dataset] = pd.DataFrame(dataset_metrics)\n",
    "    \n",
    "metrics_averages[\"nq\"]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for dataset in datasets:\n",
    "    save_path = results_path + f\"/{dataset}/metrics_averages.csv\"\n",
    "    metrics_averages[dataset].to_csv(save_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate heatmap\n",
    "for dataset in datasets:\n",
    "    save_path = results_path + f\"/{dataset}/metrics_averages.png\"\n",
    "    \n",
    "    plt.figure(figsize=(7, 4))\n",
    "    sns.heatmap(metrics_averages[dataset], annot=True, cmap=\"Blues\")\n",
    "    plt.title(f\"Evaluation Averages for {dataset.upper()}\")\n",
    "    plt.xlabel(\"Agents\")\n",
    "    plt.ylabel(\"Metrics\")\n",
    "    plt.tight_layout()\n",
    "  \n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
