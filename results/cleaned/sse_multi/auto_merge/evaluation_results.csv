user_input,retrieved_contexts,reference_contexts,response,reference,context_precision,context_recall,faithfulness,factual_correctness(mode=f1),context_entity_recall,answer_relevancy
How EcoAndroid reduce risk by reduce size?,"['‚Ä¢ When a connection is not possible, the app automatically retries later. \n‚Ä¢ https://github.com/k9mail/k-9/commit/\n86f3b28f79509d1a4d613eb39f60603e08579ea3\n\nEcoAndroid\n‚Ä¢ Plugin for IntelliJ (Android Studio) \n‚Ä¢ Dynamic Retry Delay \n‚Ä¢ Push Over Poll \n‚Ä¢ Reduce Size \n‚Ä¢ Cache \n‚Ä¢ Avoid Graphics and Animations\n20\n\nCarbon-Aware Computing for \nDatacenters \nhttps://sites.google.com/view/energy-efficiency-languages\n21\nAna Radovanovic ÃÅ,', '*/\n6028 \nchanges\n\nThematic Analysis\n1. Familiarization with data \n2. Generating initial labels \n3. Reviewing themes\n4. Defining and naming themes\n\n‚Ä¢ Energy Pattern: design pattern to improve energy efficiency.. \n‚Ä¢ 22 energy patterns. \n‚Ä¢ Each pattern is described by Context, Solution, Example, References from \nliterature, and Occurences (links to code changes from git repositories).\n8\n\nhttps://tqrg.github.io/energy-patterns\n\nDark UI Colors \nProvide a dark UI color theme to \nsave battery on devices with \nAMOLED screens.\n‚Ä¢ Context: [‚Ä¶] Apps that require heavy usage of screen can \nhave a substantial negative impact on battery life. \n‚Ä¢ Solution: Provide a UI theme with dark background colors. [‚Ä¶]\n‚Ä¢ Example: In a reading app, provide a theme with a dark \nbackground using light colors to display text. [‚Ä¶] \n10\n\nDark UI Colors \nProvide a dark UI color theme to \nsave battery on devices with \nAMOLED screens.\n‚Ä¢ Context: [‚Ä¶] Apps that require heavy usage of screen can \nhave a substantial negative impact on battery life. \n‚Ä¢ Solution: Provide a UI theme with dark background colors. [‚Ä¶]\n‚Ä¢ Example: In a reading app, provide a theme with a dark \nbackground using light colors to display text. [‚Ä¶] \n11\n\nDynamic Retry Delay\n‚Ä¢ Context: [‚Ä¶] In a mobile app, when a given resource is \nunavailable, the app will unnecessarily try to connect the resource \nfor a number of times, leading to unnecessary power consumption. \n‚Ä¢ Solution: Increase retry interval after each failed connection. [‚Ä¶]\n‚Ä¢ Example: Consider a mobile app that provides a news feed and \nthe app is not able to reach the server to collect updates. [‚Ä¶] use \nthe Fibonacci series to increase the time between attempts.\nWhenever an attempt to access a \nresource fails, increase the time \ninterval before retrying.\n12\n\nBatch Operations \n‚Ä¢ Context: Executing operations separately leads to extraneous \ntail energy consumptions  \n‚Ä¢ Solution: Bundle multiple operations in a single one. [‚Ä¶]\n‚Ä¢ Example: Use system provided APIs to schedule background \ntasks. These APIs, guarantee that device will exit sleep mode \nonly when there is a reasonable amount of work to do or when a \ngiven task is urgent. [‚Ä¶]\n13\n\nAvoid Extraneous Graphics and \nAnimations  \n‚Ä¢ Context: Mobile apps that feature impressive graphics \nand animations.', 'Green Architectural Tactics for ML-Enabled \nSystem\n38\nICSE-SEIS 2024\n\nArchitectural tactics\n39\n\nData-centric\n40\n‚Ä¢ Reduce data size \n‚Ä¢ Sampling \n‚Ä¢ Dimensionality reduction \n‚Ä¢ Quantization\n\nAlgorithm Design\n41\n‚Ä¢ Carefully select your algorithm \n‚Ä¢ You don‚Äôt need the fanciest \ntechniques\n\nModel Optimization\n42\n‚Ä¢ Add energy to training parameters \n‚Ä¢ Reduce FLOPs \n‚Ä¢ Pruning,']","['<1-hop>\n\nShuffle \nüîÄ\n‚Ä¢ It is not a mystery that energy consumption depends on so many factors that \nit is impossible to control all of them.\n‚Ä¢ If you run 30 executions for version A and another batch for version B:\n‚Ä¢ External conditions that change over time will have a diÔ¨Äerent bias in \nthe 2 versions (e.g., room temperature changes).\n‚Ä¢ If you shuÔ¨Ñe, you reduce this risk.\n11', '<2-hop>\n\nEcoAndroid\n‚Ä¢ Plugin for IntelliJ (Android Studio)\n‚Ä¢ Dynamic Retry Delay\n‚Ä¢ Push Over Poll\n‚Ä¢ Reduce Size\n‚Ä¢ Cache\n‚Ä¢ Avoid Graphics and Animations\n20', '<1-hop>\n\nShuffle \nüîÄ\n‚Ä¢ It is not a mystery that energy consumption depends on so many factors that \nit is impossible to control all of them.\n‚Ä¢ If you run 30 executions for version A and another batch for version B:\n‚Ä¢ External conditions that change over time will have a diÔ¨Äerent bias in \nthe 2 versions (e.g., room temperature changes).\n‚Ä¢ If you shuÔ¨Ñe, you reduce this risk.\n11', '<2-hop>\n\nEcoAndroid\n‚Ä¢ Plugin for IntelliJ (Android Studio)\n‚Ä¢ Dynamic Retry Delay\n‚Ä¢ Push Over Poll\n‚Ä¢ Reduce Size\n‚Ä¢ Cache\n‚Ä¢ Avoid Graphics and Animations\n20']","EcoAndroid reduces risk by reducing size through techniques such as caching and avoiding graphics and animations. By minimizing the size of data and resources used, the app can operate more efficiently, which can lead to lower energy consumption and improved performance. This approach helps in mitigating risks associated with excessive resource usage and potential performance bottlenecks.",EcoAndroid reduce risk by using Reduce Size feature which help minimize energy consumption and reduce impact of external factors like temperature changes.,0.0,0.5,0.1428571428571428,0.67,0.249999999375,0.9484229489049844
How can static code analysis in data science be used to improve the energy efficiency of data centers and contribute to sustainable software engineering?,"['Energy profiling of static analysis tools\n‚Ä¢ What is the energy consumption of a ""typical run"" for a few OSS projects? \n‚Ä¢ Does the type of analysis matter? Are certain analysis more expensive? Does \nthe number of rules that are activated in a static analysis tool important for the \nenergy consumption? \n‚Ä¢ Differences between static analysis tools [lower priority]\n\nB5.', '‚Ä¢ => Improve data collection, data labelling, and data preprocessing. \n‚Ä¢ More about data-centric AI by Andrew Ng:\u2028\nhttps://www.youtube.com/watch?v=06-AZXmwHjo\n15\n\n\nGreen Data-centric AI\n‚Ä¢ How do different ML algorithms compare \nin terms of energy consumption? \n‚Ä¢ How does number of rows relate to the \nenergy consumption of ML models? \n‚Ä¢ How does number of features relate to \nthe energy consumption of ML models? \n‚Ä¢ What is the impact of reducing data in the \nperformance of the model?', '19\n(?)\n\nResults: energy consumption of algorithms\n\nResults: energy vs data shape\n\nResults: performance vs data shape\n\nDiscussion\n‚Ä¢ Other data properties should be investigated. \n‚Ä¢ E.g., data types \n‚Ä¢ Reporting energy data is essential. It can lead to different model selection \nwithout hindering model performance. \n‚Ä¢ There is a big opportunity in Model and Data Simplification.\n23\n\nData/Model Simplification\n‚Ä¢ (?) \n‚Ä¢ Data selection \n‚Ä¢ Data quantisation. Posit? \n‚Ä¢ Data distillation \n‚Ä¢ Coreset extraction (?)']","['<1-hop>\n\nC6 - Green Shift Left\n‚Ä¢ Estimate energy eÔ¨Éciency using static code analysis.\n‚Ä¢ We don‚Äôt need an accurate value.\n‚Ä¢ It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention.\n‚Ä¢ Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)\n27', '<2-hop>\n\nWhy?\n‚Ä¢ Throughout your career you might:\n‚Ä¢ Design/maintain/contract data centers\n‚Ä¢ Set up operations/devops\n‚Ä¢ Develop AI for IoT devices\n‚Ä¢ Be the next CEO/CTO of a software company\n‚Ä¢ Sustainability can be your main role:\n‚Ä¢ Green Software Developer\n‚Ä¢ Sustainability Consultant\n‚Ä¢ Green Advocate \n‚Ä¢ Founder of a Green Tech startup (B2B?)\n28', '<1-hop>\n\nC6 - Green Shift Left\n‚Ä¢ Estimate energy eÔ¨Éciency using static code analysis.\n‚Ä¢ We don‚Äôt need an accurate value.\n‚Ä¢ It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention.\n‚Ä¢ Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)\n27', '<2-hop>\n\nWhy?\n‚Ä¢ Throughout your career you might:\n‚Ä¢ Design/maintain/contract data centers\n‚Ä¢ Set up operations/devops\n‚Ä¢ Develop AI for IoT devices\n‚Ä¢ Be the next CEO/CTO of a software company\n‚Ä¢ Sustainability can be your main role:\n‚Ä¢ Green Software Developer\n‚Ä¢ Sustainability Consultant\n‚Ä¢ Green Advocate \n‚Ä¢ Founder of a Green Tech startup (B2B?)\n28', '<1-hop>\n\nC6 - Green Shift Left\n‚Ä¢ Estimate energy eÔ¨Éciency using static code analysis.\n‚Ä¢ We don‚Äôt need an accurate value.\n‚Ä¢ It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention.\n‚Ä¢ Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)\n27', '<2-hop>\n\nWhy?\n‚Ä¢ Throughout your career you might:\n‚Ä¢ Design/maintain/contract data centers\n‚Ä¢ Set up operations/devops\n‚Ä¢ Develop AI for IoT devices\n‚Ä¢ Be the next CEO/CTO of a software company\n‚Ä¢ Sustainability can be your main role:\n‚Ä¢ Green Software Developer\n‚Ä¢ Sustainability Consultant\n‚Ä¢ Green Advocate \n‚Ä¢ Founder of a Green Tech startup (B2B?)\n28', '<1-hop>\n\nC6 - Green Shift Left\n‚Ä¢ Estimate energy eÔ¨Éciency using static code analysis.\n‚Ä¢ We don‚Äôt need an accurate value.\n‚Ä¢ It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention.\n‚Ä¢ Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)\n27', '<2-hop>\n\nWhy?\n‚Ä¢ Throughout your career you might:\n‚Ä¢ Design/maintain/contract data centers\n‚Ä¢ Set up operations/devops\n‚Ä¢ Develop AI for IoT devices\n‚Ä¢ Be the next CEO/CTO of a software company\n‚Ä¢ Sustainability can be your main role:\n‚Ä¢ Green Software Developer\n‚Ä¢ Sustainability Consultant\n‚Ä¢ Green Advocate \n‚Ä¢ Founder of a Green Tech startup (B2B?)\n28']","Static code analysis in data science can improve the energy efficiency of data centers and contribute to sustainable software engineering in several ways:

1. **Optimizing Code for Efficiency**: Static analysis tools can identify inefficient code patterns that consume more energy. By optimizing these patterns, developers can reduce the energy consumption of software applications.

2. **Reducing Redundant Computations**: Static analysis can detect redundant computations or unnecessary data processing steps, allowing developers to streamline code execution and reduce energy usage.

3. **Improving Resource Management**: By analyzing code, static analysis tools can suggest better resource management practices, such as efficient memory usage and optimal data storage solutions, which can lead to lower energy consumption.

4. **Identifying Energy-Intensive Operations**: Static analysis can highlight operations that are particularly energy-intensive, enabling developers to refactor or replace them with more energy-efficient alternatives.

5. **Guiding Data-Centric Improvements**: As mentioned in the sources, improving data collection, labeling, and preprocessing can contribute to energy efficiency. Static analysis can help ensure that data handling in software is optimized for minimal energy use.

By incorporating static code analysis into the software development lifecycle, data centers can achieve more energy-efficient operations, aligning with the goals of sustainable software engineering.","Static code analysis in data science can be used to estimate energy efficiency by identifying potential energy hotspots in the code. This helps in reviewing and optimizing the code to reduce energy consumption. By applying this approach, developers can contribute to the design and maintenance of more energy-efficient data centers, aligning with sustainable software engineering practices.",0.0,1.0,1.0,0.67,0.0,0.996656445430904
How can static code analysis in data science contribute to the development of more sustainable data centers?,"['Debugging Energy with \nDocker Images\n\nUnveiling the Energy Vampires \nA methodology for debugging Software \nEnergy Consumption\n ‚úÖ ICSE 2025\n\nData Centers and Docker\n‚Ä¢ Containers are the most popular way to deploy apps into the cloud \n‚Ä¢ Base image is an important choice when building an image \n‚Ä¢ Criteria \n‚Ä¢ Linux distribution and binaries \n‚Ä¢ Image size \n‚Ä¢ Energy?', '‚Ä¢ Libraries for stats? ML? Web Dev? Cloud?\n\nC5. Add energy-awareness to existing software\n26\n‚Ä¢ Streamlit, Notebooks, JSFiddle, \nPyScript, zsh, etc. \n‚Ä¢\n\nC6 - Green Shift Left\n‚Ä¢ Estimate energy efficiency using static code analysis. \n‚Ä¢ We don‚Äôt need an accurate value. \n‚Ä¢ It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention.', '19\n(?)\n\nResults: energy consumption of algorithms\n\nResults: energy vs data shape\n\nResults: performance vs data shape\n\nDiscussion\n‚Ä¢ Other data properties should be investigated. \n‚Ä¢ E.g., data types \n‚Ä¢ Reporting energy data is essential. It can lead to different model selection \nwithout hindering model performance. \n‚Ä¢ There is a big opportunity in Model and Data Simplification.\n23\n\nData/Model Simplification\n‚Ä¢ (?) \n‚Ä¢ Data selection \n‚Ä¢ Data quantisation. Posit? \n‚Ä¢ Data distillation \n‚Ä¢ Coreset extraction (?)']","['<1-hop>\n\nC6 - Green Shift Left\n‚Ä¢ Estimate energy eÔ¨Éciency using static code analysis.\n‚Ä¢ We don‚Äôt need an accurate value.\n‚Ä¢ It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention.\n‚Ä¢ Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)\n27', '<2-hop>\n\nWhy?\n‚Ä¢ Throughout your career you might:\n‚Ä¢ Design/maintain/contract data centers\n‚Ä¢ Set up operations/devops\n‚Ä¢ Develop AI for IoT devices\n‚Ä¢ Be the next CEO/CTO of a software company\n‚Ä¢ Sustainability can be your main role:\n‚Ä¢ Green Software Developer\n‚Ä¢ Sustainability Consultant\n‚Ä¢ Green Advocate \n‚Ä¢ Founder of a Green Tech startup (B2B?)\n28']","Static code analysis can contribute to the development of more sustainable data centers by estimating energy efficiency. This process helps identify which parts of the code are more likely to introduce energy hotspots. By reviewing and optimizing these areas, developers can reduce the energy consumption of their applications, leading to more energy-efficient software deployments in data centers. This approach aligns with the ""Green Shift Left"" methodology, which emphasizes the importance of considering energy efficiency early in the software development process.","Static code analysis in data science can contribute to the development of more sustainable data centers by estimating energy efficiency and identifying potential energy hotspots within the code. This approach allows developers to focus on reviewing code segments that are more likely to introduce inefficiencies, thereby optimizing resource usage. By integrating such practices, especially in domains like data science, developers can help create more sustainable solutions, which aligns with broader career goals in sustainability, such as designing or maintaining energy-efficient data centers.",0.49999999995,0.6666666666666666,1.0,0.57,0.0,0.9846241457649476
How can static code analysis in data science projects help in designing more sustainablility-focused data centers?,"['‚Ä¢ Libraries for stats? ML? Web Dev? Cloud?\n\nC5. Add energy-awareness to existing software\n26\n‚Ä¢ Streamlit, Notebooks, JSFiddle, \nPyScript, zsh, etc. \n‚Ä¢\n\nC6 - Green Shift Left\n‚Ä¢ Estimate energy efficiency using static code analysis. \n‚Ä¢ We don‚Äôt need an accurate value. \n‚Ä¢ It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention.', 'Green Architectural Tactics for ML-Enabled \nSystem\n38\nICSE-SEIS 2024\n\nArchitectural tactics\n39\n\nData-centric\n40\n‚Ä¢ Reduce data size \n‚Ä¢ Sampling \n‚Ä¢ Dimensionality reduction \n‚Ä¢ Quantization\n\nAlgorithm Design\n41\n‚Ä¢ Carefully select your algorithm \n‚Ä¢ You don‚Äôt need the fanciest \ntechniques\n\nModel Optimization\n42\n‚Ä¢ Add energy to training parameters \n‚Ä¢ Reduce FLOPs \n‚Ä¢ Pruning,', '‚Ä¢ Energy/Power is more useful at the software usecase level. \n‚Ä¢ Carbon is more useful at the infrastructure level (e.g., datacenter) or at the \nproject level (e.g., the impact of developing a full software project). \n‚Ä¢ Choose your metrics wisely ;)\n26\n\n‚Ä¢ Carbon emissions ‚úÖ \n‚Ä¢ How do we go from energy consumption to carbon consumption?\n27\n\nCarbon intensity\n‚Ä¢ How much carbon is emitted per kWh of electricity consumed. \n‚Ä¢ The common unit: \n ü§∑ \n‚Ä¢ E.g.,  gas-based power plants emit less carbon than coal-based plants.']","['<1-hop>\n\nC6 - Green Shift Left\n‚Ä¢ Estimate energy eÔ¨Éciency using static code analysis.\n‚Ä¢ We don‚Äôt need an accurate value.\n‚Ä¢ It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention.\n‚Ä¢ Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)\n27', '<2-hop>\n\nWhy?\n‚Ä¢ Throughout your career you might:\n‚Ä¢ Design/maintain/contract data centers\n‚Ä¢ Set up operations/devops\n‚Ä¢ Develop AI for IoT devices\n‚Ä¢ Be the next CEO/CTO of a software company\n‚Ä¢ Sustainability can be your main role:\n‚Ä¢ Green Software Developer\n‚Ä¢ Sustainability Consultant\n‚Ä¢ Green Advocate \n‚Ä¢ Founder of a Green Tech startup (B2B?)\n28', '<1-hop>\n\nC6 - Green Shift Left\n‚Ä¢ Estimate energy eÔ¨Éciency using static code analysis.\n‚Ä¢ We don‚Äôt need an accurate value.\n‚Ä¢ It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention.\n‚Ä¢ Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)\n27', '<2-hop>\n\nWhy?\n‚Ä¢ Throughout your career you might:\n‚Ä¢ Design/maintain/contract data centers\n‚Ä¢ Set up operations/devops\n‚Ä¢ Develop AI for IoT devices\n‚Ä¢ Be the next CEO/CTO of a software company\n‚Ä¢ Sustainability can be your main role:\n‚Ä¢ Green Software Developer\n‚Ä¢ Sustainability Consultant\n‚Ä¢ Green Advocate \n‚Ä¢ Founder of a Green Tech startup (B2B?)\n28', '<1-hop>\n\nC6 - Green Shift Left\n‚Ä¢ Estimate energy eÔ¨Éciency using static code analysis.\n‚Ä¢ We don‚Äôt need an accurate value.\n‚Ä¢ It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention.\n‚Ä¢ Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)\n27', '<2-hop>\n\nWhy?\n‚Ä¢ Throughout your career you might:\n‚Ä¢ Design/maintain/contract data centers\n‚Ä¢ Set up operations/devops\n‚Ä¢ Develop AI for IoT devices\n‚Ä¢ Be the next CEO/CTO of a software company\n‚Ä¢ Sustainability can be your main role:\n‚Ä¢ Green Software Developer\n‚Ä¢ Sustainability Consultant\n‚Ä¢ Green Advocate \n‚Ä¢ Founder of a Green Tech startup (B2B?)\n28']","Static code analysis in data science projects can help in designing more sustainability-focused data centers by identifying parts of the code that are likely to introduce energy hotspots. By estimating energy efficiency through static code analysis, developers can pinpoint inefficient code segments that may lead to higher energy consumption. This allows for targeted optimization efforts, reducing the overall energy demand of software applications. Consequently, this can lead to more efficient use of resources in data centers, contributing to their sustainability by lowering energy consumption and, indirectly, carbon emissions.","Static code analysis in data science projects can help in designing more sustainablility-focused data centers by identifying energy hotspots in the code. This allows developers to review and optimize code that may contribute to higher energy consumption, thereby supporting the development of more efficient and sustainable data centers.",0.9999999999,1.0,,1.0,0.0,0.9683760004857151
How does the use of static code analysis in data science projects contribute to the design and optimization of more sustanable data centers?,"['‚Ä¢ Libraries for stats? ML? Web Dev? Cloud?\n\nC5. Add energy-awareness to existing software\n26\n‚Ä¢ Streamlit, Notebooks, JSFiddle, \nPyScript, zsh, etc. \n‚Ä¢\n\nC6 - Green Shift Left\n‚Ä¢ Estimate energy efficiency using static code analysis. \n‚Ä¢ We don‚Äôt need an accurate value. \n‚Ä¢ It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention.', '‚Ä¢ DeepSeek-V3 claims ‚Äúonly‚Äù 2.78M GPU hours \n‚Ä¢ Bloom by Huggingface reports 25 tons, 51 when considering embodied \nand operational carbon footprint. (176billion params)\n9\n\nRed AI\nAccuracy: 0.999999999\nGreen AI\n‚Ä¢ Energy \n‚Ä¢ Time \n‚Ä¢ Reproducibility \n‚Ä¢ Reusage\n\nHow can we adopt Green AI\n‚Ä¢ Check whether AI is needed. \n‚Ä¢ Select green datacenters. \n‚Ä¢ Run on low carbon intensity hours. \n‚Ä¢ Opt for GPU-optimised solutions (?)', 'Energy profiling of static analysis tools\n‚Ä¢ What is the energy consumption of a ""typical run"" for a few OSS projects? \n‚Ä¢ Does the type of analysis matter? Are certain analysis more expensive? Does \nthe number of rules that are activated in a static analysis tool important for the \nenergy consumption? \n‚Ä¢ Differences between static analysis tools [lower priority]\n\nB5.']","['<1-hop>\n\nC6 - Green Shift Left\n‚Ä¢ Estimate energy eÔ¨Éciency using static code analysis.\n‚Ä¢ We don‚Äôt need an accurate value.\n‚Ä¢ It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention.\n‚Ä¢ Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)\n27', '<2-hop>\n\nWhy?\n‚Ä¢ Throughout your career you might:\n‚Ä¢ Design/maintain/contract data centers\n‚Ä¢ Set up operations/devops\n‚Ä¢ Develop AI for IoT devices\n‚Ä¢ Be the next CEO/CTO of a software company\n‚Ä¢ Sustainability can be your main role:\n‚Ä¢ Green Software Developer\n‚Ä¢ Sustainability Consultant\n‚Ä¢ Green Advocate \n‚Ä¢ Founder of a Green Tech startup (B2B?)\n28', '<1-hop>\n\nC6 - Green Shift Left\n‚Ä¢ Estimate energy eÔ¨Éciency using static code analysis.\n‚Ä¢ We don‚Äôt need an accurate value.\n‚Ä¢ It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention.\n‚Ä¢ Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)\n27', '<2-hop>\n\nWhy?\n‚Ä¢ Throughout your career you might:\n‚Ä¢ Design/maintain/contract data centers\n‚Ä¢ Set up operations/devops\n‚Ä¢ Develop AI for IoT devices\n‚Ä¢ Be the next CEO/CTO of a software company\n‚Ä¢ Sustainability can be your main role:\n‚Ä¢ Green Software Developer\n‚Ä¢ Sustainability Consultant\n‚Ä¢ Green Advocate \n‚Ä¢ Founder of a Green Tech startup (B2B?)\n28']","The use of static code analysis in data science projects can contribute to the design and optimization of more sustainable data centers by identifying energy hotspots in the code. By estimating energy efficiency through static code analysis, developers can pinpoint which parts of the code are likely to consume more energy and should be reviewed more attentively. This process doesn't require precise energy values but helps in identifying areas that could be optimized to reduce energy consumption. Consequently, optimizing these areas can lead to more efficient use of resources in data centers, thereby contributing to sustainability efforts.","The use of static code analysis in data science projects contributes to the design and optimization of more sustainable data centers by identifying energy hotspots within the code. This process allows developers to focus on reviewing and optimizing sections of code that are more likely to introduce energy inefficiencies. By doing so, it supports the broader goal of creating more energy-efficient systems, which is crucial for the sustainable operation of data centers. This approach aligns with the growing need for green software development practices, where professionals can play roles such as green software developers or sustainability consultants, ensuring that energy efficiency is a key consideration throughout the software lifecycle.",0.9999999999,0.75,1.0,0.75,0.0909090908264462,0.9801163369424024
How reduce risk and reduce size in software engineering?,"['Green Architectural Tactics for ML-Enabled \nSystem\n38\nICSE-SEIS 2024\n\nArchitectural tactics\n39\n\nData-centric\n40\n‚Ä¢ Reduce data size \n‚Ä¢ Sampling \n‚Ä¢ Dimensionality reduction \n‚Ä¢ Quantization\n\nAlgorithm Design\n41\n‚Ä¢ Carefully select your algorithm \n‚Ä¢ You don‚Äôt need the fanciest \ntechniques\n\nModel Optimization\n42\n‚Ä¢ Add energy to training parameters \n‚Ä¢ Reduce FLOPs \n‚Ä¢ Pruning, sparsity \n‚Ä¢  Take advantage of existing models\n\nModel Training\n43\n‚Ä¢ Quantization\n‚Ä¢ SAVE TRAINING PROGRESS\n\nModel Deployment\n44\n‚Ä¢ Distributed deployment \n‚Ä¢ Energy efficient hardware and \nconfigurations\n\nModel Management\n45\n‚Ä¢ Reuse the model as much as possible \n\nRethinking the Architecture: Spiking Neural Networks\n46\n\nRethinking the Architecture: Spiking Neural Networks\n47\n\nSELF Lab\n48\n\nrecap\n\n\nLu√≠s Cruz \nL.Cruz@tudelft.nl \n10. Project 2\nSustainable Software Engineering\u2028\nCS4575\nSustainableSE 2025\nCarolin Brandt \nC.E.Brandt@tudelft.nl \nEnrique Barba Roque \nE.BarbaRoque@tudelft.nl\n\n1. Goal/assignment \n2. Deliverables \n3. Strategy \n4. Ideas\n\nAssignment\n‚Ä¢ Goal: Solve a Sustainable Software Engineering problem. \n‚Ä¢ Identify 1 problem that should be fixed to help enabling sustainability in \nthe software engineering industry/community. \n‚Ä¢ Propose a solution. A tool, framework, guidelines, etc. \n‚Ä¢ Implementation. \n‚Ä¢ Validation. (Depending on the idea) \n‚Ä¢ Dissemination/social impact. (Solution should be open source, welcome \ncontributors, post on social media? Tool website? Available in a package \nmanager?)\n3\n\nDeliverables\n‚Ä¢ Paper-like article. (Min 4 pages, max 10) \n‚Ä¢ Online git repo with open source codebase and/or replication package. \n‚Ä¢ Presentation: 5 min + 5min Q&A\n4\n\nArticle\n‚Ä¢ Different projects will have different expectations -> Make agreements with \nyour coach. \n‚Ä¢ Some projects are more technical and some projects more theoretical. \n‚Ä¢ Common requirements: \n‚Ä¢ Motivation, formulation of the problem being addressed, etc. \n‚Ä¢ Description of the solution.', 'Energy Efficiency vs Code \nQuality\n\nMeasuring Maintainability\n‚Ä¢ According to ISO/IEC 25010, Maintainability is ‚Äúthe degree of \neffectiveness and efficiency with which a software product or \nsystem can be modified to improve it, correct it or adapt it to \nchanges in environment, and in requirements‚Äù \n‚Ä¢ We use the code analysis tool Better Code Hub to assess \nmaintainability \n‚Ä¢ Better Code Hub maps the ISO/IEC 25010 standard on \nmaintainability into a set of guidelines derived from static analysis\n46\n\nMaintainability of Energy Changes\n‚Ä¢ What is the impact of making energy-oriented code changes on \nthe maintainability of mobile apps? \nMaintainability \nDifference \nvE-2\nvE-1\nvE\nEnergy \nCommit\nvE+1\nM(vE-1)\nM(vE)\nParent \nCommit\n‚àÜM\n47\n\nThreshold Marks\n48\n\nBetter Code Hub\nMaintainability\nCombine\ndatasets\nEnergy\nCommits\nBaseline\nCommits\nBao et al. \n(2015)\nMoura et al. \n(2016)\nCruz et al. \n(2018)\nCruz et al. \n(2019)\nEnergy Code Changes \nDataset\n539 commits\u2028\nfrom 306 mobile apps\n539 baseline commits\u2028\nfrom 306 mobile apps\n49\n\nImpact of energy changes on \nmaintainability\n50\n\nWhich energy \npatterns are more \nlikely to affect \nmaintainability?\n51\n\nTypical maintainability issue I\nhttps://github.com/einmalfel/PodListen/commit/2ed5a65\n4 changed files with 28 additions and 0 deletions.\n‚Ä¶\n‚Ä¶\n52\n\nTypical maintainability issue II\nhttps://github.com/mozilla/MozStumbler/commit/6ea0268\n5 changed files with 66 additions and 14 deletions.\n53\n\n54', 'Carolin Brandt \nC.E.Brandt@tudelft.nl \nLu√≠s Cruz \nL.Cruz@tudelft.nl \n1. Intro Class\nSustainable Software Engineering\u2028\nCS4575\nSustainableSE 2025\nEnrique Barba Roque \nE.BarbaRoque@tudelft.nl\n\n1. Intro to Sustainable SE \n2. Intro to the course\n2\n\nof the electricity consumed worldwide \nby 2040 will stem from ICT\n14%\n\n4\nhttps://xkcd.com/1007/\n\nBuzz words\n‚Ä¢ Eco-friendly \n‚Ä¢ Climate change, action, adaption \n‚Ä¢ Energy efficiency \n‚Ä¢ Environmental-responsible \n‚Ä¢ Carbon-neutral; Climate-neutral; Net zero \n‚Ä¢ Carbon-offsetting \n‚Ä¢ Carbon-free \n‚Ä¢ Clean technology \n‚Ä¢ E-waste\n5\n\nhttps://www.menti.com/uns9d89kzn\nWhat is Sustainable \nSoftware Engineering?\n?\n6\n\nSustainable Software \nEngineering is‚Ä¶\n7\n‚Ä¶the discipline that studies the process of creating software systems that are able \nto create value in the long term without hindering its surrounding environment.\n\n8\nEconomical\nSocial\nTechnical\nIndividual\nEnvironmental\nSustainable\nSoftware\nEngineering\n‚≠ê\nTechnical\u2028\n‚öô\nEconomical\u2028\nüí∞\nSocial\u2028\nüë©üíºüë®üíºüë©üíºüë©üíº\nIndividual\u2028\nüë©üíª\nEnvironmental\u2028\nüå±\nSustainable\u2028\nSoftware \nEngineering\n\nEconomical\n‚Ä¢ Focused on assets, capital and added value\u2028\n(wealth creation, prosperity, profitability, capital \ninvestment, income, etc.) \n‚Ä¢ Nr of customers  \n‚Ä¢ Man-day-rate estimate \n‚Ä¢ Next round of funding \n‚Ä¢ Meet requirements in the contract\n9\n\nTechnical\n‚Ä¢ Longevity of information, systems, and infrastructure and their \nadequate evolution with changing surrounding conditions. \n‚Ä¢ Examples: \n‚Ä¢ Technical Debt \n‚Ä¢ Does it scale? \n‚Ä¢ Software testing \n‚Ä¢ Bus-factor \n‚Ä¢ Data integrity \n‚Ä¢ Innovation \n‚Ä¢ ‚Ä¶\n10\n\nIndividual\n‚Ä¢ Well-being of the individuals in an organisation.\u2028\nNote that it also includes how well individuals interact \nwith each other within the org.']","['<1-hop>\n\nShuffle \nüîÄ\n‚Ä¢ It is not a mystery that energy consumption depends on so many factors that \nit is impossible to control all of them.\n‚Ä¢ If you run 30 executions for version A and another batch for version B:\n‚Ä¢ External conditions that change over time will have a diÔ¨Äerent bias in \nthe 2 versions (e.g., room temperature changes).\n‚Ä¢ If you shuÔ¨Ñe, you reduce this risk.\n11', '<2-hop>\n\nEcoAndroid\n‚Ä¢ Plugin for IntelliJ (Android Studio)\n‚Ä¢ Dynamic Retry Delay\n‚Ä¢ Push Over Poll\n‚Ä¢ Reduce Size\n‚Ä¢ Cache\n‚Ä¢ Avoid Graphics and Animations\n20', '<1-hop>\n\nShuffle \nüîÄ\n‚Ä¢ It is not a mystery that energy consumption depends on so many factors that \nit is impossible to control all of them.\n‚Ä¢ If you run 30 executions for version A and another batch for version B:\n‚Ä¢ External conditions that change over time will have a diÔ¨Äerent bias in \nthe 2 versions (e.g., room temperature changes).\n‚Ä¢ If you shuÔ¨Ñe, you reduce this risk.\n11', '<2-hop>\n\nEcoAndroid\n‚Ä¢ Plugin for IntelliJ (Android Studio)\n‚Ä¢ Dynamic Retry Delay\n‚Ä¢ Push Over Poll\n‚Ä¢ Reduce Size\n‚Ä¢ Cache\n‚Ä¢ Avoid Graphics and Animations\n20']","To reduce risk and reduce size in software engineering, you can employ several strategies:

1. **Reduce Data Size**: Implement data-centric tactics such as sampling, dimensionality reduction, and quantization to minimize the amount of data processed and stored.

2. **Algorithm Design**: Carefully select algorithms that are efficient and effective without being unnecessarily complex. Avoid using the most complex techniques if simpler ones suffice.

3. **Model Optimization**: Optimize models by reducing the number of floating-point operations (FLOPs), using pruning and sparsity techniques, and leveraging existing models to avoid redundant work.

4. **Model Training**: Use quantization and save training progress to prevent unnecessary re-computation and reduce the computational load.

5. **Model Deployment**: Deploy models in a distributed manner and use energy-efficient hardware configurations to minimize resource usage.

By implementing these tactics, you can effectively reduce both the risk and size of software systems, leading to more sustainable and efficient software engineering practices.","To reduce risk, shuffling executions helps control external factors like temperature changes. To reduce size, EcoAndroid uses features like dynamic retry delay and push over poll.",0.0,0.0,0.8666666666666667,0.17,0.0,0.9808039774417124
How shuffling reduce risk and EcoAndroid reduce size?,"['‚Ä¢ When a connection is not possible, the app automatically retries later. \n‚Ä¢ https://github.com/k9mail/k-9/commit/\n86f3b28f79509d1a4d613eb39f60603e08579ea3\n\nEcoAndroid\n‚Ä¢ Plugin for IntelliJ (Android Studio) \n‚Ä¢ Dynamic Retry Delay \n‚Ä¢ Push Over Poll \n‚Ä¢ Reduce Size \n‚Ä¢ Cache \n‚Ä¢ Avoid Graphics and Animations\n20\n\nCarbon-Aware Computing for \nDatacenters \nhttps://sites.google.com/view/energy-efficiency-languages\n21\nAna Radovanovic ÃÅ,', '*/\n6028 \nchanges\n\nThematic Analysis\n1. Familiarization with data \n2. Generating initial labels \n3. Reviewing themes\n4. Defining and naming themes\n\n‚Ä¢ Energy Pattern: design pattern to improve energy efficiency.. \n‚Ä¢ 22 energy patterns. \n‚Ä¢ Each pattern is described by Context, Solution, Example, References from \nliterature, and Occurences (links to code changes from git repositories).\n8\n\nhttps://tqrg.github.io/energy-patterns\n\nDark UI Colors \nProvide a dark UI color theme to \nsave battery on devices with \nAMOLED screens.\n‚Ä¢ Context: [‚Ä¶] Apps that require heavy usage of screen can \nhave a substantial negative impact on battery life. \n‚Ä¢ Solution: Provide a UI theme with dark background colors. [‚Ä¶]\n‚Ä¢ Example: In a reading app, provide a theme with a dark \nbackground using light colors to display text. [‚Ä¶] \n10\n\nDark UI Colors \nProvide a dark UI color theme to \nsave battery on devices with \nAMOLED screens.\n‚Ä¢ Context: [‚Ä¶] Apps that require heavy usage of screen can \nhave a substantial negative impact on battery life. \n‚Ä¢ Solution: Provide a UI theme with dark background colors. [‚Ä¶]\n‚Ä¢ Example: In a reading app, provide a theme with a dark \nbackground using light colors to display text. [‚Ä¶] \n11\n\nDynamic Retry Delay\n‚Ä¢ Context: [‚Ä¶] In a mobile app, when a given resource is \nunavailable, the app will unnecessarily try to connect the resource \nfor a number of times, leading to unnecessary power consumption. \n‚Ä¢ Solution: Increase retry interval after each failed connection. [‚Ä¶]\n‚Ä¢ Example: Consider a mobile app that provides a news feed and \nthe app is not able to reach the server to collect updates. [‚Ä¶] use \nthe Fibonacci series to increase the time between attempts.\nWhenever an attempt to access a \nresource fails, increase the time \ninterval before retrying.\n12\n\nBatch Operations \n‚Ä¢ Context: Executing operations separately leads to extraneous \ntail energy consumptions  \n‚Ä¢ Solution: Bundle multiple operations in a single one. [‚Ä¶]\n‚Ä¢ Example: Use system provided APIs to schedule background \ntasks. These APIs, guarantee that device will exit sleep mode \nonly when there is a reasonable amount of work to do or when a \ngiven task is urgent. [‚Ä¶]\n13\n\nAvoid Extraneous Graphics and \nAnimations  \n‚Ä¢ Context: Mobile apps that feature impressive graphics \nand animations.', '19\n(?)\n\nResults: energy consumption of algorithms\n\nResults: energy vs data shape\n\nResults: performance vs data shape\n\nDiscussion\n‚Ä¢ Other data properties should be investigated. \n‚Ä¢ E.g., data types \n‚Ä¢ Reporting energy data is essential. It can lead to different model selection \nwithout hindering model performance. \n‚Ä¢ There is a big opportunity in Model and Data Simplification.\n23\n\nData/Model Simplification\n‚Ä¢ (?) \n‚Ä¢ Data selection \n‚Ä¢ Data quantisation. Posit? \n‚Ä¢ Data distillation \n‚Ä¢ Coreset extraction (?)']","['<1-hop>\n\nShuffle \nüîÄ\n‚Ä¢ It is not a mystery that energy consumption depends on so many factors that \nit is impossible to control all of them.\n‚Ä¢ If you run 30 executions for version A and another batch for version B:\n‚Ä¢ External conditions that change over time will have a diÔ¨Äerent bias in \nthe 2 versions (e.g., room temperature changes).\n‚Ä¢ If you shuÔ¨Ñe, you reduce this risk.\n11', '<2-hop>\n\nEcoAndroid\n‚Ä¢ Plugin for IntelliJ (Android Studio)\n‚Ä¢ Dynamic Retry Delay\n‚Ä¢ Push Over Poll\n‚Ä¢ Reduce Size\n‚Ä¢ Cache\n‚Ä¢ Avoid Graphics and Animations\n20', '<1-hop>\n\nShuffle \nüîÄ\n‚Ä¢ It is not a mystery that energy consumption depends on so many factors that \nit is impossible to control all of them.\n‚Ä¢ If you run 30 executions for version A and another batch for version B:\n‚Ä¢ External conditions that change over time will have a diÔ¨Äerent bias in \nthe 2 versions (e.g., room temperature changes).\n‚Ä¢ If you shuÔ¨Ñe, you reduce this risk.\n11', '<2-hop>\n\nEcoAndroid\n‚Ä¢ Plugin for IntelliJ (Android Studio)\n‚Ä¢ Dynamic Retry Delay\n‚Ä¢ Push Over Poll\n‚Ä¢ Reduce Size\n‚Ä¢ Cache\n‚Ä¢ Avoid Graphics and Animations\n20', '<1-hop>\n\nShuffle \nüîÄ\n‚Ä¢ It is not a mystery that energy consumption depends on so many factors that \nit is impossible to control all of them.\n‚Ä¢ If you run 30 executions for version A and another batch for version B:\n‚Ä¢ External conditions that change over time will have a diÔ¨Äerent bias in \nthe 2 versions (e.g., room temperature changes).\n‚Ä¢ If you shuÔ¨Ñe, you reduce this risk.\n11', '<2-hop>\n\nEcoAndroid\n‚Ä¢ Plugin for IntelliJ (Android Studio)\n‚Ä¢ Dynamic Retry Delay\n‚Ä¢ Push Over Poll\n‚Ä¢ Reduce Size\n‚Ä¢ Cache\n‚Ä¢ Avoid Graphics and Animations\n20']","The sources provided do not specifically mention how shuffling reduces risk or how EcoAndroid reduces size. However, based on general knowledge:

1. **Shuffling to Reduce Risk**: In the context of data processing or machine learning, shuffling data can help reduce the risk of overfitting by ensuring that the model does not learn the order of the data, which might contain patterns not representative of the general data distribution.

2. **EcoAndroid Reducing Size**: EcoAndroid likely reduces size by optimizing the app's code and resources, possibly by avoiding unnecessary graphics and animations, caching data efficiently, and using other techniques to minimize the app's footprint on the device. This can lead to reduced energy consumption and better performance on mobile devices.",Shuffling reduce risk by controlling external factors like room temperature changes. EcoAndroid reduce size by using features like caching and avoiding graphics and animations.,0.0,0.5,0.6,0.0,0.1666666663888888,0.0
How shuffling reduce risk when compare to EcoAndroid reduce size?,"['‚Ä¢ When a connection is not possible, the app automatically retries later. \n‚Ä¢ https://github.com/k9mail/k-9/commit/\n86f3b28f79509d1a4d613eb39f60603e08579ea3\n\nEcoAndroid\n‚Ä¢ Plugin for IntelliJ (Android Studio) \n‚Ä¢ Dynamic Retry Delay \n‚Ä¢ Push Over Poll \n‚Ä¢ Reduce Size \n‚Ä¢ Cache \n‚Ä¢ Avoid Graphics and Animations\n20\n\nCarbon-Aware Computing for \nDatacenters \nhttps://sites.google.com/view/energy-efficiency-languages\n21\nAna Radovanovic ÃÅ,', 'Green Architectural Tactics for ML-Enabled \nSystem\n38\nICSE-SEIS 2024\n\nArchitectural tactics\n39\n\nData-centric\n40\n‚Ä¢ Reduce data size \n‚Ä¢ Sampling \n‚Ä¢ Dimensionality reduction \n‚Ä¢ Quantization\n\nAlgorithm Design\n41\n‚Ä¢ Carefully select your algorithm \n‚Ä¢ You don‚Äôt need the fanciest \ntechniques\n\nModel Optimization\n42\n‚Ä¢ Add energy to training parameters \n‚Ä¢ Reduce FLOPs \n‚Ä¢ Pruning,', '[‚Ä¶] \n‚Ä¢ Solution: Study the importance of graphics and \nanimations to the user experience and reduce them when \napplicable. [‚Ä¶]\n‚Ä¢ Example: Resort to low frame rates for animations when \npossible. \nDespite being important to improve user experience, graphics \nand animations are battery intensive and should be used with \nmoderation.  \n14\n\nEnergy Patterns are \nmore Frequent\nin Android Apps\n\nExample case: Nextcloud\nFOSS\n\nExample case: Nextcloud\n‚Ä¢ Users complain that sometimes they go on a trip and  Nextcloud drains their \nbattery. Users consider uninstalling the app when battery life is essential. \n‚Ä¢ File sync can be energy-greedy. Send large files to the server, long 3G/4G data connections. \n‚Ä¢ It is mostly used for backup. No real-time collaboration is needed. \n‚Ä¢ Energy requirements vary depending on context and user. Some days you really \nneed all the battery you can get. \n‚Ä¢ https://github.com/nextcloud/android/commit/\n8bc432027e0d33e8043cf40192203203a40ca29c\nSolutions?\n17\n\nExample case: K-9 mail\n18\n\nExample case: K-9 mail\n19\nSolutions?\n‚Ä¢ Some users noticed that K-9 mail was spending more energy than usual. üôâ \n‚Ä¢ A user that was having issues with a personal mail server  noticed that K-9 \nmail was the one of the most energy-greedy apps. IMAP IDLE protocol for real-time \nnotifications. \n‚Ä¢ When a connection is not possible, the app automatically retries later.']","['<1-hop>\n\nShuffle \nüîÄ\n‚Ä¢ It is not a mystery that energy consumption depends on so many factors that \nit is impossible to control all of them.\n‚Ä¢ If you run 30 executions for version A and another batch for version B:\n‚Ä¢ External conditions that change over time will have a diÔ¨Äerent bias in \nthe 2 versions (e.g., room temperature changes).\n‚Ä¢ If you shuÔ¨Ñe, you reduce this risk.\n11', '<2-hop>\n\nEcoAndroid\n‚Ä¢ Plugin for IntelliJ (Android Studio)\n‚Ä¢ Dynamic Retry Delay\n‚Ä¢ Push Over Poll\n‚Ä¢ Reduce Size\n‚Ä¢ Cache\n‚Ä¢ Avoid Graphics and Animations\n20']","The provided sources do not mention ""shuffling"" in the context of reducing risk, nor do they provide a direct comparison between shuffling and EcoAndroid's tactic of reducing size. However, in general terms, shuffling data can help in reducing risk by ensuring that data is not processed or stored in a predictable order, which can enhance security and privacy. On the other hand, EcoAndroid's tactic of reducing size focuses on minimizing resource usage, such as memory and processing power, which can lead to improved energy efficiency and performance. Both tactics aim to optimize different aspects of system performance and security.",Shuffling reduce risk by balance external factor like room temperature change while EcoAndroid reduce size by optimize resource use.,0.0,0.5,0.6666666666666666,0.29,0.249999999375,0.0
How shuffling reduce risk when reduce size in software engineering?,"['‚Ä¢ If you run 30 executions for version A and another batch for version B: \n‚Ä¢ External conditions that change over time will have a different bias in \nthe 2 versions (e.g., room temperature changes). \n‚Ä¢ If you shuffle, you reduce this risk.\n11\n\nKeep it cool üå°\n‚Ä¢ Always make sure there is a stable room temperature. \n‚Ä¢ Tricky because, some times, experiments may have to run over a few days. \n‚Ä¢ If you cannot control room temperature: collect temperature data and filter \nout measurements where the room temperature is clearly deviating.', 'Green Architectural Tactics for ML-Enabled \nSystem\n38\nICSE-SEIS 2024\n\nArchitectural tactics\n39\n\nData-centric\n40\n‚Ä¢ Reduce data size \n‚Ä¢ Sampling \n‚Ä¢ Dimensionality reduction \n‚Ä¢ Quantization\n\nAlgorithm Design\n41\n‚Ä¢ Carefully select your algorithm \n‚Ä¢ You don‚Äôt need the fanciest \ntechniques\n\nModel Optimization\n42\n‚Ä¢ Add energy to training parameters \n‚Ä¢ Reduce FLOPs \n‚Ä¢ Pruning, sparsity \n‚Ä¢  Take advantage of existing models\n\nModel Training\n43\n‚Ä¢ Quantization\n‚Ä¢ SAVE TRAINING PROGRESS\n\nModel Deployment\n44\n‚Ä¢ Distributed deployment \n‚Ä¢ Energy efficient hardware and \nconfigurations\n\nModel Management\n45\n‚Ä¢ Reuse the model as much as possible \n\nRethinking the Architecture: Spiking Neural Networks\n46\n\nRethinking the Architecture: Spiking Neural Networks\n47\n\nSELF Lab\n48\n\nrecap\n\n\nLu√≠s Cruz \nL.Cruz@tudelft.nl \n10. Project 2\nSustainable Software Engineering\u2028\nCS4575\nSustainableSE 2025\nCarolin Brandt \nC.E.Brandt@tudelft.nl \nEnrique Barba Roque \nE.BarbaRoque@tudelft.nl\n\n1. Goal/assignment \n2. Deliverables \n3. Strategy \n4. Ideas\n\nAssignment\n‚Ä¢ Goal: Solve a Sustainable Software Engineering problem. \n‚Ä¢ Identify 1 problem that should be fixed to help enabling sustainability in \nthe software engineering industry/community. \n‚Ä¢ Propose a solution. A tool, framework, guidelines, etc. \n‚Ä¢ Implementation. \n‚Ä¢ Validation. (Depending on the idea) \n‚Ä¢ Dissemination/social impact. (Solution should be open source, welcome \ncontributors, post on social media? Tool website? Available in a package \nmanager?)\n3\n\nDeliverables\n‚Ä¢ Paper-like article. (Min 4 pages, max 10) \n‚Ä¢ Online git repo with open source codebase and/or replication package. \n‚Ä¢ Presentation: 5 min + 5min Q&A\n4\n\nArticle\n‚Ä¢ Different projects will have different expectations -> Make agreements with \nyour coach. \n‚Ä¢ Some projects are more technical and some projects more theoretical. \n‚Ä¢ Common requirements: \n‚Ä¢ Motivation, formulation of the problem being addressed, etc. \n‚Ä¢ Description of the solution.', '‚Ä¢ What is the impact of reducing data in the \nperformance of the model? \n‚Ä¢ Method -> results -> discussion\n17\n\nMethod\n‚Ä¢ Single object of study: natural language model to detect spam messages. \n‚Ä¢ 6 machine learning algorithms: SVM, Decision Tree, KNN, Random Forrest, \nAdaBoost, Bagging Classifier. \n‚Ä¢ Reduce the number of rows. 10%, 20%, .., 100% \n‚Ä¢ Stratified random sampling (?) \n‚Ä¢ Reduce the number of features.']","['<1-hop>\n\nShuffle \nüîÄ\n‚Ä¢ It is not a mystery that energy consumption depends on so many factors that \nit is impossible to control all of them.\n‚Ä¢ If you run 30 executions for version A and another batch for version B:\n‚Ä¢ External conditions that change over time will have a diÔ¨Äerent bias in \nthe 2 versions (e.g., room temperature changes).\n‚Ä¢ If you shuÔ¨Ñe, you reduce this risk.\n11', '<2-hop>\n\nEcoAndroid\n‚Ä¢ Plugin for IntelliJ (Android Studio)\n‚Ä¢ Dynamic Retry Delay\n‚Ä¢ Push Over Poll\n‚Ä¢ Reduce Size\n‚Ä¢ Cache\n‚Ä¢ Avoid Graphics and Animations\n20', '<1-hop>\n\nShuffle \nüîÄ\n‚Ä¢ It is not a mystery that energy consumption depends on so many factors that \nit is impossible to control all of them.\n‚Ä¢ If you run 30 executions for version A and another batch for version B:\n‚Ä¢ External conditions that change over time will have a diÔ¨Äerent bias in \nthe 2 versions (e.g., room temperature changes).\n‚Ä¢ If you shuÔ¨Ñe, you reduce this risk.\n11', '<2-hop>\n\nEcoAndroid\n‚Ä¢ Plugin for IntelliJ (Android Studio)\n‚Ä¢ Dynamic Retry Delay\n‚Ä¢ Push Over Poll\n‚Ä¢ Reduce Size\n‚Ä¢ Cache\n‚Ä¢ Avoid Graphics and Animations\n20']","Shuffling reduces risk by minimizing the impact of external conditions that may change over time, such as room temperature, which can introduce bias in different versions of an experiment. By shuffling, you ensure that these external factors are evenly distributed across all versions, thereby reducing the risk of biased results when reducing data size in software engineering experiments.","Shuffling reduce risk by controlling external factors like room temperature changes, which help in reducing size in software engineering.",0.0,0.0,0.6,0.67,0.0,0.9544273024077174
How spiking neural networks relate to McCulloch-Pitts model?,"['References\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947.', 'Neural networks. 1997. 10(9):1659-71. \nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012).', 'https://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997.']","[""<1-hop>\n\nEncoding strategies\nWe considered single neurons with Boolean output (‚Äòspike‚Äô=1 or ‚Äòno spike‚Äô=0).\nHow should we encode information about 'features' in a large network with many spikes?\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\nElephant?\nhttps://www.nationalgeographic.com/anim\nals/mammals/facts/african-elephant"", '<2-hop>\n\nReferences\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch‚ÄìPitts neural networks. Journal of the History of the Behavioral \nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \n(not publicly available) doi:10.1037/h0042519\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\n6) https://github.com/idsc-frazzoli/retina\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\n10) G√ºtig R, Sompolinsky H. The tempotron: a neuron that learns spike timing‚Äìbased decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568', ""<1-hop>\n\nEncoding strategies\nWe considered single neurons with Boolean output (‚Äòspike‚Äô=1 or ‚Äòno spike‚Äô=0).\nHow should we encode information about 'features' in a large network with many spikes?\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\nElephant?\nhttps://www.nationalgeographic.com/anim\nals/mammals/facts/african-elephant"", '<2-hop>\n\nReferences\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch‚ÄìPitts neural networks. Journal of the History of the Behavioral \nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \n(not publicly available) doi:10.1037/h0042519\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\n6) https://github.com/idsc-frazzoli/retina\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\n10) G√ºtig R, Sompolinsky H. The tempotron: a neuron that learns spike timing‚Äìbased decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568', ""<1-hop>\n\nEncoding strategies\nWe considered single neurons with Boolean output (‚Äòspike‚Äô=1 or ‚Äòno spike‚Äô=0).\nHow should we encode information about 'features' in a large network with many spikes?\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\nElephant?\nhttps://www.nationalgeographic.com/anim\nals/mammals/facts/african-elephant"", '<2-hop>\n\nReferences\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch‚ÄìPitts neural networks. Journal of the History of the Behavioral \nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \n(not publicly available) doi:10.1037/h0042519\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\n6) https://github.com/idsc-frazzoli/retina\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\n10) G√ºtig R, Sompolinsky H. The tempotron: a neuron that learns spike timing‚Äìbased decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568']","Spiking neural networks (SNNs) and the McCulloch-Pitts model both represent approaches to modeling neural activity, but they differ significantly in their complexity and biological realism.

The McCulloch-Pitts model, introduced in 1943, is a simplified mathematical model of a neuron that uses binary threshold units to simulate neural activity. It operates on the principle that neurons either fire or do not fire based on whether the weighted sum of their inputs exceeds a certain threshold (Source 1).

Spiking neural networks, on the other hand, are considered the third generation of neural network models and aim to more closely mimic the temporal dynamics of biological neurons. SNNs incorporate the concept of time into their operation, where neurons communicate by sending discrete spikes at particular points in time. This allows SNNs to capture the timing of spikes, which is crucial for understanding many neural processes (Source 8).

In summary, while the McCulloch-Pitts model provides a foundational framework for understanding neural computation, spiking neural networks offer a more detailed and biologically plausible representation by incorporating the temporal aspect of neural signaling.","Spiking neural networks relate to the McCulloch-Pitts model by building upon the foundational concepts of neural computation, where the McCulloch-Pitts model introduced the idea of binary neuron states, which is extended in spiking networks to model more complex information processing through spike timing and patterns.",0.0,1.0,0.8666666666666667,0.92,0.0,0.9386147745268784
How spiking neural networks used in encoding strategies?,"[""‚Ä¶ with energy costs reaching 200M USD!\n\nPower-efficiency at scale\nYour brain runs on:\n\nPower-efficiency at scale\nYour brain runs on:\nHigh estimate ~3000 kcal a day\n   ‚âà145 Watts\n* Human brain has ~600 trillion synapses \n(‚âàparameters).\n\nPower-efficiency at scale\nYour brain runs on:\nHigh estimate ~3000 kcal a day\n   ‚âà145 Watts\n* Human brain has ~600 trillion synapses \n(‚âàparameters).\nOversimplification\nThere are also multiple other advantages‚Ä¶\n\nHuman vs. computer computation\n- Fast real-time decision making, e.g. sports, e-sports\n- Adaptive, e.g. context-aware and employs selective attention\n- Energy efficient: Close to 100 billion neurons in the brain\n- Robust, for example to changes in illumination or obstructions in object tracking\n\nHuman vs. computer computation\nBrains are energy efficient: Why?\n1.\nHigh temporal resolution (more computation with less neurons)\n2.\nSparse encoding\n\nHuman vs. computer computation\nQuestions?\nBrains are energy efficient: Why?\n1.\nHigh temporal resolution (more computation with less neurons)\n2.\nSparse encoding\n\nHow do biological neurons communicate?\nAnalogy to artificial neural networks\nReal neuron\nArtificial neuron\nCurrent\n\nHow do biological neurons communicate?\nHow does the electrical activity propagate?\n?\nCurrent\n\nHow do biological neurons work?\ntime\nelectrode\nhttps://tinmard.github.io/spike-sorting-animation.html\nQuick electrical pulses trigger chemical signals for the next neuron ‚Üí Spikes\n\nHow do biological neurons work?\nQuick electrical pulses trigger chemical signals for the next neuron ‚Üí Spikes\nhttps://tinmard.github.io/spike-sorting-animation.html\ntime\nelectrode\nFrom CS perspective:\nSparse binary \nencoding\n\nBiologically realistic spiking neuron models\nBiologically realistic neuron models have \na new dimension: Time!\nSpiking neural networks (SNNs):\nThe input x  (t  ) to each neuron is summed \n(integrated) over time.\nscalar\nfunction of \ntime\n(t)\n(t)\nhttps://www.mdpi.com/2076-3425/12/7/863\n\nHow to model spiking neurons?\nANN: Perceptron, threshold activation function:\n\nHow to model spiking neurons?\nANN: Perceptron, threshold activation function:\n\nHow to model spiking neurons?\nSpiking neural network (SNN): The 'input current' Œµ(t) is integrated over time.\nspike\n\nHow to model spiking neurons?\nEquivalence to perceptron: Computation at least as complex as a perceptron.\n\nNon-leaky integrate-and-fire (IF) neuron\nThe temporal profile of the input current Œµ(t) can be chosen differently, for different computations.\n\nQuestions?\n\nComputation with spiking neurons\nCoincidence detection\n\nComputation with spiking neurons\nCoincidence detection\nx, y ‚àà {0, 1}n\nExample: n = 2\ninput = \nx1\nx2\ny1\ny2\n= \n0\n1\n1\n1\noutput? \n\nCoincidence detection\nComputation with spiking neurons\n\nCoincidence detection\nCan be trivially computed with a single spiking neuron! Requires at least n/log(n+1) hidden \nunits for a perceptron (proof in [7]).\nComputation with spiking neurons\n\nCoincidence detection\nCan be trivially computed with a single spiking neuron! Requires at least n/log(n+1) hidden \nunits for a perceptron (proof in [7]).\nBrains are energy efficient: \n1.\nHigh temporal resolution (more computation with less neurons)\nComputation with spiking neurons\n\nEncoding strategies\nWe considered single neurons with Boolean output (‚Äòspike‚Äô=1 or ‚Äòno spike‚Äô=0).\nHow should we encode information about 'features' in a large network with many spikes?\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\nElephant?\nhttps://www.nationalgeographic.com/anim\nals/mammals/facts/african-elephant\n\nFiring rates\nClassical view of the brain:\n- Each neuron is selective for one specific \nfeature in the input.\n- Higher firing rate (spikes per unit time) for \n'selected' feature.\nAdapted from Hubel & Wiesel, 1959\n\nFiring rates\nClassical view of the brain:\n- Each neuron is selective for one specific \nfeature in the input.\n- Higher firing rate (spikes per unit time) for \n'selected' feature.\n- Link to modern ANNs: The scalar output of \nan artificial neuron is interpreted as the firing \nrate.\nAdapted from Hubel & Wiesel, 1959\n\nFiring rates\nClassical view of the brain:\n- Each neuron is selective for one specific \nfeature in the input.\n- Higher firing rate (spikes per unit time) for \n'selected' feature.\n- Link to modern ANNs: The scalar output of \nan artificial neuron is interpreted as the firing \nrate.\nAdapted from Hubel & Wiesel, 1959\nBut rate coding is inefficient and slow‚Ä¶\n(i.e. each neuron needs to fire many spikes to get good precision)\nboth in vivo and in silico.\n\nDifferent encoding strategies with spiking neurons [8]\n7 neurons\n7 time bins\n\nDifferent encoding strategies with spiking neurons\n7 neurons\n7 time bins\nn\n6\n(in the general \ncase a \nnumber \nbetween 0-7)\n\nDifferent encoding strategies with spiking neurons\n7 neurons\n7 time bins\nn\nTotal spike \ncount can \nencode 3 bits.\n\nDifferent encoding strategies with spiking neurons\n7 neurons\n7 time bins\nIndividual rates \ncan encode 7 \nbits.\n\nDifferent encoding strategies with spiking neurons\n7 neurons\n7 time bins\nLatency can \nencode ~3*7 or \n~19 bits.\n\nDifferent encoding strategies with spiking neurons\n7 neurons\n7 time bins\nRank order can \nencode ~12 bits.\n\nDifferent encoding strategies with spiking neurons\n\nDifferent encoding strategies with spiking neurons\nUsing latency or spatio-temporal codes\nour encoding is faster and more spike-efficient (sparser)!\n\nDifferent encoding strategies with spiking neurons\nUsing latency or spatio-temporal codes\nour encoding is faster and more spike-efficient (sparser)!\nQuestions?\n\nNeuromorphic computing\nWhat is the advantage for applications?\n-\nLess spikes = less energy consumption in specialized neuromorphic hardware\n(e.g. Intel Loihi [12])\nBrains are energy efficient: \n2. Sparse encoding\n\nMultiply-accumulate (MAC) operations:\nNormal neuron: Multiplies input with \nweights, then adds.\nSpiking neuron: Consider binary input (e.g. input currents are piecewise \nconstant and assume values {0,1}). There is no multiplication, only addition.\n\nAssume one multiplier and one adder circuit uses M and A energy respectively with AÔπ§M\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\nNormal neuron: nin x nout multiplications,\n(nin‚Äì 1) x nout additions\nSpiking neuron: 0 multiplications, (nactive‚Äì 1) x nout \nadditions, with nactive ‚â§ nin\nEnergy consumption:\nEnormal = M nin nout + A (nin‚Äì 1) nout = 17.5 pJ\nEnergy consumption:\nEspiking = A (nactive‚Äì 1) nout = 2.7 pJ\nMultiply-accumulate (MAC) operations:\n\nAssume one multiplier and one adder circuit uses M and A energy respectively with AÔπ§M\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\nNormal neuron: nin x nout multiplications,\n(nin‚Äì 1) x nout additions\nSpiking neuron: 0 multiplications, (nactive‚Äì 1) x nout \nadditions, with nactive ‚â§ nin\nEnergy consumption:\nEnormal = M nin nout + A (nin‚Äì 1) nout = 17.5 pJ\nEnergy consumption:\nEspiking = A (nactive‚Äì 1) nout = 2.7 pJ\nMultiply-accumulate (MAC) operations:\nSNN challenge: how to \ncompute with the least \namount of spikes!\n\nOften, we observe a \nsparsity (energy)-\ntask accuracy trade-off\n(Left: results for image \nclassification)\nSNN challenge: how to \ncompute with the least \namount of spikes!\nhttps://arxiv.org/pdf/2409.08290\n\nIn practice, energy \nconsumption computations \nare complex. \nNeed to take into account\n- memory access,\n- addressing,\n- auxiliary operations,\nin addition to MACs.\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \nand video classification respectively). FNN‚Äôs are conventional feed-forward neural networks.\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.\nhttps://arxiv.org/pdf/2210.13107\nComputing energy consumption\n\nIn practice, energy \nconsumption computations \nare complex. \nNeed to take into account\n- memory access,\n- addressing,\n- auxiliary operations,\nin addition to MACs.\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \nand video classification respectively). FNN‚Äôs are conventional feed-forward neural networks."", 'Often, we observe a \nsparsity (energy)-\ntask accuracy trade-off\n(Left: results for image \nclassification)\nSNN challenge: how to \ncompute with the least \namount of spikes!\nhttps://arxiv.org/pdf/2409.08290\n\nIn practice, energy \nconsumption computations \nare complex. \nNeed to take into account\n- memory access,\n- addressing,\n- auxiliary operations,\nin addition to MACs.\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \nand video classification respectively).', 'Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\n6) https://github.com/idsc-frazzoli/retina\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\n10) G√ºtig R, Sompolinsky H. The tempotron: a neuron that learns spike timing‚Äìbased decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945.']","[""<1-hop>\n\nEncoding strategies\nWe considered single neurons with Boolean output (‚Äòspike‚Äô=1 or ‚Äòno spike‚Äô=0).\nHow should we encode information about 'features' in a large network with many spikes?\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\nElephant?\nhttps://www.nationalgeographic.com/anim\nals/mammals/facts/african-elephant"", '<2-hop>\n\nReferences\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch‚ÄìPitts neural networks. Journal of the History of the Behavioral \nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \n(not publicly available) doi:10.1037/h0042519\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\n6) https://github.com/idsc-frazzoli/retina\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\n10) G√ºtig R, Sompolinsky H. The tempotron: a neuron that learns spike timing‚Äìbased decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568']","Spiking neural networks (SNNs) use various encoding strategies to efficiently process information. These strategies include:

1. **Latency Encoding**: The timing of spikes is used to encode information. This method can encode a significant amount of data quickly and efficiently, as it relies on the precise timing of spikes rather than the rate of firing.

2. **Rate Encoding**: This classical view involves each neuron being selective for a specific feature, with the firing rate (spikes per unit time) representing the intensity of that feature. However, this method is considered inefficient and slow because it requires many spikes to achieve good precision.

3. **Rank Order Encoding**: This strategy uses the order in which neurons fire to encode information, which can be more efficient than rate encoding.

4. **Sparse Encoding**: SNNs often use sparse encoding, where fewer spikes are used to represent information, leading to reduced energy consumption. This is particularly advantageous in neuromorphic hardware, where less energy is consumed with fewer spikes.

These encoding strategies allow SNNs to be more energy-efficient and faster in processing information compared to traditional neural networks.",Spiking neural networks use single neurons with Boolean output (‚Äòspike‚Äô=1 or ‚Äòno spike‚Äô=0) to encode information about 'features' in a large network with many spikes. This approach is supported by foundational works such as McCulloch and Pitts' logical calculus of nervous activity and later developments in spiking neuron models.,0.0,0.5,1.0,0.29,,0.9524510480717828
What are the benefits of neuromorphic computing in the context of green software practices?,"['Questions?\n\nNeuromorphic computing\nWhat is the advantage for applications?\n-\nLess spikes = less energy consumption in specialized neuromorphic hardware\n(e.g. Intel Loihi [12])\nBrains are energy efficient: \n2. Sparse encoding\n\nMultiply-accumulate (MAC) operations:\nNormal neuron: Multiplies input with \nweights, then adds.\nSpiking neuron: Consider binary input (e.g. input currents are piecewise \nconstant and assume values {0,1}). There is no multiplication, only addition.', 'Oversimplification\nThere are also multiple other advantages‚Ä¶\n\nHuman vs. computer computation\n- Fast real-time decision making, e.g. sports, e-sports\n- Adaptive, e.g. context-aware and employs selective attention\n- Energy efficient: Close to 100 billion neurons in the brain\n- Robust, for example to changes in illumination or obstructions in object tracking\n\nHuman vs. computer computation\nBrains are energy efficient: Why?\n1.\nHigh temporal resolution (more computation with less neurons)\n2.\nSparse encoding\n\nHuman vs. computer computation\nQuestions?\nBrains are energy efficient: Why?', 'Browser plugin?\n24\nCO2\n\nC4. NutriScore for software libraries\n25\n‚Ä¢ NutriScore labels are not perfect but they are a good starting point! \n‚Ä¢ What if we could do the same thing for the energy efficiency of software. \n‚Ä¢ (Also open to individual or social sustainability) \n‚Ä¢ This work can be scoped in particular domains/ecossystems/use cases. \n‚Ä¢ Libraries for stats? ML? Web Dev? Cloud?\n\nC5. Add energy-awareness to existing software\n26\n‚Ä¢ Streamlit, Notebooks, JSFiddle, \nPyScript, zsh, etc. \n‚Ä¢\n\nC6 - Green Shift Left\n‚Ä¢ Estimate energy efficiency using static code analysis. \n‚Ä¢ We don‚Äôt need an accurate value. \n‚Ä¢ It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention. \n‚Ä¢ Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)\n27\n\nC7 - Education\n‚Ä¢ Educational game for Software \nSustainability practices \n‚Ä¢ Purpose: use within software teams to \ndiscuss or learn about different sustainable \nIT practices: at the organisation level, \nsoftware, etc.\n28\nhttps://github.com/OttoKaaij/Ticket-To-Sustainability/?tab=readme-ov-file\n\nProject ideas (old)\n‚Ä¢ Plugin from EnergiBridge (GUI, report generator, python library, etc.) \n‚Ä¢ Plugin for ChatGPT (carbon emissions per chat window) \n‚Ä¢ Seamless measurements for AI libraries \n‚Ä¢ Energy patterns for Green AI \n‚Ä¢ Sustainable SW dev gamification \n‚Ä¢ Sustainability auditor for AI projects \n‚Ä¢ Energy Profiling of screen colour filter tools (or display settings) \n‚Ä¢ ‚Ä¶ you can also propose yours!\n\nhttps://edu.nl/64gpk\nedu.nl/64gpk\n\n\nNeuromorphic computing\nCS4575\nSustainable Software Engineering\n05.03.2025\nNergis T√∂men\n\nIntroduction\nComputer Vision Lab\nBiomorphic Intelligence Lab\nBiomedical Intervention Optimisation Lab\nNergis\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.']","['<1-hop>\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\n\nReading materials\nMain reading:\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n- Neuromorphic computing:\n- Based on biology: Zenke F, Boht√© SM, Clopath C, Com≈üa IM, G√∂ltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \nhttps://www.nature.com/articles/s41586-019-1677-2\nExtra reading:\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\n- Also an important part of neuromorphic systems and vision ‚Üí Event Cameras: Gallego G, Delbr√ºck T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\nBasics of \'conventional\' neural networks:\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.', '<1-hop>\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\n\nReading materials\nMain reading:\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n- Neuromorphic computing:\n- Based on biology: Zenke F, Boht√© SM, Clopath C, Com≈üa IM, G√∂ltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \nhttps://www.nature.com/articles/s41586-019-1677-2\nExtra reading:\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\n- Also an important part of neuromorphic systems and vision ‚Üí Event Cameras: Gallego G, Delbr√ºck T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\nBasics of \'conventional\' neural networks:\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.', '<1-hop>\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\n\nReading materials\nMain reading:\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n- Neuromorphic computing:\n- Based on biology: Zenke F, Boht√© SM, Clopath C, Com≈üa IM, G√∂ltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \nhttps://www.nature.com/articles/s41586-019-1677-2\nExtra reading:\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\n- Also an important part of neuromorphic systems and vision ‚Üí Event Cameras: Gallego G, Delbr√ºck T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\nBasics of \'conventional\' neural networks:\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.']","Neuromorphic computing offers several benefits in the context of green software practices:

1. **Energy Efficiency**: Neuromorphic computing is inspired by the human brain, which is highly energy efficient. It uses less energy due to sparse encoding and the nature of spiking neurons, which only perform addition instead of multiply-accumulate operations. This results in less energy consumption, especially in specialized neuromorphic hardware like Intel's Loihi.

2. **Real-time Decision Making**: Neuromorphic systems can make fast real-time decisions, which is beneficial for applications requiring quick responses, such as sports or e-sports, while maintaining energy efficiency.

3. **Adaptive and Robust**: These systems are adaptive and robust, similar to the human brain, allowing them to handle changes in conditions like illumination or obstructions in object tracking without significant energy overhead.

Overall, neuromorphic computing aligns well with green software practices by reducing energy consumption and improving computational efficiency.","Neuromorphic computing offers several benefits in the context of green software practices. It is inspired by the structure and function of the human brain, making it highly energy-efficient compared to traditional computing methods. This efficiency is particularly relevant for sustainable software development, as it reduces the environmental impact of computing systems. Additionally, neuromorphic systems can process information in a way that mimics biological neural networks, which can lead to more efficient and sustainable computing solutions. These aspects make neuromorphic computing a promising approach for developers looking to implement green software practices.",0.9999999999,0.4,0.9166666666666666,0.67,0.1249999998437499,1.0000000000000009
What are the challenges in creating a dataset of government-developed OS software?,"['Dataset of government-developed OS software\n‚Ä¢ Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise \n‚Ä¢ But studying government software is difficult b/c we don‚Äôt know what is out \nthere\u2028\n‚Üí Lack of incentive to make popular \u2028\n‚Üí Language barriers \n‚Ä¢ Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation? \n‚Ä¢ Could start with NL, but including your / other countries greatly appreciated!\n\nB7.', '(2016)\nCruz et al. \n(2018)\nCruz et al. \n(2019)\nEnergy Code Changes \nDataset\n539 commits\u2028\nfrom 306 mobile apps\n539 baseline commits\u2028\nfrom 306 mobile apps\n49\n\nImpact of energy changes on \nmaintainability\n50\n\nWhich energy \npatterns are more \nlikely to affect \nmaintainability?\n51\n\nTypical maintainability issue I\nhttps://github.com/einmalfel/PodListen/commit/2ed5a65\n4 changed files with 28 additions and 0 deletions.', 'This is time consuming and requires \nexpertise. \n‚Ä¢ How can we reuse these efforts?\n4\n\nEnergy Patterns for Mobile \nApps\nhttps://tqrg.github.io/energy-patterns/\n\nMethodology\n5. Catalog of Energy Patterns\n22 \npatterns\nF-droid\nCurated Lists\n1. App Collection\n1783 \napps\n3. Manual Refinement of Subjects of \nInterest\n1563 \nchanges\n4. Thematic Analysis\n431 \nreusable \nchanges\n2. Collect Changes With Potential Interest\n/.*(energy|power|battery).']","['<1-hop>\n\nB6. Dataset of government-developed OS software\n‚Ä¢ Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise\n‚Ä¢ But studying government software is diÔ¨Écult b/c we don‚Äôt know what is out \nthere\u2028\n‚Üí Lack of incentive to make popular \u2028\n‚Üí Language barriers\n‚Ä¢ Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation?\n‚Ä¢ Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\n\nProject ideas\n‚Ä¢ B1. Measure energy consumption of single JUnit tests \n‚Ä¢ B2. Study test generation energy consumption\n‚Ä¢ B3. Energy proÔ¨Åling of static analysis tools\n‚Ä¢ B4. Detailed energy proÔ¨Åling of build pipelines\n‚Ä¢ B5. Tool supporting SusAF workshop / process\n‚Ä¢ B6. Dataset of government-developed OS software\n‚Ä¢ B7. Queue - but better for the student / TA society\nQuality Assurance & Testing\nSocial & Individual \u2028\nSustainability', '<1-hop>\n\nB6. Dataset of government-developed OS software\n‚Ä¢ Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise\n‚Ä¢ But studying government software is diÔ¨Écult b/c we don‚Äôt know what is out \nthere\u2028\n‚Üí Lack of incentive to make popular \u2028\n‚Üí Language barriers\n‚Ä¢ Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation?\n‚Ä¢ Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\n\nProject ideas\n‚Ä¢ B1. Measure energy consumption of single JUnit tests \n‚Ä¢ B2. Study test generation energy consumption\n‚Ä¢ B3. Energy proÔ¨Åling of static analysis tools\n‚Ä¢ B4. Detailed energy proÔ¨Åling of build pipelines\n‚Ä¢ B5. Tool supporting SusAF workshop / process\n‚Ä¢ B6. Dataset of government-developed OS software\n‚Ä¢ B7. Queue - but better for the student / TA society\nQuality Assurance & Testing\nSocial & Individual \u2028\nSustainability', '<1-hop>\n\nB6. Dataset of government-developed OS software\n‚Ä¢ Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise\n‚Ä¢ But studying government software is diÔ¨Écult b/c we don‚Äôt know what is out \nthere\u2028\n‚Üí Lack of incentive to make popular \u2028\n‚Üí Language barriers\n‚Ä¢ Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation?\n‚Ä¢ Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\n\nProject ideas\n‚Ä¢ B1. Measure energy consumption of single JUnit tests \n‚Ä¢ B2. Study test generation energy consumption\n‚Ä¢ B3. Energy proÔ¨Åling of static analysis tools\n‚Ä¢ B4. Detailed energy proÔ¨Åling of build pipelines\n‚Ä¢ B5. Tool supporting SusAF workshop / process\n‚Ä¢ B6. Dataset of government-developed OS software\n‚Ä¢ B7. Queue - but better for the student / TA society\nQuality Assurance & Testing\nSocial & Individual \u2028\nSustainability']","The challenges in creating a dataset of government-developed OS software include:

1. **Lack of Incentive to Make Software Popular**: There is often little motivation for governments to promote their software widely, which can lead to difficulties in discovering and cataloging these projects.

2. **Language Barriers**: Government software may be developed in various languages, making it challenging to compile and understand the software across different regions.

3. **Difficulty in Understanding the State of Software**: It is challenging to determine whether the software is buildable, has an open development history, or includes requirements documentation.

These factors make it difficult to study and create a comprehensive dataset of government-developed software.","Creating a dataset of government-developed OS software faces challenges such as a lack of incentive to make the software popular and language barriers, which hinder comprehensive understanding of the software's buildability, open development history, and requirements documentation.",0.9999999999,1.0,0.7272727272727273,1.0,0.249999999375,0.9999999999999996
"What are the practical applications of neuromorphic computing, and how does its biological inspiration influence its implementation in modern computing systems?","['Questions?\n\nNeuromorphic computing\nWhat is the advantage for applications?\n-\nLess spikes = less energy consumption in specialized neuromorphic hardware\n(e.g. Intel Loihi [12])\nBrains are energy efficient: \n2. Sparse encoding\n\nMultiply-accumulate (MAC) operations:\nNormal neuron: Multiplies input with \nweights, then adds.\nSpiking neuron: Consider binary input (e.g. input currents are piecewise \nconstant and assume values {0,1}). There is no multiplication, only addition.', 'A neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing\n\nHow many of you are familiar \nwith neural networks?\n\nSimple model of an artificial neuron\nhttps://medium.com/@cprasenjit32/perceptron-a-simple-yet-mighty-machine-learning-algorithm-9ff6b7d86a71\n\nWhat is a neural network?\nhttps://www.tibco.com/reference-center/what-is-a-neural-network\n\nWhy is it called a \'neural network\'?\nhttps://www.tibco.com/reference-center/what-is-a-neural-network\n\nWhat is a neural network?\nWhy is it called a \'neural network\'?\nMcCulloch-Pitts neuron [1, 2] (1943)\nWarren Sturgis McCulloch\n(Neurophysiologist)\nWalter Pitts\n(Logician)\n\nHow do biological neurons work?\nhttps://commons.wikimedia.org/wiki/File:Coincidence_detection_in_dendrites_of_pyramidal_neurons.gif\n\'Input current\' travels down the dendrites (top),\nget integrated (summed!) in the cell body\nwhich generates an \'output current\' (bottom)\nwhich is chemically transmitted to the dendrites of \nother neurons.\n\nSimplified picture\nReal neuron\nArtificial neuron\nhttps://vajiramandravi.com/quest-upsc-notes/artificial-neural-network/\n\nWhat is a neural network?\nMcCulloch-Pitts neuron [1, 2]\n1940s: How do biological neurons compute basic \nlogic functions? (e.g. logic gates)\nNote: Ref. [3] gives a nice brief history on the ideas which lead to the \nMcCulloch-Pitts neuron.\n\nWhat is a neural network?\n1950s: How are neurons organized to perform \nsensory perception?\n\nWhat is a neural network?\n1950s: How are neurons organized to perform \nsensory perception?\nThe first ""neural network"": Perceptron (1958).', 'https://edu.nl/64gpk\nedu.nl/64gpk\n\n\nNeuromorphic computing\nCS4575\nSustainable Software Engineering\n05.03.2025\nNergis T√∂men\n\nIntroduction\nComputer Vision Lab\nBiomorphic Intelligence Lab\nBiomedical Intervention Optimisation Lab\nNergis\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.']","['<1-hop>\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\n\nReading materials\nMain reading:\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n- Neuromorphic computing:\n- Based on biology: Zenke F, Boht√© SM, Clopath C, Com≈üa IM, G√∂ltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \nhttps://www.nature.com/articles/s41586-019-1677-2\nExtra reading:\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\n- Also an important part of neuromorphic systems and vision ‚Üí Event Cameras: Gallego G, Delbr√ºck T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\nBasics of \'conventional\' neural networks:\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.', '<1-hop>\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\n\nReading materials\nMain reading:\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n- Neuromorphic computing:\n- Based on biology: Zenke F, Boht√© SM, Clopath C, Com≈üa IM, G√∂ltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \nhttps://www.nature.com/articles/s41586-019-1677-2\nExtra reading:\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\n- Also an important part of neuromorphic systems and vision ‚Üí Event Cameras: Gallego G, Delbr√ºck T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\nBasics of \'conventional\' neural networks:\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.']","Neuromorphic computing is inspired by the structure and function of the human brain, and it uses physical artificial neurons to perform computations. This approach offers several advantages for practical applications, particularly in areas where energy efficiency and processing speed are critical.

1. **Energy Efficiency**: Neuromorphic systems, such as Intel's Loihi chip, consume less energy because they use spiking neurons that perform operations with fewer spikes, leading to reduced energy consumption. This is particularly beneficial for applications that require continuous processing with minimal power usage.

2. **Sparse Encoding**: Neuromorphic systems use sparse encoding, which mimics the brain's way of processing information. This allows for efficient data representation and processing, making it suitable for applications like real-time data analysis and sensory processing.

3. **Efficient Computation**: Unlike traditional neurons that perform multiply-accumulate (MAC) operations, spiking neurons in neuromorphic systems only perform addition, which simplifies computations and reduces the computational load.

These characteristics make neuromorphic computing particularly suitable for applications in computer vision, sensory perception, and other areas where mimicking the brain's efficiency and processing capabilities can lead to significant improvements in performance and energy savings.","Neuromorphic computing, inspired by the human brain, offers practical applications such as event-based vision systems and efficient processing of sensory data. Its biological inspiration influences its implementation through the use of spiking neural networks and physical artificial neurons, enabling more energy-efficient and brain-like computation methods.",0.0,0.5,0.7142857142857143,0.6,0.1111111109876543,0.886272890966556
"What are the practical applications of neuromorphic computing, and how does its biological inspiration influence its implementation?","['Questions?\n\nNeuromorphic computing\nWhat is the advantage for applications?\n-\nLess spikes = less energy consumption in specialized neuromorphic hardware\n(e.g. Intel Loihi [12])\nBrains are energy efficient: \n2. Sparse encoding\n\nMultiply-accumulate (MAC) operations:\nNormal neuron: Multiplies input with \nweights, then adds.\nSpiking neuron: Consider binary input (e.g. input currents are piecewise \nconstant and assume values {0,1}). There is no multiplication, only addition.', 'The first ""neural network"": Perceptron (1958). [4]\nhttps://www.reddit.com/r/interestingasfuck/comments/e8a8oy/frank_rosenblatt_with_a_mark_i_perceptron/\nFrank Rosenblatt (Psychologist)\nwith a Mark I Perceptron computer in 1960\nPower-efficient\n\nNeuromorphic computing\nWhy neuromorphic computing?\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.', 'Proceedings of the National \nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\n- Also an important part of neuromorphic systems and vision ‚Üí Event Cameras: Gallego G, Delbr√ºck T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\nBasics of \'conventional\' neural networks:\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.\n\nReferences\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch‚ÄìPitts neural networks. Journal of the History of the Behavioral \nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \n(not publicly available) doi:10.1037/h0042519\n5) Tappert CC.']","['<1-hop>\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\n\nReading materials\nMain reading:\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n- Neuromorphic computing:\n- Based on biology: Zenke F, Boht√© SM, Clopath C, Com≈üa IM, G√∂ltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \nhttps://www.nature.com/articles/s41586-019-1677-2\nExtra reading:\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\n- Also an important part of neuromorphic systems and vision ‚Üí Event Cameras: Gallego G, Delbr√ºck T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\nBasics of \'conventional\' neural networks:\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.']","Neuromorphic computing is inspired by the structure and function of the human brain, which influences its implementation by using physical artificial neurons to perform computations. This approach offers several advantages for practical applications:

1. **Energy Efficiency**: Neuromorphic systems, like the human brain, are highly energy-efficient. They consume less energy, especially in specialized neuromorphic hardware, because they use sparse encoding and fewer spikes, which reduces energy consumption (e.g., Intel Loihi).

2. **Efficient Computation**: Unlike traditional neurons that perform multiply-accumulate (MAC) operations, spiking neurons in neuromorphic systems use binary input and only perform addition, which simplifies computations and reduces power usage.

3. **Event-Based Vision**: Neuromorphic computing is particularly useful in vision systems, such as event cameras, which mimic the way biological systems process visual information. These systems can efficiently handle dynamic scenes and reduce data redundancy by only recording changes in the visual field.

Overall, the biological inspiration of neuromorphic computing leads to systems that are more efficient in terms of energy and computation, making them suitable for applications in areas like robotics, sensory processing, and real-time data analysis.","Neuromorphic computing, inspired by the human brain, offers practical applications through its ability to process information in a manner similar to biological neurons. This biological inspiration influences its implementation by enabling the development of systems that can efficiently handle tasks such as pattern recognition and adaptive learning, as detailed in resources like 'Computing with spiking neuron networks' and discussions on the convergence of deep learning with biological systems.",0.0,0.6666666666666666,0.7333333333333333,0.29,0.1428571426530612,0.9435087801651508
What are the practical applications of neuromorphic systems that leverage biological inspiration and advanced training methods for efficient computation?,"['Questions?\n\nNeuromorphic computing\nWhat is the advantage for applications?\n-\nLess spikes = less energy consumption in specialized neuromorphic hardware\n(e.g. Intel Loihi [12])\nBrains are energy efficient: \n2. Sparse encoding\n\nMultiply-accumulate (MAC) operations:\nNormal neuron: Multiplies input with \nweights, then adds.\nSpiking neuron: Consider binary input (e.g. input currents are piecewise \nconstant and assume values {0,1}). There is no multiplication, only addition.', 'FNN‚Äôs are conventional feed-forward neural networks.\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.\nComputing energy consumption\nQuestions?\n\nReading materials\nMain reading:\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n- Neuromorphic computing:\n- Based on biology: Zenke F, Boht√© SM, Clopath C, Com≈üa IM, G√∂ltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \nhttps://www.nature.com/articles/s41586-019-1677-2\nExtra reading:\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \nAcademy of Sciences. 2020.', 'Oversimplification\nThere are also multiple other advantages‚Ä¶\n\nHuman vs. computer computation\n- Fast real-time decision making, e.g. sports, e-sports\n- Adaptive, e.g. context-aware and employs selective attention\n- Energy efficient: Close to 100 billion neurons in the brain\n- Robust, for example to changes in illumination or obstructions in object tracking\n\nHuman vs. computer computation\nBrains are energy efficient: Why?\n1.\nHigh temporal resolution (more computation with less neurons)\n2.\nSparse encoding\n\nHuman vs. computer computation\nQuestions?\nBrains are energy efficient: Why?']","['<1-hop>\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\n\nReading materials\nMain reading:\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n- Neuromorphic computing:\n- Based on biology: Zenke F, Boht√© SM, Clopath C, Com≈üa IM, G√∂ltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \nhttps://www.nature.com/articles/s41586-019-1677-2\nExtra reading:\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\n- Also an important part of neuromorphic systems and vision ‚Üí Event Cameras: Gallego G, Delbr√ºck T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\nBasics of \'conventional\' neural networks:\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.', '<1-hop>\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\n\nReading materials\nMain reading:\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n- Neuromorphic computing:\n- Based on biology: Zenke F, Boht√© SM, Clopath C, Com≈üa IM, G√∂ltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \nhttps://www.nature.com/articles/s41586-019-1677-2\nExtra reading:\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\n- Also an important part of neuromorphic systems and vision ‚Üí Event Cameras: Gallego G, Delbr√ºck T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\nBasics of \'conventional\' neural networks:\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.', '<1-hop>\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\n\nReading materials\nMain reading:\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n- Neuromorphic computing:\n- Based on biology: Zenke F, Boht√© SM, Clopath C, Com≈üa IM, G√∂ltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \nhttps://www.nature.com/articles/s41586-019-1677-2\nExtra reading:\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\n- Also an important part of neuromorphic systems and vision ‚Üí Event Cameras: Gallego G, Delbr√ºck T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\nBasics of \'conventional\' neural networks:\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.']","Neuromorphic systems, inspired by biological processes and advanced training methods, have several practical applications due to their efficient computation capabilities. These include:

1. **Energy Efficiency**: Neuromorphic systems, such as those using spiking neural networks (SNNs), are significantly more energy-efficient than conventional feed-forward neural networks (FNNs). This makes them suitable for applications where power consumption is a critical factor.

2. **Real-Time Decision Making**: They are capable of fast real-time decision making, which is beneficial in applications like sports and e-sports, where quick responses are crucial.

3. **Adaptive Systems**: Neuromorphic systems can be context-aware and employ selective attention, making them useful in adaptive applications that require dynamic responses to changing environments.

4. **Robustness**: These systems are robust to changes, such as variations in illumination or obstructions in object tracking, making them ideal for vision and tracking applications.

5. **Sparse Encoding**: By leveraging sparse encoding, neuromorphic systems can perform high temporal resolution computations with fewer neurons, which is advantageous in applications requiring efficient data processing.

Overall, neuromorphic systems are well-suited for applications that require efficient, adaptive, and robust computation with low energy consumption.","Neuromorphic systems, inspired by the human brain, have practical applications such as event-based vision, which is detailed in the reading materials. These systems use advanced training methods like surrogate gradient learning to optimize spiking neural networks, enabling efficient computation and real-world applications.",0.49999999995,1.0,0.875,0.29,0.1249999998437499,0.960944131521646
"What is the biological basis of neuromorphic computing, and how does it differ from conventional neural networks?","['https://edu.nl/64gpk\nedu.nl/64gpk\n\n\nNeuromorphic computing\nCS4575\nSustainable Software Engineering\n05.03.2025\nNergis T√∂men\n\nIntroduction\nComputer Vision Lab\nBiomorphic Intelligence Lab\nBiomedical Intervention Optimisation Lab\nNergis\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.', ""Computation with spiking neurons\n\nCoincidence detection\nCan be trivially computed with a single spiking neuron! Requires at least n/log(n+1) hidden \nunits for a perceptron (proof in [7]).\nBrains are energy efficient: \n1.\nHigh temporal resolution (more computation with less neurons)\nComputation with spiking neurons\n\nEncoding strategies\nWe considered single neurons with Boolean output (‚Äòspike‚Äô=1 or ‚Äòno spike‚Äô=0).\nHow should we encode information about 'features' in a large network with many spikes?"", 'Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n- Neuromorphic computing:\n- Based on biology: Zenke F, Boht√© SM, Clopath C, Com≈üa IM, G√∂ltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \nneuroscience and neuromorphic engineering. Neuron. 2021.']","['<1-hop>\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\n\nReading materials\nMain reading:\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n- Neuromorphic computing:\n- Based on biology: Zenke F, Boht√© SM, Clopath C, Com≈üa IM, G√∂ltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \nhttps://www.nature.com/articles/s41586-019-1677-2\nExtra reading:\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\n- Also an important part of neuromorphic systems and vision ‚Üí Event Cameras: Gallego G, Delbr√ºck T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\nBasics of \'conventional\' neural networks:\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.']","Neuromorphic computing is inspired by the structure and function of the human brain. It involves using physical artificial neurons to perform computations, mimicking the way biological neurons operate. This approach is based on biology, as it seeks to replicate the brain's energy efficiency and high temporal resolution, allowing for more computation with fewer neurons.

In contrast, conventional neural networks, while also inspired by the brain, typically use mathematical models to simulate neuron behavior and are not necessarily designed to mimic the brain's physical structure or energy efficiency. Neuromorphic computing often involves spiking neurons, which encode information using spikes (binary signals), whereas conventional neural networks generally use continuous values for neuron outputs.","Neuromorphic computing is based on the biological principles of the human brain, utilizing physical artificial neurons to perform computations. Unlike conventional neural networks, which rely on traditional computing architectures, neuromorphic systems are designed to mimic the brain's structure and function, enabling more efficient and biologically inspired processing.",0.8333333332916666,0.5,0.5714285714285714,0.77,0.1666666663888888,0.9354184907309844
What is the difference between spiking neural networks and spiking neuron networks?,"['Neural networks. 1997. 10(9):1659-71. \nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012).', '9 pJ and M = 3.7 pJ).\nNormal neuron: nin x nout multiplications,\n(nin‚Äì 1) x nout additions\nSpiking neuron: 0 multiplications, (nactive‚Äì 1) x nout \nadditions, with nactive ‚â§ nin\nEnergy consumption:\nEnormal = M nin nout + A (nin‚Äì 1) nout = 17.5 pJ\nEnergy consumption:\nEspiking = A (nactive‚Äì 1) nout = 2.7 pJ\nMultiply-accumulate (MAC) operations:\nSNN challenge: how to \ncompute with the least \namount of spikes!', ""‚Ä¶ with energy costs reaching 200M USD!\n\nPower-efficiency at scale\nYour brain runs on:\n\nPower-efficiency at scale\nYour brain runs on:\nHigh estimate ~3000 kcal a day\n   ‚âà145 Watts\n* Human brain has ~600 trillion synapses \n(‚âàparameters).\n\nPower-efficiency at scale\nYour brain runs on:\nHigh estimate ~3000 kcal a day\n   ‚âà145 Watts\n* Human brain has ~600 trillion synapses \n(‚âàparameters).\nOversimplification\nThere are also multiple other advantages‚Ä¶\n\nHuman vs. computer computation\n- Fast real-time decision making, e.g. sports, e-sports\n- Adaptive, e.g. context-aware and employs selective attention\n- Energy efficient: Close to 100 billion neurons in the brain\n- Robust, for example to changes in illumination or obstructions in object tracking\n\nHuman vs. computer computation\nBrains are energy efficient: Why?\n1.\nHigh temporal resolution (more computation with less neurons)\n2.\nSparse encoding\n\nHuman vs. computer computation\nQuestions?\nBrains are energy efficient: Why?\n1.\nHigh temporal resolution (more computation with less neurons)\n2.\nSparse encoding\n\nHow do biological neurons communicate?\nAnalogy to artificial neural networks\nReal neuron\nArtificial neuron\nCurrent\n\nHow do biological neurons communicate?\nHow does the electrical activity propagate?\n?\nCurrent\n\nHow do biological neurons work?\ntime\nelectrode\nhttps://tinmard.github.io/spike-sorting-animation.html\nQuick electrical pulses trigger chemical signals for the next neuron ‚Üí Spikes\n\nHow do biological neurons work?\nQuick electrical pulses trigger chemical signals for the next neuron ‚Üí Spikes\nhttps://tinmard.github.io/spike-sorting-animation.html\ntime\nelectrode\nFrom CS perspective:\nSparse binary \nencoding\n\nBiologically realistic spiking neuron models\nBiologically realistic neuron models have \na new dimension: Time!\nSpiking neural networks (SNNs):\nThe input x  (t  ) to each neuron is summed \n(integrated) over time.\nscalar\nfunction of \ntime\n(t)\n(t)\nhttps://www.mdpi.com/2076-3425/12/7/863\n\nHow to model spiking neurons?\nANN: Perceptron, threshold activation function:\n\nHow to model spiking neurons?\nANN: Perceptron, threshold activation function:\n\nHow to model spiking neurons?\nSpiking neural network (SNN): The 'input current' Œµ(t) is integrated over time.\nspike\n\nHow to model spiking neurons?\nEquivalence to perceptron: Computation at least as complex as a perceptron.\n\nNon-leaky integrate-and-fire (IF) neuron\nThe temporal profile of the input current Œµ(t) can be chosen differently, for different computations.\n\nQuestions?\n\nComputation with spiking neurons\nCoincidence detection\n\nComputation with spiking neurons\nCoincidence detection\nx, y ‚àà {0, 1}n\nExample: n = 2\ninput = \nx1\nx2\ny1\ny2\n= \n0\n1\n1\n1\noutput? \n\nCoincidence detection\nComputation with spiking neurons\n\nCoincidence detection\nCan be trivially computed with a single spiking neuron! Requires at least n/log(n+1) hidden \nunits for a perceptron (proof in [7]).\nComputation with spiking neurons\n\nCoincidence detection\nCan be trivially computed with a single spiking neuron! Requires at least n/log(n+1) hidden \nunits for a perceptron (proof in [7]).\nBrains are energy efficient: \n1.\nHigh temporal resolution (more computation with less neurons)\nComputation with spiking neurons\n\nEncoding strategies\nWe considered single neurons with Boolean output (‚Äòspike‚Äô=1 or ‚Äòno spike‚Äô=0).\nHow should we encode information about 'features' in a large network with many spikes?\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\nElephant?\nhttps://www.nationalgeographic.com/anim\nals/mammals/facts/african-elephant\n\nFiring rates\nClassical view of the brain:\n- Each neuron is selective for one specific \nfeature in the input.\n- Higher firing rate (spikes per unit time) for \n'selected' feature.\nAdapted from Hubel & Wiesel, 1959\n\nFiring rates\nClassical view of the brain:\n- Each neuron is selective for one specific \nfeature in the input.\n- Higher firing rate (spikes per unit time) for \n'selected' feature.\n- Link to modern ANNs: The scalar output of \nan artificial neuron is interpreted as the firing \nrate.\nAdapted from Hubel & Wiesel, 1959\n\nFiring rates\nClassical view of the brain:\n- Each neuron is selective for one specific \nfeature in the input.\n- Higher firing rate (spikes per unit time) for \n'selected' feature.\n- Link to modern ANNs: The scalar output of \nan artificial neuron is interpreted as the firing \nrate.\nAdapted from Hubel & Wiesel, 1959\nBut rate coding is inefficient and slow‚Ä¶\n(i.e. each neuron needs to fire many spikes to get good precision)\nboth in vivo and in silico.\n\nDifferent encoding strategies with spiking neurons [8]\n7 neurons\n7 time bins\n\nDifferent encoding strategies with spiking neurons\n7 neurons\n7 time bins\nn\n6\n(in the general \ncase a \nnumber \nbetween 0-7)\n\nDifferent encoding strategies with spiking neurons\n7 neurons\n7 time bins\nn\nTotal spike \ncount can \nencode 3 bits.\n\nDifferent encoding strategies with spiking neurons\n7 neurons\n7 time bins\nIndividual rates \ncan encode 7 \nbits.\n\nDifferent encoding strategies with spiking neurons\n7 neurons\n7 time bins\nLatency can \nencode ~3*7 or \n~19 bits.\n\nDifferent encoding strategies with spiking neurons\n7 neurons\n7 time bins\nRank order can \nencode ~12 bits.\n\nDifferent encoding strategies with spiking neurons\n\nDifferent encoding strategies with spiking neurons\nUsing latency or spatio-temporal codes\nour encoding is faster and more spike-efficient (sparser)!\n\nDifferent encoding strategies with spiking neurons\nUsing latency or spatio-temporal codes\nour encoding is faster and more spike-efficient (sparser)!\nQuestions?\n\nNeuromorphic computing\nWhat is the advantage for applications?\n-\nLess spikes = less energy consumption in specialized neuromorphic hardware\n(e.g. Intel Loihi [12])\nBrains are energy efficient: \n2. Sparse encoding\n\nMultiply-accumulate (MAC) operations:\nNormal neuron: Multiplies input with \nweights, then adds.\nSpiking neuron: Consider binary input (e.g. input currents are piecewise \nconstant and assume values {0,1}). There is no multiplication, only addition.\n\nAssume one multiplier and one adder circuit uses M and A energy respectively with AÔπ§M\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\nNormal neuron: nin x nout multiplications,\n(nin‚Äì 1) x nout additions\nSpiking neuron: 0 multiplications, (nactive‚Äì 1) x nout \nadditions, with nactive ‚â§ nin\nEnergy consumption:\nEnormal = M nin nout + A (nin‚Äì 1) nout = 17.5 pJ\nEnergy consumption:\nEspiking = A (nactive‚Äì 1) nout = 2.7 pJ\nMultiply-accumulate (MAC) operations:\n\nAssume one multiplier and one adder circuit uses M and A energy respectively with AÔπ§M\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\nNormal neuron: nin x nout multiplications,\n(nin‚Äì 1) x nout additions\nSpiking neuron: 0 multiplications, (nactive‚Äì 1) x nout \nadditions, with nactive ‚â§ nin\nEnergy consumption:\nEnormal = M nin nout + A (nin‚Äì 1) nout = 17.5 pJ\nEnergy consumption:\nEspiking = A (nactive‚Äì 1) nout = 2.7 pJ\nMultiply-accumulate (MAC) operations:\nSNN challenge: how to \ncompute with the least \namount of spikes!\n\nOften, we observe a \nsparsity (energy)-\ntask accuracy trade-off\n(Left: results for image \nclassification)\nSNN challenge: how to \ncompute with the least \namount of spikes!\nhttps://arxiv.org/pdf/2409.08290\n\nIn practice, energy \nconsumption computations \nare complex. \nNeed to take into account\n- memory access,\n- addressing,\n- auxiliary operations,\nin addition to MACs.\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \nand video classification respectively). FNN‚Äôs are conventional feed-forward neural networks.\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.\nhttps://arxiv.org/pdf/2210.13107\nComputing energy consumption\n\nIn practice, energy \nconsumption computations \nare complex. \nNeed to take into account\n- memory access,\n- addressing,\n- auxiliary operations,\nin addition to MACs.\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \nand video classification respectively). FNN‚Äôs are conventional feed-forward neural networks.""]","[""<1-hop>\n\nEncoding strategies\nWe considered single neurons with Boolean output (‚Äòspike‚Äô=1 or ‚Äòno spike‚Äô=0).\nHow should we encode information about 'features' in a large network with many spikes?\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\nElephant?\nhttps://www.nationalgeographic.com/anim\nals/mammals/facts/african-elephant"", '<2-hop>\n\nReferences\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch‚ÄìPitts neural networks. Journal of the History of the Behavioral \nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \n(not publicly available) doi:10.1037/h0042519\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\n6) https://github.com/idsc-frazzoli/retina\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\n10) G√ºtig R, Sompolinsky H. The tempotron: a neuron that learns spike timing‚Äìbased decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568', ""<1-hop>\n\nEncoding strategies\nWe considered single neurons with Boolean output (‚Äòspike‚Äô=1 or ‚Äòno spike‚Äô=0).\nHow should we encode information about 'features' in a large network with many spikes?\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\nElephant?\nhttps://www.nationalgeographic.com/anim\nals/mammals/facts/african-elephant"", '<2-hop>\n\nReferences\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch‚ÄìPitts neural networks. Journal of the History of the Behavioral \nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \n(not publicly available) doi:10.1037/h0042519\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\n6) https://github.com/idsc-frazzoli/retina\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\n10) G√ºtig R, Sompolinsky H. The tempotron: a neuron that learns spike timing‚Äìbased decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568']","The terms ""spiking neural networks"" (SNNs) and ""spiking neuron networks"" are often used interchangeably to refer to the same concept. Both describe a type of artificial neural network that more closely mimics the way biological neurons communicate through spikes or discrete events over time. These networks use spiking neurons, which integrate input over time and fire spikes when a certain threshold is reached, as opposed to traditional artificial neural networks that use continuous activation functions.

The key characteristics of SNNs include the integration of input over time, sparse encoding, and the use of spikes for communication, which can lead to more energy-efficient computations compared to traditional neural networks. They are particularly noted for their potential in neuromorphic computing, where less energy consumption is achieved through fewer spikes and sparse encoding strategies.","Spiking neural networks (SNNs) are a type of neural network model that processes information using spikes, similar to biological neurons. Spiking neuron networks refer to the individual components within these networks, focusing on how each neuron processes and communicates information through spikes.",0.0,,0.8181818181818182,0.6,0.9999999900000002,0.904073994791426
What is the foundational concepts of spiking neural networks and how they are used in encoding strategies?,"[""‚Ä¶ with energy costs reaching 200M USD!\n\nPower-efficiency at scale\nYour brain runs on:\n\nPower-efficiency at scale\nYour brain runs on:\nHigh estimate ~3000 kcal a day\n   ‚âà145 Watts\n* Human brain has ~600 trillion synapses \n(‚âàparameters).\n\nPower-efficiency at scale\nYour brain runs on:\nHigh estimate ~3000 kcal a day\n   ‚âà145 Watts\n* Human brain has ~600 trillion synapses \n(‚âàparameters).\nOversimplification\nThere are also multiple other advantages‚Ä¶\n\nHuman vs. computer computation\n- Fast real-time decision making, e.g. sports, e-sports\n- Adaptive, e.g. context-aware and employs selective attention\n- Energy efficient: Close to 100 billion neurons in the brain\n- Robust, for example to changes in illumination or obstructions in object tracking\n\nHuman vs. computer computation\nBrains are energy efficient: Why?\n1.\nHigh temporal resolution (more computation with less neurons)\n2.\nSparse encoding\n\nHuman vs. computer computation\nQuestions?\nBrains are energy efficient: Why?\n1.\nHigh temporal resolution (more computation with less neurons)\n2.\nSparse encoding\n\nHow do biological neurons communicate?\nAnalogy to artificial neural networks\nReal neuron\nArtificial neuron\nCurrent\n\nHow do biological neurons communicate?\nHow does the electrical activity propagate?\n?\nCurrent\n\nHow do biological neurons work?\ntime\nelectrode\nhttps://tinmard.github.io/spike-sorting-animation.html\nQuick electrical pulses trigger chemical signals for the next neuron ‚Üí Spikes\n\nHow do biological neurons work?\nQuick electrical pulses trigger chemical signals for the next neuron ‚Üí Spikes\nhttps://tinmard.github.io/spike-sorting-animation.html\ntime\nelectrode\nFrom CS perspective:\nSparse binary \nencoding\n\nBiologically realistic spiking neuron models\nBiologically realistic neuron models have \na new dimension: Time!\nSpiking neural networks (SNNs):\nThe input x  (t  ) to each neuron is summed \n(integrated) over time.\nscalar\nfunction of \ntime\n(t)\n(t)\nhttps://www.mdpi.com/2076-3425/12/7/863\n\nHow to model spiking neurons?\nANN: Perceptron, threshold activation function:\n\nHow to model spiking neurons?\nANN: Perceptron, threshold activation function:\n\nHow to model spiking neurons?\nSpiking neural network (SNN): The 'input current' Œµ(t) is integrated over time.\nspike\n\nHow to model spiking neurons?\nEquivalence to perceptron: Computation at least as complex as a perceptron.\n\nNon-leaky integrate-and-fire (IF) neuron\nThe temporal profile of the input current Œµ(t) can be chosen differently, for different computations.\n\nQuestions?\n\nComputation with spiking neurons\nCoincidence detection\n\nComputation with spiking neurons\nCoincidence detection\nx, y ‚àà {0, 1}n\nExample: n = 2\ninput = \nx1\nx2\ny1\ny2\n= \n0\n1\n1\n1\noutput? \n\nCoincidence detection\nComputation with spiking neurons\n\nCoincidence detection\nCan be trivially computed with a single spiking neuron! Requires at least n/log(n+1) hidden \nunits for a perceptron (proof in [7]).\nComputation with spiking neurons\n\nCoincidence detection\nCan be trivially computed with a single spiking neuron! Requires at least n/log(n+1) hidden \nunits for a perceptron (proof in [7]).\nBrains are energy efficient: \n1.\nHigh temporal resolution (more computation with less neurons)\nComputation with spiking neurons\n\nEncoding strategies\nWe considered single neurons with Boolean output (‚Äòspike‚Äô=1 or ‚Äòno spike‚Äô=0).\nHow should we encode information about 'features' in a large network with many spikes?\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\nElephant?\nhttps://www.nationalgeographic.com/anim\nals/mammals/facts/african-elephant\n\nFiring rates\nClassical view of the brain:\n- Each neuron is selective for one specific \nfeature in the input.\n- Higher firing rate (spikes per unit time) for \n'selected' feature.\nAdapted from Hubel & Wiesel, 1959\n\nFiring rates\nClassical view of the brain:\n- Each neuron is selective for one specific \nfeature in the input.\n- Higher firing rate (spikes per unit time) for \n'selected' feature.\n- Link to modern ANNs: The scalar output of \nan artificial neuron is interpreted as the firing \nrate.\nAdapted from Hubel & Wiesel, 1959\n\nFiring rates\nClassical view of the brain:\n- Each neuron is selective for one specific \nfeature in the input.\n- Higher firing rate (spikes per unit time) for \n'selected' feature.\n- Link to modern ANNs: The scalar output of \nan artificial neuron is interpreted as the firing \nrate.\nAdapted from Hubel & Wiesel, 1959\nBut rate coding is inefficient and slow‚Ä¶\n(i.e. each neuron needs to fire many spikes to get good precision)\nboth in vivo and in silico.\n\nDifferent encoding strategies with spiking neurons [8]\n7 neurons\n7 time bins\n\nDifferent encoding strategies with spiking neurons\n7 neurons\n7 time bins\nn\n6\n(in the general \ncase a \nnumber \nbetween 0-7)\n\nDifferent encoding strategies with spiking neurons\n7 neurons\n7 time bins\nn\nTotal spike \ncount can \nencode 3 bits.\n\nDifferent encoding strategies with spiking neurons\n7 neurons\n7 time bins\nIndividual rates \ncan encode 7 \nbits.\n\nDifferent encoding strategies with spiking neurons\n7 neurons\n7 time bins\nLatency can \nencode ~3*7 or \n~19 bits.\n\nDifferent encoding strategies with spiking neurons\n7 neurons\n7 time bins\nRank order can \nencode ~12 bits.\n\nDifferent encoding strategies with spiking neurons\n\nDifferent encoding strategies with spiking neurons\nUsing latency or spatio-temporal codes\nour encoding is faster and more spike-efficient (sparser)!\n\nDifferent encoding strategies with spiking neurons\nUsing latency or spatio-temporal codes\nour encoding is faster and more spike-efficient (sparser)!\nQuestions?\n\nNeuromorphic computing\nWhat is the advantage for applications?\n-\nLess spikes = less energy consumption in specialized neuromorphic hardware\n(e.g. Intel Loihi [12])\nBrains are energy efficient: \n2. Sparse encoding\n\nMultiply-accumulate (MAC) operations:\nNormal neuron: Multiplies input with \nweights, then adds.\nSpiking neuron: Consider binary input (e.g. input currents are piecewise \nconstant and assume values {0,1}). There is no multiplication, only addition.\n\nAssume one multiplier and one adder circuit uses M and A energy respectively with AÔπ§M\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\nNormal neuron: nin x nout multiplications,\n(nin‚Äì 1) x nout additions\nSpiking neuron: 0 multiplications, (nactive‚Äì 1) x nout \nadditions, with nactive ‚â§ nin\nEnergy consumption:\nEnormal = M nin nout + A (nin‚Äì 1) nout = 17.5 pJ\nEnergy consumption:\nEspiking = A (nactive‚Äì 1) nout = 2.7 pJ\nMultiply-accumulate (MAC) operations:\n\nAssume one multiplier and one adder circuit uses M and A energy respectively with AÔπ§M\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\nNormal neuron: nin x nout multiplications,\n(nin‚Äì 1) x nout additions\nSpiking neuron: 0 multiplications, (nactive‚Äì 1) x nout \nadditions, with nactive ‚â§ nin\nEnergy consumption:\nEnormal = M nin nout + A (nin‚Äì 1) nout = 17.5 pJ\nEnergy consumption:\nEspiking = A (nactive‚Äì 1) nout = 2.7 pJ\nMultiply-accumulate (MAC) operations:\nSNN challenge: how to \ncompute with the least \namount of spikes!\n\nOften, we observe a \nsparsity (energy)-\ntask accuracy trade-off\n(Left: results for image \nclassification)\nSNN challenge: how to \ncompute with the least \namount of spikes!\nhttps://arxiv.org/pdf/2409.08290\n\nIn practice, energy \nconsumption computations \nare complex. \nNeed to take into account\n- memory access,\n- addressing,\n- auxiliary operations,\nin addition to MACs.\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \nand video classification respectively). FNN‚Äôs are conventional feed-forward neural networks.\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.\nhttps://arxiv.org/pdf/2210.13107\nComputing energy consumption\n\nIn practice, energy \nconsumption computations \nare complex. \nNeed to take into account\n- memory access,\n- addressing,\n- auxiliary operations,\nin addition to MACs.\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \nand video classification respectively). FNN‚Äôs are conventional feed-forward neural networks."", 'Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\n6) https://github.com/idsc-frazzoli/retina\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\n10) G√ºtig R, Sompolinsky H. The tempotron: a neuron that learns spike timing‚Äìbased decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945.']","[""<1-hop>\n\nEncoding strategies\nWe considered single neurons with Boolean output (‚Äòspike‚Äô=1 or ‚Äòno spike‚Äô=0).\nHow should we encode information about 'features' in a large network with many spikes?\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\nElephant?\nhttps://www.nationalgeographic.com/anim\nals/mammals/facts/african-elephant"", '<2-hop>\n\nReferences\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch‚ÄìPitts neural networks. Journal of the History of the Behavioral \nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \n(not publicly available) doi:10.1037/h0042519\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\n6) https://github.com/idsc-frazzoli/retina\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\n10) G√ºtig R, Sompolinsky H. The tempotron: a neuron that learns spike timing‚Äìbased decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568', ""<1-hop>\n\nEncoding strategies\nWe considered single neurons with Boolean output (‚Äòspike‚Äô=1 or ‚Äòno spike‚Äô=0).\nHow should we encode information about 'features' in a large network with many spikes?\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\nElephant?\nhttps://www.nationalgeographic.com/anim\nals/mammals/facts/african-elephant"", '<2-hop>\n\nReferences\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch‚ÄìPitts neural networks. Journal of the History of the Behavioral \nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \n(not publicly available) doi:10.1037/h0042519\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\n6) https://github.com/idsc-frazzoli/retina\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\n10) G√ºtig R, Sompolinsky H. The tempotron: a neuron that learns spike timing‚Äìbased decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568', ""<1-hop>\n\nEncoding strategies\nWe considered single neurons with Boolean output (‚Äòspike‚Äô=1 or ‚Äòno spike‚Äô=0).\nHow should we encode information about 'features' in a large network with many spikes?\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\nElephant?\nhttps://www.nationalgeographic.com/anim\nals/mammals/facts/african-elephant"", '<2-hop>\n\nReferences\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch‚ÄìPitts neural networks. Journal of the History of the Behavioral \nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \n(not publicly available) doi:10.1037/h0042519\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\n6) https://github.com/idsc-frazzoli/retina\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\n10) G√ºtig R, Sompolinsky H. The tempotron: a neuron that learns spike timing‚Äìbased decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568', ""<1-hop>\n\nEncoding strategies\nWe considered single neurons with Boolean output (‚Äòspike‚Äô=1 or ‚Äòno spike‚Äô=0).\nHow should we encode information about 'features' in a large network with many spikes?\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\nElephant?\nhttps://www.nationalgeographic.com/anim\nals/mammals/facts/african-elephant"", '<2-hop>\n\nReferences\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch‚ÄìPitts neural networks. Journal of the History of the Behavioral \nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \n(not publicly available) doi:10.1037/h0042519\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\n6) https://github.com/idsc-frazzoli/retina\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\n10) G√ºtig R, Sompolinsky H. The tempotron: a neuron that learns spike timing‚Äìbased decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568']","Spiking Neural Networks (SNNs) are a type of neural network that mimic the way biological neurons communicate. The foundational concept of SNNs involves the use of spikes, or quick electrical pulses, to transmit information. Unlike traditional artificial neural networks (ANNs) that use continuous values, SNNs use discrete events (spikes) to represent and process information over time.

Key foundational concepts of SNNs include:

1. **Temporal Dynamics**: SNNs incorporate the dimension of time, where the input to each neuron is integrated over time. This allows for the modeling of temporal patterns and dynamics similar to biological neurons.

2. **Sparse Encoding**: SNNs use sparse encoding strategies, which means fewer spikes are used to represent information. This leads to energy efficiency, as less energy is consumed in processing fewer spikes.

3. **Coincidence Detection**: SNNs can perform computations such as coincidence detection, which can be efficiently computed with a single spiking neuron.

In terms of encoding strategies, SNNs utilize various methods to encode information:

- **Firing Rates**: Each neuron is selective for specific features, and the firing rate (spikes per unit time) increases for selected features. However, this method can be inefficient and slow.

- **Latency and Spatio-Temporal Codes**: These methods use the timing of spikes to encode information, which is faster and more spike-efficient compared to rate coding.

- **Rank Order and Total Spike Count**: These strategies involve encoding information based on the order of spikes or the total number of spikes, allowing for efficient information representation.

Overall, SNNs are designed to be more energy-efficient and capable of processing temporal information effectively, making them suitable for applications in neuromorphic computing.","Spiking neural networks use spikes to encode information, with foundational concepts including the work of McCulloch and Pitts on logical calculus of nervous activity, and later developments in spiking neuron models.",0.0,0.0,1.0,0.29,,0.9412788113698402
What is the project idea about government-developed OS software?,"['Dataset of government-developed OS software\n‚Ä¢ Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise \n‚Ä¢ But studying government software is difficult b/c we don‚Äôt know what is out \nthere\u2028\n‚Üí Lack of incentive to make popular \u2028\n‚Üí Language barriers \n‚Ä¢ Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation? \n‚Ä¢ Could start with NL, but including your / other countries greatly appreciated!\n\nB7.', '‚Ä¢ Description of the solution. \n‚Ä¢ Validation of the solution (if applicable -> discuss with coach) \n‚Ä¢ Discussion of the solution. (Including limitations)\n5\n\nStrategy\n‚Ä¢ Starting next week, there are no lectures  \n‚Ä¢ Steering meetings from week 5 till week 9 (either online or in person).  \n‚Ä¢ 1 steering meeting per week. (4+1 sprints) \n‚Ä¢ Every week, you need to plan different tasks and assignments. \n‚Ä¢ Deadline April 4.\n6\n\nStrategy\n‚Ä¢ Week 0 \n‚Ä¢ Decide project idea (today) \n‚Ä¢ Define steering meeting schedule \n‚Ä¢ Create working document of the \narticle: Problem statement and solution \nproposal! \n‚Ä¢ Define and assign tasks for each week. \n‚Ä¢ Week 1 \n‚Ä¢ Implementation \n‚Ä¢ Agreements with supervisor. \n‚Ä¢ Week 2 \n‚Ä¢ Implementation \n‚Ä¢ Week 3 \n‚Ä¢ Implementation, Full draft of article, \ndissemination. \n‚Ä¢ Week 4 \n‚Ä¢ Final refinements \n‚Ä¢ Prepare presentation\n7\n\nProject ideas\n‚Ä¢ A1. Prototype cross-machine comparable benchmarks \n‚Ä¢ A2. Add energy metrics to LMstudio/Ollama \n‚Ä¢ A3. Visualizations built-in with EnergiBridge \n‚Ä¢ A4. Service-based version of EnergiBridge\n\nProject ideas\n‚Ä¢ B1. Measure energy consumption of single JUnit tests \n‚Ä¢ B2. Study test generation energy consumption \n‚Ä¢ B3. Energy profiling of static analysis tools \n‚Ä¢ B4. Detailed energy profiling of build pipelines \n‚Ä¢ B5. Tool supporting SusAF workshop / process \n‚Ä¢ B6. Dataset of government-developed OS software \n‚Ä¢ B7. Queue - but better for the student / TA society\nQuality Assurance & Testing\nSocial & Individual \u2028\nSustainability\n\nProject ideas\n‚Ä¢ C1. Compare energy consumption of docker images for ML workloads. \n‚Ä¢ C2. Create a plugin to visualize Hugging Face carbon emissions in detail. \n‚Ä¢ C3. Plugin for ChatGPT (footprint per chat window) \n‚Ä¢ C4. NutriScore for software libraries. \n‚Ä¢ C5. Add energy-awareness to existing software \n‚Ä¢ C6. Green Shift Left \n‚Ä¢ C7. Education for Sustainable SE\n\nA1.']","['<1-hop>\n\nB6. Dataset of government-developed OS software\n‚Ä¢ Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise\n‚Ä¢ But studying government software is diÔ¨Écult b/c we don‚Äôt know what is out \nthere\u2028\n‚Üí Lack of incentive to make popular \u2028\n‚Üí Language barriers\n‚Ä¢ Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation?\n‚Ä¢ Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\n\nProject ideas\n‚Ä¢ B1. Measure energy consumption of single JUnit tests \n‚Ä¢ B2. Study test generation energy consumption\n‚Ä¢ B3. Energy proÔ¨Åling of static analysis tools\n‚Ä¢ B4. Detailed energy proÔ¨Åling of build pipelines\n‚Ä¢ B5. Tool supporting SusAF workshop / process\n‚Ä¢ B6. Dataset of government-developed OS software\n‚Ä¢ B7. Queue - but better for the student / TA society\nQuality Assurance & Testing\nSocial & Individual \u2028\nSustainability']","The project idea about government-developed OS software involves creating a comprehensive dataset of software developed and used by governments to support society. The focus is on open-source development and policies, which are on the rise. The project aims to include data that helps understand the state of this software, such as whether it is buildable, has an open development history, and includes requirements documentation. The initiative could start with the Netherlands (NL), but contributions from other countries are also appreciated. The project addresses challenges like the lack of incentive to make such software popular and language barriers.","The project idea involves creating a comprehensive dataset of government-developed OS software to understand its state, including buildability, open development history, and requirements documentation. This addresses challenges like lack of incentives and language barriers.",0.99999999995,1.0,0.8461538461538461,0.57,0.0,0.9999999999999996
What is the project idea related to creating a dataset of goverment-developed OS software?,"['Dataset of government-developed OS software\n‚Ä¢ Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise \n‚Ä¢ But studying government software is difficult b/c we don‚Äôt know what is out \nthere\u2028\n‚Üí Lack of incentive to make popular \u2028\n‚Üí Language barriers \n‚Ä¢ Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation? \n‚Ä¢ Could start with NL, but including your / other countries greatly appreciated!\n\nB7.', 'Ideas\n\nAssignment\n‚Ä¢ Goal: Solve a Sustainable Software Engineering problem. \n‚Ä¢ Identify 1 problem that should be fixed to help enabling sustainability in \nthe software engineering industry/community. \n‚Ä¢ Propose a solution. A tool, framework, guidelines, etc. \n‚Ä¢ Implementation. \n‚Ä¢ Validation. (Depending on the idea) \n‚Ä¢ Dissemination/social impact. (Solution should be open source, welcome \ncontributors, post on social media? Tool website? Available in a package \nmanager?)\n3\n\nDeliverables\n‚Ä¢ Paper-like article.']","['<1-hop>\n\nB6. Dataset of government-developed OS software\n‚Ä¢ Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise\n‚Ä¢ But studying government software is diÔ¨Écult b/c we don‚Äôt know what is out \nthere\u2028\n‚Üí Lack of incentive to make popular \u2028\n‚Üí Language barriers\n‚Ä¢ Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation?\n‚Ä¢ Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\n\nProject ideas\n‚Ä¢ B1. Measure energy consumption of single JUnit tests \n‚Ä¢ B2. Study test generation energy consumption\n‚Ä¢ B3. Energy proÔ¨Åling of static analysis tools\n‚Ä¢ B4. Detailed energy proÔ¨Åling of build pipelines\n‚Ä¢ B5. Tool supporting SusAF workshop / process\n‚Ä¢ B6. Dataset of government-developed OS software\n‚Ä¢ B7. Queue - but better for the student / TA society\nQuality Assurance & Testing\nSocial & Individual \u2028\nSustainability', '<1-hop>\n\nB6. Dataset of government-developed OS software\n‚Ä¢ Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise\n‚Ä¢ But studying government software is diÔ¨Écult b/c we don‚Äôt know what is out \nthere\u2028\n‚Üí Lack of incentive to make popular \u2028\n‚Üí Language barriers\n‚Ä¢ Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation?\n‚Ä¢ Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\n\nProject ideas\n‚Ä¢ B1. Measure energy consumption of single JUnit tests \n‚Ä¢ B2. Study test generation energy consumption\n‚Ä¢ B3. Energy proÔ¨Åling of static analysis tools\n‚Ä¢ B4. Detailed energy proÔ¨Åling of build pipelines\n‚Ä¢ B5. Tool supporting SusAF workshop / process\n‚Ä¢ B6. Dataset of government-developed OS software\n‚Ä¢ B7. Queue - but better for the student / TA society\nQuality Assurance & Testing\nSocial & Individual \u2028\nSustainability', '<1-hop>\n\nB6. Dataset of government-developed OS software\n‚Ä¢ Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise\n‚Ä¢ But studying government software is diÔ¨Écult b/c we don‚Äôt know what is out \nthere\u2028\n‚Üí Lack of incentive to make popular \u2028\n‚Üí Language barriers\n‚Ä¢ Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation?\n‚Ä¢ Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\n\nProject ideas\n‚Ä¢ B1. Measure energy consumption of single JUnit tests \n‚Ä¢ B2. Study test generation energy consumption\n‚Ä¢ B3. Energy proÔ¨Åling of static analysis tools\n‚Ä¢ B4. Detailed energy proÔ¨Åling of build pipelines\n‚Ä¢ B5. Tool supporting SusAF workshop / process\n‚Ä¢ B6. Dataset of government-developed OS software\n‚Ä¢ B7. Queue - but better for the student / TA society\nQuality Assurance & Testing\nSocial & Individual \u2028\nSustainability', '<1-hop>\n\nB6. Dataset of government-developed OS software\n‚Ä¢ Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise\n‚Ä¢ But studying government software is diÔ¨Écult b/c we don‚Äôt know what is out \nthere\u2028\n‚Üí Lack of incentive to make popular \u2028\n‚Üí Language barriers\n‚Ä¢ Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation?\n‚Ä¢ Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\n\nProject ideas\n‚Ä¢ B1. Measure energy consumption of single JUnit tests \n‚Ä¢ B2. Study test generation energy consumption\n‚Ä¢ B3. Energy proÔ¨Åling of static analysis tools\n‚Ä¢ B4. Detailed energy proÔ¨Åling of build pipelines\n‚Ä¢ B5. Tool supporting SusAF workshop / process\n‚Ä¢ B6. Dataset of government-developed OS software\n‚Ä¢ B7. Queue - but better for the student / TA society\nQuality Assurance & Testing\nSocial & Individual \u2028\nSustainability', '<1-hop>\n\nB6. Dataset of government-developed OS software\n‚Ä¢ Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise\n‚Ä¢ But studying government software is diÔ¨Écult b/c we don‚Äôt know what is out \nthere\u2028\n‚Üí Lack of incentive to make popular \u2028\n‚Üí Language barriers\n‚Ä¢ Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation?\n‚Ä¢ Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\n\nProject ideas\n‚Ä¢ B1. Measure energy consumption of single JUnit tests \n‚Ä¢ B2. Study test generation energy consumption\n‚Ä¢ B3. Energy proÔ¨Åling of static analysis tools\n‚Ä¢ B4. Detailed energy proÔ¨Åling of build pipelines\n‚Ä¢ B5. Tool supporting SusAF workshop / process\n‚Ä¢ B6. Dataset of government-developed OS software\n‚Ä¢ B7. Queue - but better for the student / TA society\nQuality Assurance & Testing\nSocial & Individual \u2028\nSustainability']","The project idea involves creating a comprehensive dataset of government-developed open-source software. This dataset would include data necessary to understand the state of the software, such as whether it is buildable, has an open development history, and includes requirements documentation. The project acknowledges the challenges in studying government software due to a lack of incentive to make it popular and language barriers. It suggests starting with the Netherlands but welcomes the inclusion of data from other countries.","The project idea is to create a dataset of government-developed OS software, which would help in understanding the state of such software, including whether it is buildable, has open development history, and has requirements documentation.",0.9999999999,1.0,0.7,0.67,0.0,0.9368846304674228
What is the project idea related to creating a dataset of government-developed OS software?,"['Dataset of government-developed OS software\n‚Ä¢ Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise \n‚Ä¢ But studying government software is difficult b/c we don‚Äôt know what is out \nthere\u2028\n‚Üí Lack of incentive to make popular \u2028\n‚Üí Language barriers \n‚Ä¢ Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation? \n‚Ä¢ Could start with NL, but including your / other countries greatly appreciated!\n\nB7.', 'Ideas\n\nAssignment\n‚Ä¢ Goal: Solve a Sustainable Software Engineering problem. \n‚Ä¢ Identify 1 problem that should be fixed to help enabling sustainability in \nthe software engineering industry/community. \n‚Ä¢ Propose a solution. A tool, framework, guidelines, etc. \n‚Ä¢ Implementation. \n‚Ä¢ Validation. (Depending on the idea) \n‚Ä¢ Dissemination/social impact. (Solution should be open source, welcome \ncontributors, post on social media? Tool website? Available in a package \nmanager?)\n3\n\nDeliverables\n‚Ä¢ Paper-like article.', 'Browser plugin?\n24\nCO2\n\nC4. NutriScore for software libraries\n25\n‚Ä¢ NutriScore labels are not perfect but they are a good starting point! \n‚Ä¢ What if we could do the same thing for the energy efficiency of software. \n‚Ä¢ (Also open to individual or social sustainability) \n‚Ä¢ This work can be scoped in particular domains/ecossystems/use cases. \n‚Ä¢ Libraries for stats? ML? Web Dev? Cloud?\n\nC5. Add energy-awareness to existing software\n26\n‚Ä¢ Streamlit, Notebooks, JSFiddle, \nPyScript, zsh, etc. \n‚Ä¢\n\nC6 - Green Shift Left\n‚Ä¢ Estimate energy efficiency using static code analysis. \n‚Ä¢ We don‚Äôt need an accurate value. \n‚Ä¢ It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention. \n‚Ä¢ Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)\n27\n\nC7 - Education\n‚Ä¢ Educational game for Software \nSustainability practices \n‚Ä¢ Purpose: use within software teams to \ndiscuss or learn about different sustainable \nIT practices: at the organisation level, \nsoftware, etc.\n28\nhttps://github.com/OttoKaaij/Ticket-To-Sustainability/?tab=readme-ov-file\n\nProject ideas (old)\n‚Ä¢ Plugin from EnergiBridge (GUI, report generator, python library, etc.) \n‚Ä¢ Plugin for ChatGPT (carbon emissions per chat window) \n‚Ä¢ Seamless measurements for AI libraries \n‚Ä¢ Energy patterns for Green AI \n‚Ä¢ Sustainable SW dev gamification \n‚Ä¢ Sustainability auditor for AI projects \n‚Ä¢ Energy Profiling of screen colour filter tools (or display settings) \n‚Ä¢ ‚Ä¶ you can also propose yours!\n\nhttps://edu.nl/64gpk\nedu.nl/64gpk\n\n\nNeuromorphic computing\nCS4575\nSustainable Software Engineering\n05.03.2025\nNergis T√∂men\n\nIntroduction\nComputer Vision Lab\nBiomorphic Intelligence Lab\nBiomedical Intervention Optimisation Lab\nNergis\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.']","['<1-hop>\n\nB6. Dataset of government-developed OS software\n‚Ä¢ Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise\n‚Ä¢ But studying government software is diÔ¨Écult b/c we don‚Äôt know what is out \nthere\u2028\n‚Üí Lack of incentive to make popular \u2028\n‚Üí Language barriers\n‚Ä¢ Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation?\n‚Ä¢ Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\n\nProject ideas\n‚Ä¢ B1. Measure energy consumption of single JUnit tests \n‚Ä¢ B2. Study test generation energy consumption\n‚Ä¢ B3. Energy proÔ¨Åling of static analysis tools\n‚Ä¢ B4. Detailed energy proÔ¨Åling of build pipelines\n‚Ä¢ B5. Tool supporting SusAF workshop / process\n‚Ä¢ B6. Dataset of government-developed OS software\n‚Ä¢ B7. Queue - but better for the student / TA society\nQuality Assurance & Testing\nSocial & Individual \u2028\nSustainability']","The project idea related to creating a dataset of government-developed OS software involves compiling a comprehensive dataset that includes data to understand the state of such software. This would involve determining if the software is buildable, has an open development history, and includes requirements documentation. The project could start with the Netherlands (NL) but would benefit from including data from other countries as well. The challenge in studying government software is due to the lack of incentive to make it popular and language barriers.","The project idea is to create a comprehensive dataset of government-developed OS software to understand the state of such software, including its buildability, open development history, and requirements documentation. This addresses challenges like the lack of incentive to make software popular and language barriers.",0.9999999999,1.0,1.0,0.89,0.1666666663888888,0.9898905009465272
