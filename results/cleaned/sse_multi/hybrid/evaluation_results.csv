user_input,retrieved_contexts,reference_contexts,response,reference,context_precision,context_recall,faithfulness,factual_correctness(mode=f1),context_entity_recall,answer_relevancy
How EcoAndroid reduce risk by reduce size?,"['• Some users noticed that K-9 mail was spending more energy than usual. 🙉 \n• A user that was having issues with a personal mail server  noticed that K-9 \nmail was the one of the most energy-greedy apps. IMAP IDLE protocol for real-time \nnotifications. \n• When a connection is not possible, the app automatically retries later. \n• https://github.com/k9mail/k-9/commit/\n86f3b28f79509d1a4d613eb39f60603e08579ea3\n\nEcoAndroid\n• Plugin for IntelliJ (Android Studio) \n• Dynamic Retry Delay \n• Push Over Poll \n• Reduce Size \n• Cache \n• Avoid Graphics and Animations\n20\n\nCarbon-Aware Computing for \nDatacenters \nhttps://sites.google.com/view/energy-efficiency-languages\n21\nAna Radovanovic ́, Ross Koningstein, Ian Schneider, Bokan \nChen, Alexandre Duarte, Binz Roy, Diyue Xiao, Maya \nHaridasan, Patrick Hung, Nick Care, Saurav Talukdar, Eric \nMullen, Kendal Smith, MariEllen Cottman, and Walfredo Cirne \n\n\n• Google’s Carbon-Intelligent Computing System (CICS) \n• Main idea: use carbon-intensity data to shift datacenter jobs in time \n• Typically, job schedulers use a metric of cluster capacity to schedule a job in \na particular cluster. \n• CICS overrides this metric with the virtual capacity curve (VCC) that \nfactors in Carbon intensity \n• When a new job comes in, the scheduler estimates its CPU load and power \nusage and assigns it to a cluster if the VCC is not exceeded.\n23\n\n• Jobs are divided between flexible and inflexible. \n• Flexible load is considered shapeable/shiftable as long as its total daily \ncompute (CPU) demand is preserved \n• The system needs to consider that, while running a job, the virtual capacity \ncurve (VCC) might drop. Hence, this job should not start in the first place. \n• They forecast VCC for the next day\n24\n\n\nVirtual Cluster Capacity (VCC)\n• Aims at reducing the peak load at carbon intensive hours but in total it should allow for the \nsome amount of daily computation!', '• Opt for low-power hardware (e.g., Nvidia Jetson boards) \n• Or GPUs that provide energy metrics (e.g., NVIDIA GPUs via the nvidia-smi tool) \n• Report energy/carbon metrics (e.g., embed in MLFlow?) \n• Use pre-trained models (Transfer Learning) \n• Preprocess dataset to reduce size. \n• Improve parameter-tuning strategy.\n11\n\nReporting energy/carbon footprint\n• We need benchmarks. \n• AllenAI leaderboard\u2028\nhttps://leaderboard.allenai.org \n• No carbon metrics, yet \n• Report comparable proxies for energy \nconsumption. \n• ⚠ Learning algorithms behave in a non-\ndeterministic \n• ⚠ Different data-points lead to different \nenergy consumption\n12\n\nReporting energy/carbon footprint\n• Reporting measured energy consumption \n• + Accurate \n• + Easy to map to carbon emissions \n• - Hard to measure \n• - Low replicability \n• Reporting time / estimation based on time & hardware \n• + Easy to measure \n• + Correlates with energy consumption in most cases. \n• -  Difficult to compare with measurements from other setups \n• E.g., floating point operations (FPOs) (?) \n• + comparable across different setups \n• + cheap \n• - does not factor in memory energy consumption \n• - does not reflect carbon emissions\n13\n\nData-centric AI\n\nData-centric AI\n• Emerging discipline that deals with systematically engineering data to build AI \nsystems. \n• Shift from improving the training strategy to improving the data. \n• It is better to have small but reliable datasets than large but noisy \ndatasets. \n• => Improve data collection, data labelling, and data preprocessing. \n• More about data-centric AI by Andrew Ng:\u2028\nhttps://www.youtube.com/watch?v=06-AZXmwHjo\n15\n\n\nGreen Data-centric AI\n• How do different ML algorithms compare \nin terms of energy consumption? \n• How does number of rows relate to the \nenergy consumption of ML models? \n• How does number of features relate to \nthe energy consumption of ML models? \n• What is the impact of reducing data in the \nperformance of the model? \n• Method -> results -> discussion\n17\n\nMethod\n• Single object of study: natural language model to detect spam messages.']","['<1-hop>\n\nShuffle \n🔀\n• It is not a mystery that energy consumption depends on so many factors that \nit is impossible to control all of them.\n• If you run 30 executions for version A and another batch for version B:\n• External conditions that change over time will have a diﬀerent bias in \nthe 2 versions (e.g., room temperature changes).\n• If you shuﬄe, you reduce this risk.\n11', '<2-hop>\n\nEcoAndroid\n• Plugin for IntelliJ (Android Studio)\n• Dynamic Retry Delay\n• Push Over Poll\n• Reduce Size\n• Cache\n• Avoid Graphics and Animations\n20', '<1-hop>\n\nShuffle \n🔀\n• It is not a mystery that energy consumption depends on so many factors that \nit is impossible to control all of them.\n• If you run 30 executions for version A and another batch for version B:\n• External conditions that change over time will have a diﬀerent bias in \nthe 2 versions (e.g., room temperature changes).\n• If you shuﬄe, you reduce this risk.\n11', '<2-hop>\n\nEcoAndroid\n• Plugin for IntelliJ (Android Studio)\n• Dynamic Retry Delay\n• Push Over Poll\n• Reduce Size\n• Cache\n• Avoid Graphics and Animations\n20']","EcoAndroid reduces risk by reducing size through several strategies, such as implementing dynamic retry delays, using push over poll, caching, and avoiding graphics and animations. These strategies help in minimizing the app's resource consumption, which in turn reduces energy usage and the risk of excessive battery drain.",EcoAndroid reduce risk by using Reduce Size feature which help minimize energy consumption and reduce impact of external factors like temperature changes.,0.0,0.5,0.875,0.0,0.1999999996,0.9282730300567732
How can static code analysis in data science be used to improve the energy efficiency of data centers and contribute to sustainable software engineering?,"['•\n\nC6 - Green Shift Left\n• Estimate energy efficiency using static code analysis. \n• We don’t need an accurate value. \n• It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention. \n• Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)\n27\n\nC7 - Education\n• Educational game for Software \nSustainability practices \n• Purpose: use within software teams to \ndiscuss or learn about different sustainable \nIT practices: at the organisation level, \nsoftware, etc.\n28\nhttps://github.com/OttoKaaij/Ticket-To-Sustainability/?tab=readme-ov-file\n\nProject ideas (old)\n• Plugin from EnergiBridge (GUI, report generator, python library, etc.) \n• Plugin for ChatGPT (carbon emissions per chat window) \n• Seamless measurements for AI libraries \n• Energy patterns for Green AI \n• Sustainable SW dev gamification \n• Sustainability auditor for AI projects \n• Energy Profiling of screen colour filter tools (or display settings) \n• … you can also propose yours!\n\nhttps://edu.nl/64gpk\nedu.nl/64gpk', 'data to understand state of software: \nbuildable?, open dev. history?, requirements documentation? \n• Could start with NL, but including your / other countries greatly appreciated!\n\nB7. Queue - but better for the student / TA society\n• Requirements analysis regarding social and individual sustainability effects \nof Queue \n• Other EIP / TUD used software also possible: e.g., Answers EWI \n• Non-technical project → proper process (workshops? Interviews?) & rich \ndescription of outcomes focus of grading\nhttps://eip.pages.ewi.tudelft.nl/eip-website/queue.html\n\nC1. Compare energy consumption of docker images for \nML workloads.\n• Similar to what we have seen in the lectures but for ML-specific workloads. \n• We can reuse existing experiment replication packages.\n22\n\nC2. Plugin to visualize Hugging Face carbon emissions.\n• https://huggingface.co/blog/leaderboard-emissions-\nanalysis \n•\nYoo, Taewon, et al. ""Visualizing the Carbon Intensity of Machine Learning Inference for Image Analysis on TensorFlow Hub.""\n\nC3. Plugin for ChatGPT\n• Users seldom know how much carbon they are emitting when they interact \nwith chat GPT. \n• Let’s make it transparent to the users. Browser plugin?\n24\nCO2\n\nC4. NutriScore for software libraries\n25\n• NutriScore labels are not perfect but they are a good starting point! \n• What if we could do the same thing for the energy efficiency of software. \n• (Also open to individual or social sustainability) \n• This work can be scoped in particular domains/ecossystems/use cases. \n• Libraries for stats? ML? Web Dev? Cloud?\n\nC5. Add energy-awareness to existing software\n26\n• Streamlit, Notebooks, JSFiddle, \nPyScript, zsh, etc. \n•\n\nC6 - Green Shift Left\n• Estimate energy efficiency using static code analysis. \n• We don’t need an accurate value. \n• It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention. \n• Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)']","['<1-hop>\n\nC6 - Green Shift Left\n• Estimate energy eﬃciency using static code analysis.\n• We don’t need an accurate value.\n• It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention.\n• Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)\n27', '<2-hop>\n\nWhy?\n• Throughout your career you might:\n• Design/maintain/contract data centers\n• Set up operations/devops\n• Develop AI for IoT devices\n• Be the next CEO/CTO of a software company\n• Sustainability can be your main role:\n• Green Software Developer\n• Sustainability Consultant\n• Green Advocate \n• Founder of a Green Tech startup (B2B?)\n28', '<1-hop>\n\nC6 - Green Shift Left\n• Estimate energy eﬃciency using static code analysis.\n• We don’t need an accurate value.\n• It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention.\n• Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)\n27', '<2-hop>\n\nWhy?\n• Throughout your career you might:\n• Design/maintain/contract data centers\n• Set up operations/devops\n• Develop AI for IoT devices\n• Be the next CEO/CTO of a software company\n• Sustainability can be your main role:\n• Green Software Developer\n• Sustainability Consultant\n• Green Advocate \n• Founder of a Green Tech startup (B2B?)\n28', '<1-hop>\n\nC6 - Green Shift Left\n• Estimate energy eﬃciency using static code analysis.\n• We don’t need an accurate value.\n• It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention.\n• Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)\n27', '<2-hop>\n\nWhy?\n• Throughout your career you might:\n• Design/maintain/contract data centers\n• Set up operations/devops\n• Develop AI for IoT devices\n• Be the next CEO/CTO of a software company\n• Sustainability can be your main role:\n• Green Software Developer\n• Sustainability Consultant\n• Green Advocate \n• Founder of a Green Tech startup (B2B?)\n28', '<1-hop>\n\nC6 - Green Shift Left\n• Estimate energy eﬃciency using static code analysis.\n• We don’t need an accurate value.\n• It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention.\n• Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)\n27', '<2-hop>\n\nWhy?\n• Throughout your career you might:\n• Design/maintain/contract data centers\n• Set up operations/devops\n• Develop AI for IoT devices\n• Be the next CEO/CTO of a software company\n• Sustainability can be your main role:\n• Green Software Developer\n• Sustainability Consultant\n• Green Advocate \n• Founder of a Green Tech startup (B2B?)\n28']","Static code analysis in data science can be used to improve the energy efficiency of data centers by identifying code that is likely to introduce energy hotspots. This process involves estimating energy efficiency without needing an exact value, allowing developers to focus on reviewing and optimizing parts of the code that are more likely to consume excessive energy. By targeting these areas, developers can make informed decisions to refactor or optimize code, leading to more energy-efficient software. This approach contributes to sustainable software engineering by reducing the energy consumption of software applications, which in turn can lower the overall energy usage of data centers. This method can be scoped specifically to data science domains, ensuring that the analysis is relevant and effective for the specific types of workloads and computations typical in data science projects.","Static code analysis in data science can be used to estimate energy efficiency by identifying potential energy hotspots in the code. This helps in reviewing and optimizing the code to reduce energy consumption. By applying this approach, developers can contribute to the design and maintenance of more energy-efficient data centers, aligning with sustainable software engineering practices.",0.99999999995,0.6666666666666666,1.0,0.89,0.2499999996874999,0.9753094414060188
How can static code analysis in data science contribute to the development of more sustainable data centers?,"['•\n\nC6 - Green Shift Left\n• Estimate energy efficiency using static code analysis. \n• We don’t need an accurate value. \n• It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention. \n• Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)\n27\n\nC7 - Education\n• Educational game for Software \nSustainability practices \n• Purpose: use within software teams to \ndiscuss or learn about different sustainable \nIT practices: at the organisation level, \nsoftware, etc.\n28\nhttps://github.com/OttoKaaij/Ticket-To-Sustainability/?tab=readme-ov-file\n\nProject ideas (old)\n• Plugin from EnergiBridge (GUI, report generator, python library, etc.) \n• Plugin for ChatGPT (carbon emissions per chat window) \n• Seamless measurements for AI libraries \n• Energy patterns for Green AI \n• Sustainable SW dev gamification \n• Sustainability auditor for AI projects \n• Energy Profiling of screen colour filter tools (or display settings) \n• … you can also propose yours!\n\nhttps://edu.nl/64gpk\nedu.nl/64gpk', 'Carbon-free giants\n• Google, Microsoft, Meta/Facebook want to be carbon free by 2030 \n• Carbon free is different from carbon neutral \n• Green IT experts are needed to meet these goals\n23\n\n\nRebound effect*\n• Energy consumption decreases ↓↓ but demand also increases ↑↑. \n• *a.k.a. Jevons Paradox in economics; Downs–Thomson paradox in\xa0mobility.\nEnergy per prompt\nPrompts\n100\n80\n30\n38\nChat \nGPT\n“Energy-efficient” \nChat GPT\n0\n\nIs sustainability an\u2028\nethical issue?\n• Climate change is more likely to affect the \npoorest countries. \n• Less financial resources to adapt \n• Climate-impact does not necessarily affect \npolluting countries. \n• Poorest countries have contributed less to the \nclimate change. \n• We need to figure out how to do more using \nless resources.\n26\n\nMorality ≠ Moralising\n• We should not use climate action as a \nshaming weapon \n• Climate action should be agnostic of political \nviews, ideology, social status, etc. \n• We need everyone to take action!\n27\n\nWhy?\n• Throughout your career you might: \n• Design/maintain/contract data centers \n• Set up operations/devops \n• Develop AI for IoT devices \n• Be the next CEO/CTO of a software company \n• Sustainability can be your main role: \n• Green Software Developer \n• Sustainability Consultant \n• Green Advocate  \n• Founder of a Green Tech startup (B2B?)\n28\n\nFormat of classes\n• In-person. \n• Collegerama recordings. \n• Lectures and Labs. \n• Guest lectures. \n• Steering meetings (after week 5, new schedule)\n29\n\nFormat of classes\n• There’s no exam in this course. It’s more important that we learn how to \ndiscuss this topic and come up with new ideas than learning all the theory. \nCritical thinking over checkboxes. \n• Mix of content and discussion \n• Ultimately, the lectures aim to give you food for thought and the necessary \nknowledge to excel in Project 2.']","['<1-hop>\n\nC6 - Green Shift Left\n• Estimate energy eﬃciency using static code analysis.\n• We don’t need an accurate value.\n• It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention.\n• Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)\n27', '<2-hop>\n\nWhy?\n• Throughout your career you might:\n• Design/maintain/contract data centers\n• Set up operations/devops\n• Develop AI for IoT devices\n• Be the next CEO/CTO of a software company\n• Sustainability can be your main role:\n• Green Software Developer\n• Sustainability Consultant\n• Green Advocate \n• Founder of a Green Tech startup (B2B?)\n28']","Static code analysis in data science can contribute to the development of more sustainable data centers by estimating energy efficiency. Although it doesn't provide an accurate value, it helps identify which parts of the code are more likely to introduce energy hotspots. By focusing on these areas, developers can optimize code to reduce energy consumption, thereby contributing to the overall sustainability of data centers. This approach allows for targeted improvements in energy efficiency, which is crucial for reducing the carbon footprint of data centers.","Static code analysis in data science can contribute to the development of more sustainable data centers by estimating energy efficiency and identifying potential energy hotspots within the code. This approach allows developers to focus on reviewing code segments that are more likely to introduce inefficiencies, thereby optimizing resource usage. By integrating such practices, especially in domains like data science, developers can help create more sustainable solutions, which aligns with broader career goals in sustainability, such as designing or maintaining energy-efficient data centers.",0.99999999995,1.0,0.625,0.67,0.2222222219753086,0.9752228176640018
How can static code analysis in data science projects help in designing more sustainablility-focused data centers?,"['data to understand state of software: \nbuildable?, open dev. history?, requirements documentation? \n• Could start with NL, but including your / other countries greatly appreciated!\n\nB7. Queue - but better for the student / TA society\n• Requirements analysis regarding social and individual sustainability effects \nof Queue \n• Other EIP / TUD used software also possible: e.g., Answers EWI \n• Non-technical project → proper process (workshops? Interviews?) & rich \ndescription of outcomes focus of grading\nhttps://eip.pages.ewi.tudelft.nl/eip-website/queue.html\n\nC1. Compare energy consumption of docker images for \nML workloads.\n• Similar to what we have seen in the lectures but for ML-specific workloads. \n• We can reuse existing experiment replication packages.\n22\n\nC2. Plugin to visualize Hugging Face carbon emissions.\n• https://huggingface.co/blog/leaderboard-emissions-\nanalysis \n•\nYoo, Taewon, et al. ""Visualizing the Carbon Intensity of Machine Learning Inference for Image Analysis on TensorFlow Hub.""\n\nC3. Plugin for ChatGPT\n• Users seldom know how much carbon they are emitting when they interact \nwith chat GPT. \n• Let’s make it transparent to the users. Browser plugin?\n24\nCO2\n\nC4. NutriScore for software libraries\n25\n• NutriScore labels are not perfect but they are a good starting point! \n• What if we could do the same thing for the energy efficiency of software. \n• (Also open to individual or social sustainability) \n• This work can be scoped in particular domains/ecossystems/use cases. \n• Libraries for stats? ML? Web Dev? Cloud?\n\nC5. Add energy-awareness to existing software\n26\n• Streamlit, Notebooks, JSFiddle, \nPyScript, zsh, etc. \n•\n\nC6 - Green Shift Left\n• Estimate energy efficiency using static code analysis. \n• We don’t need an accurate value. \n• It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention. \n• Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)', '•\n\nC6 - Green Shift Left\n• Estimate energy efficiency using static code analysis. \n• We don’t need an accurate value. \n• It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention. \n• Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)\n27\n\nC7 - Education\n• Educational game for Software \nSustainability practices \n• Purpose: use within software teams to \ndiscuss or learn about different sustainable \nIT practices: at the organisation level, \nsoftware, etc.\n28\nhttps://github.com/OttoKaaij/Ticket-To-Sustainability/?tab=readme-ov-file\n\nProject ideas (old)\n• Plugin from EnergiBridge (GUI, report generator, python library, etc.) \n• Plugin for ChatGPT (carbon emissions per chat window) \n• Seamless measurements for AI libraries \n• Energy patterns for Green AI \n• Sustainable SW dev gamification \n• Sustainability auditor for AI projects \n• Energy Profiling of screen colour filter tools (or display settings) \n• … you can also propose yours!\n\nhttps://edu.nl/64gpk\nedu.nl/64gpk']","['<1-hop>\n\nC6 - Green Shift Left\n• Estimate energy eﬃciency using static code analysis.\n• We don’t need an accurate value.\n• It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention.\n• Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)\n27', '<2-hop>\n\nWhy?\n• Throughout your career you might:\n• Design/maintain/contract data centers\n• Set up operations/devops\n• Develop AI for IoT devices\n• Be the next CEO/CTO of a software company\n• Sustainability can be your main role:\n• Green Software Developer\n• Sustainability Consultant\n• Green Advocate \n• Founder of a Green Tech startup (B2B?)\n28', '<1-hop>\n\nC6 - Green Shift Left\n• Estimate energy eﬃciency using static code analysis.\n• We don’t need an accurate value.\n• It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention.\n• Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)\n27', '<2-hop>\n\nWhy?\n• Throughout your career you might:\n• Design/maintain/contract data centers\n• Set up operations/devops\n• Develop AI for IoT devices\n• Be the next CEO/CTO of a software company\n• Sustainability can be your main role:\n• Green Software Developer\n• Sustainability Consultant\n• Green Advocate \n• Founder of a Green Tech startup (B2B?)\n28', '<1-hop>\n\nC6 - Green Shift Left\n• Estimate energy eﬃciency using static code analysis.\n• We don’t need an accurate value.\n• It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention.\n• Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)\n27', '<2-hop>\n\nWhy?\n• Throughout your career you might:\n• Design/maintain/contract data centers\n• Set up operations/devops\n• Develop AI for IoT devices\n• Be the next CEO/CTO of a software company\n• Sustainability can be your main role:\n• Green Software Developer\n• Sustainability Consultant\n• Green Advocate \n• Founder of a Green Tech startup (B2B?)\n28']","Static code analysis in data science projects can help in designing more sustainability-focused data centers by identifying code that is likely to introduce energy hotspots. By analyzing the code statically, developers can estimate energy efficiency and pinpoint areas that may require optimization to reduce energy consumption. This proactive approach allows for more efficient use of resources, leading to reduced energy usage in data centers. By focusing on optimizing code for energy efficiency, data centers can operate more sustainably, minimizing their carbon footprint and overall environmental impact. This process can be scoped to particular domains such as data science, ensuring that the specific needs and characteristics of these projects are taken into account.","Static code analysis in data science projects can help in designing more sustainablility-focused data centers by identifying energy hotspots in the code. This allows developers to review and optimize code that may contribute to higher energy consumption, thereby supporting the development of more efficient and sustainable data centers.",0.99999999995,1.0,0.4545454545454545,0.91,0.0,0.9648912845671904
How does the use of static code analysis in data science projects contribute to the design and optimization of more sustanable data centers?,"['Carbon-free giants\n• Google, Microsoft, Meta/Facebook want to be carbon free by 2030 \n• Carbon free is different from carbon neutral \n• Green IT experts are needed to meet these goals\n23\n\n\nRebound effect*\n• Energy consumption decreases ↓↓ but demand also increases ↑↑. \n• *a.k.a. Jevons Paradox in economics; Downs–Thomson paradox in\xa0mobility.\nEnergy per prompt\nPrompts\n100\n80\n30\n38\nChat \nGPT\n“Energy-efficient” \nChat GPT\n0\n\nIs sustainability an\u2028\nethical issue?\n• Climate change is more likely to affect the \npoorest countries. \n• Less financial resources to adapt \n• Climate-impact does not necessarily affect \npolluting countries. \n• Poorest countries have contributed less to the \nclimate change. \n• We need to figure out how to do more using \nless resources.\n26\n\nMorality ≠ Moralising\n• We should not use climate action as a \nshaming weapon \n• Climate action should be agnostic of political \nviews, ideology, social status, etc. \n• We need everyone to take action!\n27\n\nWhy?\n• Throughout your career you might: \n• Design/maintain/contract data centers \n• Set up operations/devops \n• Develop AI for IoT devices \n• Be the next CEO/CTO of a software company \n• Sustainability can be your main role: \n• Green Software Developer \n• Sustainability Consultant \n• Green Advocate  \n• Founder of a Green Tech startup (B2B?)\n28\n\nFormat of classes\n• In-person. \n• Collegerama recordings. \n• Lectures and Labs. \n• Guest lectures. \n• Steering meetings (after week 5, new schedule)\n29\n\nFormat of classes\n• There’s no exam in this course. It’s more important that we learn how to \ndiscuss this topic and come up with new ideas than learning all the theory. \nCritical thinking over checkboxes. \n• Mix of content and discussion \n• Ultimately, the lectures aim to give you food for thought and the necessary \nknowledge to excel in Project 2.', '•\n\nC6 - Green Shift Left\n• Estimate energy efficiency using static code analysis. \n• We don’t need an accurate value. \n• It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention. \n• Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)\n27\n\nC7 - Education\n• Educational game for Software \nSustainability practices \n• Purpose: use within software teams to \ndiscuss or learn about different sustainable \nIT practices: at the organisation level, \nsoftware, etc.\n28\nhttps://github.com/OttoKaaij/Ticket-To-Sustainability/?tab=readme-ov-file\n\nProject ideas (old)\n• Plugin from EnergiBridge (GUI, report generator, python library, etc.) \n• Plugin for ChatGPT (carbon emissions per chat window) \n• Seamless measurements for AI libraries \n• Energy patterns for Green AI \n• Sustainable SW dev gamification \n• Sustainability auditor for AI projects \n• Energy Profiling of screen colour filter tools (or display settings) \n• … you can also propose yours!\n\nhttps://edu.nl/64gpk\nedu.nl/64gpk']","['<1-hop>\n\nC6 - Green Shift Left\n• Estimate energy eﬃciency using static code analysis.\n• We don’t need an accurate value.\n• It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention.\n• Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)\n27', '<2-hop>\n\nWhy?\n• Throughout your career you might:\n• Design/maintain/contract data centers\n• Set up operations/devops\n• Develop AI for IoT devices\n• Be the next CEO/CTO of a software company\n• Sustainability can be your main role:\n• Green Software Developer\n• Sustainability Consultant\n• Green Advocate \n• Founder of a Green Tech startup (B2B?)\n28', '<1-hop>\n\nC6 - Green Shift Left\n• Estimate energy eﬃciency using static code analysis.\n• We don’t need an accurate value.\n• It is useful to know which code is more likely to \nintroduce energy hotspots and that should be \nreviewed with more attention.\n• Can be scoped to a particular domain (react, php, data \nscience, web, etc., etc.)\n27', '<2-hop>\n\nWhy?\n• Throughout your career you might:\n• Design/maintain/contract data centers\n• Set up operations/devops\n• Develop AI for IoT devices\n• Be the next CEO/CTO of a software company\n• Sustainability can be your main role:\n• Green Software Developer\n• Sustainability Consultant\n• Green Advocate \n• Founder of a Green Tech startup (B2B?)\n28']","The use of static code analysis in data science projects helps estimate energy efficiency by identifying code that is likely to introduce energy hotspots. By reviewing and optimizing such code, data centers can reduce energy consumption, contributing to more sustainable operations. This process allows for targeted improvements in energy efficiency without needing precise energy values, thus supporting the design and optimization of more sustainable data centers.","The use of static code analysis in data science projects contributes to the design and optimization of more sustainable data centers by identifying energy hotspots within the code. This process allows developers to focus on reviewing and optimizing sections of code that are more likely to introduce energy inefficiencies. By doing so, it supports the broader goal of creating more energy-efficient systems, which is crucial for the sustainable operation of data centers. This approach aligns with the growing need for green software development practices, where professionals can play roles such as green software developers or sustainability consultants, ensuring that energy efficiency is a key consideration throughout the software lifecycle.",0.49999999995,1.0,0.8,0.33,0.0,0.9452657018599367
How reduce risk and reduce size in software engineering?,"['• Opt for low-power hardware (e.g., Nvidia Jetson boards) \n• Or GPUs that provide energy metrics (e.g., NVIDIA GPUs via the nvidia-smi tool) \n• Report energy/carbon metrics (e.g., embed in MLFlow?) \n• Use pre-trained models (Transfer Learning) \n• Preprocess dataset to reduce size. \n• Improve parameter-tuning strategy.\n11\n\nReporting energy/carbon footprint\n• We need benchmarks. \n• AllenAI leaderboard\u2028\nhttps://leaderboard.allenai.org \n• No carbon metrics, yet \n• Report comparable proxies for energy \nconsumption. \n• ⚠ Learning algorithms behave in a non-\ndeterministic \n• ⚠ Different data-points lead to different \nenergy consumption\n12\n\nReporting energy/carbon footprint\n• Reporting measured energy consumption \n• + Accurate \n• + Easy to map to carbon emissions \n• - Hard to measure \n• - Low replicability \n• Reporting time / estimation based on time & hardware \n• + Easy to measure \n• + Correlates with energy consumption in most cases. \n• -  Difficult to compare with measurements from other setups \n• E.g., floating point operations (FPOs) (?) \n• + comparable across different setups \n• + cheap \n• - does not factor in memory energy consumption \n• - does not reflect carbon emissions\n13\n\nData-centric AI\n\nData-centric AI\n• Emerging discipline that deals with systematically engineering data to build AI \nsystems. \n• Shift from improving the training strategy to improving the data. \n• It is better to have small but reliable datasets than large but noisy \ndatasets. \n• => Improve data collection, data labelling, and data preprocessing. \n• More about data-centric AI by Andrew Ng:\u2028\nhttps://www.youtube.com/watch?v=06-AZXmwHjo\n15\n\n\nGreen Data-centric AI\n• How do different ML algorithms compare \nin terms of energy consumption? \n• How does number of rows relate to the \nenergy consumption of ML models? \n• How does number of features relate to \nthe energy consumption of ML models? \n• What is the impact of reducing data in the \nperformance of the model? \n• Method -> results -> discussion\n17\n\nMethod\n• Single object of study: natural language model to detect spam messages.', 'Rest ⏸\n• It is common practice to do a pause/sleep between executions/\nmeasurements. \n• Prevent tail energy consumption from previous measurements. ? \n• Prevent collateral tasks of previous measurement from affecting the next \nmeasurement. \n• There is no golden rule but one minute should be enough. It can be more or \nless depending on your hardware or the duration of your energy test.\n9\n\nt\nP\nTail Energy Consumption\n\nShuffle 🔀\n• It is not a mystery that energy consumption depends on so many factors that \nit is impossible to control all of them. \n• If you run 30 executions for version A and another batch for version B: \n• External conditions that change over time will have a different bias in \nthe 2 versions (e.g., room temperature changes). \n• If you shuffle, you reduce this risk.\n11\n\nKeep it cool 🌡\n• Always make sure there is a stable room temperature. \n• Tricky because, some times, experiments may have to run over a few days. \n• If you cannot control room temperature: collect temperature data and filter \nout measurements where the room temperature is clearly deviating.\n12\n\nAutomate Executions 🤖\n• (Already mentioned in the previous classes) \n• One cannot run 30 shuffled experiments per version without automation…\n13\n\nData analysis\n14\n\n1. Exploratory Analysis\n• Plot the data and inspect outliers or \nunexpected biases. \n• Violin+box plots are usually handy. (?) \n• It’s a nice way of combining the 30 \nexperiments, and of showing \ndescriptive statistics. (?) \n• Shows the shape of the distribution \nof the data.\n15\n\n1. Exploratory Analysis (II)\n• Data should be Normal. Unless there’s a \ngood reason. \n• E.g., somewhere amongst the 30 executions, \nthere might be 1 or 2 that failed to finish due \nto some unexpected error. \n• (It happens and that’s ok!)– consequently, \nthe execution is shorter and spends less \nenergy – falsely appearing as more \nenergy efficient. \n• If data is not Normal there might be some \nissues affecting the measurements that \nmight be ruining results. It is important to \ninvestigate this.\n16\n\nEnergy data is not normal. Why?']","['<1-hop>\n\nShuffle \n🔀\n• It is not a mystery that energy consumption depends on so many factors that \nit is impossible to control all of them.\n• If you run 30 executions for version A and another batch for version B:\n• External conditions that change over time will have a diﬀerent bias in \nthe 2 versions (e.g., room temperature changes).\n• If you shuﬄe, you reduce this risk.\n11', '<2-hop>\n\nEcoAndroid\n• Plugin for IntelliJ (Android Studio)\n• Dynamic Retry Delay\n• Push Over Poll\n• Reduce Size\n• Cache\n• Avoid Graphics and Animations\n20', '<1-hop>\n\nShuffle \n🔀\n• It is not a mystery that energy consumption depends on so many factors that \nit is impossible to control all of them.\n• If you run 30 executions for version A and another batch for version B:\n• External conditions that change over time will have a diﬀerent bias in \nthe 2 versions (e.g., room temperature changes).\n• If you shuﬄe, you reduce this risk.\n11', '<2-hop>\n\nEcoAndroid\n• Plugin for IntelliJ (Android Studio)\n• Dynamic Retry Delay\n• Push Over Poll\n• Reduce Size\n• Cache\n• Avoid Graphics and Animations\n20']","To reduce risk and reduce size in software engineering, consider the following strategies:

1. **Opt for Low-Power Hardware**: Use energy-efficient hardware like Nvidia Jetson boards or GPUs that provide energy metrics, which can help in reducing the energy footprint and potentially the size of the hardware setup.

2. **Use Pre-trained Models (Transfer Learning)**: This approach can reduce the size of the training dataset needed and the computational resources required, thereby minimizing risk and resource consumption.

3. **Preprocess Dataset to Reduce Size**: By preprocessing the dataset, you can eliminate unnecessary data, which reduces the size and complexity of the data, leading to more efficient processing and reduced risk of errors.

4. **Improve Parameter-Tuning Strategy**: Efficient parameter tuning can lead to better model performance with less computational overhead, reducing both risk and resource usage.

5. **Data-Centric AI**: Focus on improving data quality rather than just the training strategy. Reliable, smaller datasets are preferable to large, noisy ones, reducing the risk of errors and inefficiencies.

6. **Automate Executions**: Automation can help manage and reduce the risk of human error in repetitive tasks, ensuring consistency and efficiency.

7. **Shuffle and Control External Conditions**: Shuffling experiments and controlling external conditions like room temperature can help reduce biases and risks associated with environmental factors.

These strategies collectively help in managing risk and reducing the size and complexity of software engineering projects.","To reduce risk, shuffling executions helps control external factors like temperature changes. To reduce size, EcoAndroid uses features like dynamic retry delay and push over poll.",0.0,0.5,0.4583333333333333,0.36,0.0,0.9809110510312556
How shuffling reduce risk and EcoAndroid reduce size?,"['Rest ⏸\n• It is common practice to do a pause/sleep between executions/\nmeasurements. \n• Prevent tail energy consumption from previous measurements. ? \n• Prevent collateral tasks of previous measurement from affecting the next \nmeasurement. \n• There is no golden rule but one minute should be enough. It can be more or \nless depending on your hardware or the duration of your energy test.\n9\n\nt\nP\nTail Energy Consumption\n\nShuffle 🔀\n• It is not a mystery that energy consumption depends on so many factors that \nit is impossible to control all of them. \n• If you run 30 executions for version A and another batch for version B: \n• External conditions that change over time will have a different bias in \nthe 2 versions (e.g., room temperature changes). \n• If you shuffle, you reduce this risk.\n11\n\nKeep it cool 🌡\n• Always make sure there is a stable room temperature. \n• Tricky because, some times, experiments may have to run over a few days. \n• If you cannot control room temperature: collect temperature data and filter \nout measurements where the room temperature is clearly deviating.\n12\n\nAutomate Executions 🤖\n• (Already mentioned in the previous classes) \n• One cannot run 30 shuffled experiments per version without automation…\n13\n\nData analysis\n14\n\n1. Exploratory Analysis\n• Plot the data and inspect outliers or \nunexpected biases. \n• Violin+box plots are usually handy. (?) \n• It’s a nice way of combining the 30 \nexperiments, and of showing \ndescriptive statistics. (?) \n• Shows the shape of the distribution \nof the data.\n15\n\n1. Exploratory Analysis (II)\n• Data should be Normal. Unless there’s a \ngood reason. \n• E.g., somewhere amongst the 30 executions, \nthere might be 1 or 2 that failed to finish due \nto some unexpected error. \n• (It happens and that’s ok!)– consequently, \nthe execution is shorter and spends less \nenergy – falsely appearing as more \nenergy efficient. \n• If data is not Normal there might be some \nissues affecting the measurements that \nmight be ruining results. It is important to \ninvestigate this.\n16\n\nEnergy data is not normal. Why?', '• Some users noticed that K-9 mail was spending more energy than usual. 🙉 \n• A user that was having issues with a personal mail server  noticed that K-9 \nmail was the one of the most energy-greedy apps. IMAP IDLE protocol for real-time \nnotifications. \n• When a connection is not possible, the app automatically retries later. \n• https://github.com/k9mail/k-9/commit/\n86f3b28f79509d1a4d613eb39f60603e08579ea3\n\nEcoAndroid\n• Plugin for IntelliJ (Android Studio) \n• Dynamic Retry Delay \n• Push Over Poll \n• Reduce Size \n• Cache \n• Avoid Graphics and Animations\n20\n\nCarbon-Aware Computing for \nDatacenters \nhttps://sites.google.com/view/energy-efficiency-languages\n21\nAna Radovanovic ́, Ross Koningstein, Ian Schneider, Bokan \nChen, Alexandre Duarte, Binz Roy, Diyue Xiao, Maya \nHaridasan, Patrick Hung, Nick Care, Saurav Talukdar, Eric \nMullen, Kendal Smith, MariEllen Cottman, and Walfredo Cirne \n\n\n• Google’s Carbon-Intelligent Computing System (CICS) \n• Main idea: use carbon-intensity data to shift datacenter jobs in time \n• Typically, job schedulers use a metric of cluster capacity to schedule a job in \na particular cluster. \n• CICS overrides this metric with the virtual capacity curve (VCC) that \nfactors in Carbon intensity \n• When a new job comes in, the scheduler estimates its CPU load and power \nusage and assigns it to a cluster if the VCC is not exceeded.\n23\n\n• Jobs are divided between flexible and inflexible. \n• Flexible load is considered shapeable/shiftable as long as its total daily \ncompute (CPU) demand is preserved \n• The system needs to consider that, while running a job, the virtual capacity \ncurve (VCC) might drop. Hence, this job should not start in the first place. \n• They forecast VCC for the next day\n24\n\n\nVirtual Cluster Capacity (VCC)\n• Aims at reducing the peak load at carbon intensive hours but in total it should allow for the \nsome amount of daily computation!']","['<1-hop>\n\nShuffle \n🔀\n• It is not a mystery that energy consumption depends on so many factors that \nit is impossible to control all of them.\n• If you run 30 executions for version A and another batch for version B:\n• External conditions that change over time will have a diﬀerent bias in \nthe 2 versions (e.g., room temperature changes).\n• If you shuﬄe, you reduce this risk.\n11', '<2-hop>\n\nEcoAndroid\n• Plugin for IntelliJ (Android Studio)\n• Dynamic Retry Delay\n• Push Over Poll\n• Reduce Size\n• Cache\n• Avoid Graphics and Animations\n20', '<1-hop>\n\nShuffle \n🔀\n• It is not a mystery that energy consumption depends on so many factors that \nit is impossible to control all of them.\n• If you run 30 executions for version A and another batch for version B:\n• External conditions that change over time will have a diﬀerent bias in \nthe 2 versions (e.g., room temperature changes).\n• If you shuﬄe, you reduce this risk.\n11', '<2-hop>\n\nEcoAndroid\n• Plugin for IntelliJ (Android Studio)\n• Dynamic Retry Delay\n• Push Over Poll\n• Reduce Size\n• Cache\n• Avoid Graphics and Animations\n20', '<1-hop>\n\nShuffle \n🔀\n• It is not a mystery that energy consumption depends on so many factors that \nit is impossible to control all of them.\n• If you run 30 executions for version A and another batch for version B:\n• External conditions that change over time will have a diﬀerent bias in \nthe 2 versions (e.g., room temperature changes).\n• If you shuﬄe, you reduce this risk.\n11', '<2-hop>\n\nEcoAndroid\n• Plugin for IntelliJ (Android Studio)\n• Dynamic Retry Delay\n• Push Over Poll\n• Reduce Size\n• Cache\n• Avoid Graphics and Animations\n20']","Shuffling reduces risk by minimizing the impact of external conditions that can change over time, such as room temperature, which might introduce bias into the results of different versions of an experiment. By shuffling the order of executions, you ensure that these external factors are more evenly distributed across all versions, thus reducing the risk of skewed results (Source: Shuffle 🔀).

EcoAndroid reduces size by implementing strategies such as dynamic retry delay, push over poll, and caching, which help to optimize the app's operations and reduce unnecessary resource usage, thereby reducing the overall size and energy consumption of the app (Source: EcoAndroid).",Shuffling reduce risk by controlling external factors like room temperature changes. EcoAndroid reduce size by using features like caching and avoiding graphics and animations.,,1.0,1.0,0.4,0.1666666663888888,0.9247690643830894
How shuffling reduce risk when compare to EcoAndroid reduce size?,"['Rest ⏸\n• It is common practice to do a pause/sleep between executions/\nmeasurements. \n• Prevent tail energy consumption from previous measurements. ? \n• Prevent collateral tasks of previous measurement from affecting the next \nmeasurement. \n• There is no golden rule but one minute should be enough. It can be more or \nless depending on your hardware or the duration of your energy test.\n9\n\nt\nP\nTail Energy Consumption\n\nShuffle 🔀\n• It is not a mystery that energy consumption depends on so many factors that \nit is impossible to control all of them. \n• If you run 30 executions for version A and another batch for version B: \n• External conditions that change over time will have a different bias in \nthe 2 versions (e.g., room temperature changes). \n• If you shuffle, you reduce this risk.\n11\n\nKeep it cool 🌡\n• Always make sure there is a stable room temperature. \n• Tricky because, some times, experiments may have to run over a few days. \n• If you cannot control room temperature: collect temperature data and filter \nout measurements where the room temperature is clearly deviating.\n12\n\nAutomate Executions 🤖\n• (Already mentioned in the previous classes) \n• One cannot run 30 shuffled experiments per version without automation…\n13\n\nData analysis\n14\n\n1. Exploratory Analysis\n• Plot the data and inspect outliers or \nunexpected biases. \n• Violin+box plots are usually handy. (?) \n• It’s a nice way of combining the 30 \nexperiments, and of showing \ndescriptive statistics. (?) \n• Shows the shape of the distribution \nof the data.\n15\n\n1. Exploratory Analysis (II)\n• Data should be Normal. Unless there’s a \ngood reason. \n• E.g., somewhere amongst the 30 executions, \nthere might be 1 or 2 that failed to finish due \nto some unexpected error. \n• (It happens and that’s ok!)– consequently, \nthe execution is shorter and spends less \nenergy – falsely appearing as more \nenergy efficient. \n• If data is not Normal there might be some \nissues affecting the measurements that \nmight be ruining results. It is important to \ninvestigate this.\n16\n\nEnergy data is not normal. Why?', '• Some users noticed that K-9 mail was spending more energy than usual. 🙉 \n• A user that was having issues with a personal mail server  noticed that K-9 \nmail was the one of the most energy-greedy apps. IMAP IDLE protocol for real-time \nnotifications. \n• When a connection is not possible, the app automatically retries later. \n• https://github.com/k9mail/k-9/commit/\n86f3b28f79509d1a4d613eb39f60603e08579ea3\n\nEcoAndroid\n• Plugin for IntelliJ (Android Studio) \n• Dynamic Retry Delay \n• Push Over Poll \n• Reduce Size \n• Cache \n• Avoid Graphics and Animations\n20\n\nCarbon-Aware Computing for \nDatacenters \nhttps://sites.google.com/view/energy-efficiency-languages\n21\nAna Radovanovic ́, Ross Koningstein, Ian Schneider, Bokan \nChen, Alexandre Duarte, Binz Roy, Diyue Xiao, Maya \nHaridasan, Patrick Hung, Nick Care, Saurav Talukdar, Eric \nMullen, Kendal Smith, MariEllen Cottman, and Walfredo Cirne \n\n\n• Google’s Carbon-Intelligent Computing System (CICS) \n• Main idea: use carbon-intensity data to shift datacenter jobs in time \n• Typically, job schedulers use a metric of cluster capacity to schedule a job in \na particular cluster. \n• CICS overrides this metric with the virtual capacity curve (VCC) that \nfactors in Carbon intensity \n• When a new job comes in, the scheduler estimates its CPU load and power \nusage and assigns it to a cluster if the VCC is not exceeded.\n23\n\n• Jobs are divided between flexible and inflexible. \n• Flexible load is considered shapeable/shiftable as long as its total daily \ncompute (CPU) demand is preserved \n• The system needs to consider that, while running a job, the virtual capacity \ncurve (VCC) might drop. Hence, this job should not start in the first place. \n• They forecast VCC for the next day\n24\n\n\nVirtual Cluster Capacity (VCC)\n• Aims at reducing the peak load at carbon intensive hours but in total it should allow for the \nsome amount of daily computation!']","['<1-hop>\n\nShuffle \n🔀\n• It is not a mystery that energy consumption depends on so many factors that \nit is impossible to control all of them.\n• If you run 30 executions for version A and another batch for version B:\n• External conditions that change over time will have a diﬀerent bias in \nthe 2 versions (e.g., room temperature changes).\n• If you shuﬄe, you reduce this risk.\n11', '<2-hop>\n\nEcoAndroid\n• Plugin for IntelliJ (Android Studio)\n• Dynamic Retry Delay\n• Push Over Poll\n• Reduce Size\n• Cache\n• Avoid Graphics and Animations\n20']","Shuffling reduces risk by minimizing the impact of external conditions that change over time, such as room temperature, which can introduce bias in energy consumption measurements when comparing different versions of software. By shuffling the order of executions, you ensure that these external factors affect both versions more evenly, thus reducing the risk of biased results.

On the other hand, EcoAndroid's approach to reducing size is focused on optimizing the app itself to consume less energy. This involves techniques like dynamic retry delay, push over poll, and caching, which directly aim to make the app more energy-efficient.

In summary, shuffling is a method to ensure fair comparison by controlling external variables, while reducing size in EcoAndroid is a direct optimization strategy to lower energy consumption.",Shuffling reduce risk by balance external factor like room temperature change while EcoAndroid reduce size by optimize resource use.,0.0,1.0,0.6363636363636364,0.77,0.3333333322222222,0.9300479437368656
How shuffling reduce risk when reduce size in software engineering?,"['Rest ⏸\n• It is common practice to do a pause/sleep between executions/\nmeasurements. \n• Prevent tail energy consumption from previous measurements. ? \n• Prevent collateral tasks of previous measurement from affecting the next \nmeasurement. \n• There is no golden rule but one minute should be enough. It can be more or \nless depending on your hardware or the duration of your energy test.\n9\n\nt\nP\nTail Energy Consumption\n\nShuffle 🔀\n• It is not a mystery that energy consumption depends on so many factors that \nit is impossible to control all of them. \n• If you run 30 executions for version A and another batch for version B: \n• External conditions that change over time will have a different bias in \nthe 2 versions (e.g., room temperature changes). \n• If you shuffle, you reduce this risk.\n11\n\nKeep it cool 🌡\n• Always make sure there is a stable room temperature. \n• Tricky because, some times, experiments may have to run over a few days. \n• If you cannot control room temperature: collect temperature data and filter \nout measurements where the room temperature is clearly deviating.\n12\n\nAutomate Executions 🤖\n• (Already mentioned in the previous classes) \n• One cannot run 30 shuffled experiments per version without automation…\n13\n\nData analysis\n14\n\n1. Exploratory Analysis\n• Plot the data and inspect outliers or \nunexpected biases. \n• Violin+box plots are usually handy. (?) \n• It’s a nice way of combining the 30 \nexperiments, and of showing \ndescriptive statistics. (?) \n• Shows the shape of the distribution \nof the data.\n15\n\n1. Exploratory Analysis (II)\n• Data should be Normal. Unless there’s a \ngood reason. \n• E.g., somewhere amongst the 30 executions, \nthere might be 1 or 2 that failed to finish due \nto some unexpected error. \n• (It happens and that’s ok!)– consequently, \nthe execution is shorter and spends less \nenergy – falsely appearing as more \nenergy efficient. \n• If data is not Normal there might be some \nissues affecting the measurements that \nmight be ruining results. It is important to \ninvestigate this.\n16\n\nEnergy data is not normal. Why?', '• Opt for low-power hardware (e.g., Nvidia Jetson boards) \n• Or GPUs that provide energy metrics (e.g., NVIDIA GPUs via the nvidia-smi tool) \n• Report energy/carbon metrics (e.g., embed in MLFlow?) \n• Use pre-trained models (Transfer Learning) \n• Preprocess dataset to reduce size. \n• Improve parameter-tuning strategy.\n11\n\nReporting energy/carbon footprint\n• We need benchmarks. \n• AllenAI leaderboard\u2028\nhttps://leaderboard.allenai.org \n• No carbon metrics, yet \n• Report comparable proxies for energy \nconsumption. \n• ⚠ Learning algorithms behave in a non-\ndeterministic \n• ⚠ Different data-points lead to different \nenergy consumption\n12\n\nReporting energy/carbon footprint\n• Reporting measured energy consumption \n• + Accurate \n• + Easy to map to carbon emissions \n• - Hard to measure \n• - Low replicability \n• Reporting time / estimation based on time & hardware \n• + Easy to measure \n• + Correlates with energy consumption in most cases. \n• -  Difficult to compare with measurements from other setups \n• E.g., floating point operations (FPOs) (?) \n• + comparable across different setups \n• + cheap \n• - does not factor in memory energy consumption \n• - does not reflect carbon emissions\n13\n\nData-centric AI\n\nData-centric AI\n• Emerging discipline that deals with systematically engineering data to build AI \nsystems. \n• Shift from improving the training strategy to improving the data. \n• It is better to have small but reliable datasets than large but noisy \ndatasets. \n• => Improve data collection, data labelling, and data preprocessing. \n• More about data-centric AI by Andrew Ng:\u2028\nhttps://www.youtube.com/watch?v=06-AZXmwHjo\n15\n\n\nGreen Data-centric AI\n• How do different ML algorithms compare \nin terms of energy consumption? \n• How does number of rows relate to the \nenergy consumption of ML models? \n• How does number of features relate to \nthe energy consumption of ML models? \n• What is the impact of reducing data in the \nperformance of the model? \n• Method -> results -> discussion\n17\n\nMethod\n• Single object of study: natural language model to detect spam messages.']","['<1-hop>\n\nShuffle \n🔀\n• It is not a mystery that energy consumption depends on so many factors that \nit is impossible to control all of them.\n• If you run 30 executions for version A and another batch for version B:\n• External conditions that change over time will have a diﬀerent bias in \nthe 2 versions (e.g., room temperature changes).\n• If you shuﬄe, you reduce this risk.\n11', '<2-hop>\n\nEcoAndroid\n• Plugin for IntelliJ (Android Studio)\n• Dynamic Retry Delay\n• Push Over Poll\n• Reduce Size\n• Cache\n• Avoid Graphics and Animations\n20', '<1-hop>\n\nShuffle \n🔀\n• It is not a mystery that energy consumption depends on so many factors that \nit is impossible to control all of them.\n• If you run 30 executions for version A and another batch for version B:\n• External conditions that change over time will have a diﬀerent bias in \nthe 2 versions (e.g., room temperature changes).\n• If you shuﬄe, you reduce this risk.\n11', '<2-hop>\n\nEcoAndroid\n• Plugin for IntelliJ (Android Studio)\n• Dynamic Retry Delay\n• Push Over Poll\n• Reduce Size\n• Cache\n• Avoid Graphics and Animations\n20']","Shuffling reduces risk in software engineering experiments by minimizing the bias introduced by external conditions that change over time. For example, if you run multiple executions for different versions of a software, external factors like room temperature can vary and affect the results. By shuffling the order of these executions, you reduce the likelihood that these external factors will consistently bias one version over another, leading to more reliable and unbiased results.","Shuffling reduce risk by controlling external factors like room temperature changes, which help in reducing size in software engineering.",0.0,0.0,1.0,0.8,0.0,0.9557516651861332
How spiking neural networks relate to McCulloch-Pitts model?,"[""Neuromorphic computing\nCS4575\nSustainable Software Engineering\n05.03.2025\nNergis Tömen\n\nIntroduction\nComputer Vision Lab\nBiomorphic Intelligence Lab\nBiomedical Intervention Optimisation Lab\nNergis\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing\n\nHow many of you are familiar \nwith neural networks?\n\nSimple model of an artificial neuron\nhttps://medium.com/@cprasenjit32/perceptron-a-simple-yet-mighty-machine-learning-algorithm-9ff6b7d86a71\n\nWhat is a neural network?\nhttps://www.tibco.com/reference-center/what-is-a-neural-network\n\nWhy is it called a 'neural network'?\nhttps://www.tibco.com/reference-center/what-is-a-neural-network\n\nWhat is a neural network?\nWhy is it called a 'neural network'?\nMcCulloch-Pitts neuron [1, 2] (1943)\nWarren Sturgis McCulloch\n(Neurophysiologist)\nWalter Pitts\n(Logician)\n\nHow do biological neurons work?\nhttps://commons.wikimedia.org/wiki/File:Coincidence_detection_in_dendrites_of_pyramidal_neurons.gif\n'Input current' travels down the dendrites (top),\nget integrated (summed!) in the cell body\nwhich generates an 'output current' (bottom)\nwhich is chemically transmitted to the dendrites of \nother neurons.\n\nSimplified picture\nReal neuron\nArtificial neuron\nhttps://vajiramandravi.com/quest-upsc-notes/artificial-neural-network/\n\nWhat is a neural network?\nMcCulloch-Pitts neuron [1, 2]\n1940s: How do biological neurons compute basic \nlogic functions? (e.g. logic gates)\nNote: Ref. [3] gives a nice brief history on the ideas which lead to the \nMcCulloch-Pitts neuron.\n\nWhat is a neural network?"", 'How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch–Pitts neural networks. Journal of the History of the Behavioral \nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \n(not publicly available) doi:10.1037/h0042519\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\n6) https://github.com/idsc-frazzoli/retina\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\n10) Gütig R, Sompolinsky H. The tempotron: a neuron that learns spike timing–based decisions. Nature neuroscience.']","[""<1-hop>\n\nEncoding strategies\nWe considered single neurons with Boolean output (‘spike’=1 or ‘no spike’=0).\nHow should we encode information about 'features' in a large network with many spikes?\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\nElephant?\nhttps://www.nationalgeographic.com/anim\nals/mammals/facts/african-elephant"", '<2-hop>\n\nReferences\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch–Pitts neural networks. Journal of the History of the Behavioral \nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \n(not publicly available) doi:10.1037/h0042519\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\n6) https://github.com/idsc-frazzoli/retina\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\n10) Gütig R, Sompolinsky H. The tempotron: a neuron that learns spike timing–based decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568', ""<1-hop>\n\nEncoding strategies\nWe considered single neurons with Boolean output (‘spike’=1 or ‘no spike’=0).\nHow should we encode information about 'features' in a large network with many spikes?\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\nElephant?\nhttps://www.nationalgeographic.com/anim\nals/mammals/facts/african-elephant"", '<2-hop>\n\nReferences\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch–Pitts neural networks. Journal of the History of the Behavioral \nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \n(not publicly available) doi:10.1037/h0042519\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\n6) https://github.com/idsc-frazzoli/retina\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\n10) Gütig R, Sompolinsky H. The tempotron: a neuron that learns spike timing–based decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568', ""<1-hop>\n\nEncoding strategies\nWe considered single neurons with Boolean output (‘spike’=1 or ‘no spike’=0).\nHow should we encode information about 'features' in a large network with many spikes?\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\nElephant?\nhttps://www.nationalgeographic.com/anim\nals/mammals/facts/african-elephant"", '<2-hop>\n\nReferences\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch–Pitts neural networks. Journal of the History of the Behavioral \nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \n(not publicly available) doi:10.1037/h0042519\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\n6) https://github.com/idsc-frazzoli/retina\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\n10) Gütig R, Sompolinsky H. The tempotron: a neuron that learns spike timing–based decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568']","The McCulloch-Pitts model, developed in the 1940s, is one of the earliest models of artificial neurons, designed to compute basic logic functions. It represents neurons as simple binary threshold units that output a signal if the sum of their weighted inputs exceeds a certain threshold.

Spiking neural networks (SNNs), on the other hand, are a more advanced and biologically realistic model of neural networks. They incorporate the concept of time into the neural processing, where neurons communicate by sending discrete spikes at particular points in time, rather than continuous values. This allows SNNs to capture the temporal dynamics of biological neural systems more accurately.

While the McCulloch-Pitts model laid the groundwork for understanding how neurons could perform logical operations, spiking neural networks extend this by modeling the timing of spikes, which is crucial for understanding more complex neural processes. Thus, SNNs can be seen as an evolution of the ideas introduced by the McCulloch-Pitts model, incorporating more biological realism.","Spiking neural networks relate to the McCulloch-Pitts model by building upon the foundational concepts of neural computation, where the McCulloch-Pitts model introduced the idea of binary neuron states, which is extended in spiking networks to model more complex information processing through spike timing and patterns.",0.49999999995,1.0,1.0,0.89,0.0,0.9355391667219908
How spiking neural networks used in encoding strategies?,"[""ANN: Perceptron, threshold activation function:\n\nHow to model spiking neurons?\nANN: Perceptron, threshold activation function:\n\nHow to model spiking neurons?\nSpiking neural network (SNN): The 'input current' ε(t) is integrated over time.\nspike\n\nHow to model spiking neurons?\nEquivalence to perceptron: Computation at least as complex as a perceptron.\n\nNon-leaky integrate-and-fire (IF) neuron\nThe temporal profile of the input current ε(t) can be chosen differently, for different computations.\n\nQuestions?\n\nComputation with spiking neurons\nCoincidence detection\n\nComputation with spiking neurons\nCoincidence detection\nx, y ∈ {0, 1}n\nExample: n = 2\ninput = \nx1\nx2\ny1\ny2\n= \n0\n1\n1\n1\noutput? \n\nCoincidence detection\nComputation with spiking neurons\n\nCoincidence detection\nCan be trivially computed with a single spiking neuron! Requires at least n/log(n+1) hidden \nunits for a perceptron (proof in [7]).\nComputation with spiking neurons\n\nCoincidence detection\nCan be trivially computed with a single spiking neuron! Requires at least n/log(n+1) hidden \nunits for a perceptron (proof in [7]).\nBrains are energy efficient: \n1.\nHigh temporal resolution (more computation with less neurons)\nComputation with spiking neurons\n\nEncoding strategies\nWe considered single neurons with Boolean output (‘spike’=1 or ‘no spike’=0).\nHow should we encode information about 'features' in a large network with many spikes?\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\nElephant?\nhttps://www.nationalgeographic.com/anim\nals/mammals/facts/african-elephant\n\nFiring rates\nClassical view of the brain:\n- Each neuron is selective for one specific \nfeature in the input.\n- Higher firing rate (spikes per unit time) for \n'selected' feature.\nAdapted from Hubel & Wiesel, 1959\n\nFiring rates\nClassical view of the brain:\n- Each neuron is selective for one specific \nfeature in the input.\n- Higher firing rate (spikes per unit time) for \n'selected' feature."", ""- Higher firing rate (spikes per unit time) for \n'selected' feature.\nAdapted from Hubel & Wiesel, 1959\n\nFiring rates\nClassical view of the brain:\n- Each neuron is selective for one specific \nfeature in the input.\n- Higher firing rate (spikes per unit time) for \n'selected' feature.\n- Link to modern ANNs: The scalar output of \nan artificial neuron is interpreted as the firing \nrate.\nAdapted from Hubel & Wiesel, 1959\n\nFiring rates\nClassical view of the brain:\n- Each neuron is selective for one specific \nfeature in the input.\n- Higher firing rate (spikes per unit time) for \n'selected' feature.\n- Link to modern ANNs: The scalar output of \nan artificial neuron is interpreted as the firing \nrate.\nAdapted from Hubel & Wiesel, 1959\nBut rate coding is inefficient and slow…\n(i.e. each neuron needs to fire many spikes to get good precision)\nboth in vivo and in silico.\n\nDifferent encoding strategies with spiking neurons [8]\n7 neurons\n7 time bins\n\nDifferent encoding strategies with spiking neurons\n7 neurons\n7 time bins\nn\n6\n(in the general \ncase a \nnumber \nbetween 0-7)\n\nDifferent encoding strategies with spiking neurons\n7 neurons\n7 time bins\nn\nTotal spike \ncount can \nencode 3 bits.\n\nDifferent encoding strategies with spiking neurons\n7 neurons\n7 time bins\nIndividual rates \ncan encode 7 \nbits.\n\nDifferent encoding strategies with spiking neurons\n7 neurons\n7 time bins\nLatency can \nencode ~3*7 or \n~19 bits.\n\nDifferent encoding strategies with spiking neurons\n7 neurons\n7 time bins\nRank order can \nencode ~12 bits.\n\nDifferent encoding strategies with spiking neurons\n\nDifferent encoding strategies with spiking neurons\nUsing latency or spatio-temporal codes\nour encoding is faster and more spike-efficient (sparser)!\n\nDifferent encoding strategies with spiking neurons\nUsing latency or spatio-temporal codes\nour encoding is faster and more spike-efficient (sparser)!\nQuestions?\n\nNeuromorphic computing\nWhat is the advantage for applications?\n-\nLess spikes = less energy consumption in specialized neuromorphic hardware\n(e.g.""]","[""<1-hop>\n\nEncoding strategies\nWe considered single neurons with Boolean output (‘spike’=1 or ‘no spike’=0).\nHow should we encode information about 'features' in a large network with many spikes?\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\nElephant?\nhttps://www.nationalgeographic.com/anim\nals/mammals/facts/african-elephant"", '<2-hop>\n\nReferences\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch–Pitts neural networks. Journal of the History of the Behavioral \nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \n(not publicly available) doi:10.1037/h0042519\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\n6) https://github.com/idsc-frazzoli/retina\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\n10) Gütig R, Sompolinsky H. The tempotron: a neuron that learns spike timing–based decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568']","Spiking neural networks (SNNs) use various encoding strategies to efficiently represent information. These strategies include:

1. **Firing Rates**: Each neuron is selective for a specific feature, and a higher firing rate (spikes per unit time) indicates the presence of that feature. However, this method is considered inefficient and slow because it requires many spikes to achieve good precision.

2. **Total Spike Count**: This method can encode information by counting the total number of spikes, which can represent multiple bits of information.

3. **Individual Rates**: By considering the firing rates of individual neurons, more bits of information can be encoded.

4. **Latency**: The timing of spikes (latency) can encode a significant amount of information, making it a fast and efficient encoding strategy.

5. **Rank Order**: The order in which neurons fire can also encode information, allowing for a compact representation.

Using latency or spatio-temporal codes, SNNs achieve faster and more spike-efficient encoding, which is beneficial for reducing energy consumption in neuromorphic computing applications.",Spiking neural networks use single neurons with Boolean output (‘spike’=1 or ‘no spike’=0) to encode information about 'features' in a large network with many spikes. This approach is supported by foundational works such as McCulloch and Pitts' logical calculus of nervous activity and later developments in spiking neuron models.,0.9999999999,0.5,1.0,0.0,0.0,0.943518823072558
What are the benefits of neuromorphic computing in the context of green software practices?,"[""Neuromorphic computing\nCS4575\nSustainable Software Engineering\n05.03.2025\nNergis Tömen\n\nIntroduction\nComputer Vision Lab\nBiomorphic Intelligence Lab\nBiomedical Intervention Optimisation Lab\nNergis\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing\n\nHow many of you are familiar \nwith neural networks?\n\nSimple model of an artificial neuron\nhttps://medium.com/@cprasenjit32/perceptron-a-simple-yet-mighty-machine-learning-algorithm-9ff6b7d86a71\n\nWhat is a neural network?\nhttps://www.tibco.com/reference-center/what-is-a-neural-network\n\nWhy is it called a 'neural network'?\nhttps://www.tibco.com/reference-center/what-is-a-neural-network\n\nWhat is a neural network?\nWhy is it called a 'neural network'?\nMcCulloch-Pitts neuron [1, 2] (1943)\nWarren Sturgis McCulloch\n(Neurophysiologist)\nWalter Pitts\n(Logician)\n\nHow do biological neurons work?\nhttps://commons.wikimedia.org/wiki/File:Coincidence_detection_in_dendrites_of_pyramidal_neurons.gif\n'Input current' travels down the dendrites (top),\nget integrated (summed!) in the cell body\nwhich generates an 'output current' (bottom)\nwhich is chemically transmitted to the dendrites of \nother neurons.\n\nSimplified picture\nReal neuron\nArtificial neuron\nhttps://vajiramandravi.com/quest-upsc-notes/artificial-neural-network/\n\nWhat is a neural network?\nMcCulloch-Pitts neuron [1, 2]\n1940s: How do biological neurons compute basic \nlogic functions? (e.g. logic gates)\nNote: Ref. [3] gives a nice brief history on the ideas which lead to the \nMcCulloch-Pitts neuron.\n\nWhat is a neural network?"", 'McCulloch-Pitts neuron [1, 2]\n1940s: How do biological neurons compute basic \nlogic functions? (e.g. logic gates)\nNote: Ref. [3] gives a nice brief history on the ideas which lead to the \nMcCulloch-Pitts neuron.\n\nWhat is a neural network?\n1950s: How are neurons organized to perform \nsensory perception?\n\nWhat is a neural network?\n1950s: How are neurons organized to perform \nsensory perception?\nThe first ""neural network"": Perceptron (1958). [4]\nhttps://www.reddit.com/r/interestingasfuck/comments/e8a8oy/frank_rosenblatt_with_a_mark_i_perceptron/\nFrank Rosenblatt (Psychologist)\nwith a Mark I Perceptron computer in 1960\n\nWhat is a neural network?\n1950s: How are neurons organized to perform \nsensory perception?\nThe first ""neural network"": Perceptron (1958). [4]\nhttps://www.reddit.com/r/interestingasfuck/comments/e8a8oy/frank_rosenblatt_with_a_mark_i_perceptron/\nFrank Rosenblatt (Psychologist)\nwith a Mark I Perceptron computer in 1960\nPower-efficient\n\nNeuromorphic computing\nWhy neuromorphic computing?\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\n\nNeuromorphic computing\nWhy neuromorphic computing?\n‘Biological inspiration’ for artificial neural networks (ANNs) is not a new idea.\nEmulation (as opposed to simulation) of neural networks in hardware is not a new idea.\nQuestion: What can we gain from increasing biological realism in existing neural networks?\n\nPower-efficiency at scale\nModern, deep neural networks*\nare trained using GPUs.\n* It is estimated that ChatGPT was trained on \n10,000-20,000 GPUs and that it will require\n30,000 GPUs to keep running stably in the future.\n* It is estimated that ChatGPT has 10-20 billion \nparameters.']","['<1-hop>\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\n\nReading materials\nMain reading:\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n- Neuromorphic computing:\n- Based on biology: Zenke F, Bohté SM, Clopath C, Comşa IM, Göltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \nhttps://www.nature.com/articles/s41586-019-1677-2\nExtra reading:\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\n- Also an important part of neuromorphic systems and vision → Event Cameras: Gallego G, Delbrück T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\nBasics of \'conventional\' neural networks:\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.', '<1-hop>\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\n\nReading materials\nMain reading:\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n- Neuromorphic computing:\n- Based on biology: Zenke F, Bohté SM, Clopath C, Comşa IM, Göltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \nhttps://www.nature.com/articles/s41586-019-1677-2\nExtra reading:\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\n- Also an important part of neuromorphic systems and vision → Event Cameras: Gallego G, Delbrück T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\nBasics of \'conventional\' neural networks:\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.', '<1-hop>\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\n\nReading materials\nMain reading:\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n- Neuromorphic computing:\n- Based on biology: Zenke F, Bohté SM, Clopath C, Comşa IM, Göltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \nhttps://www.nature.com/articles/s41586-019-1677-2\nExtra reading:\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\n- Also an important part of neuromorphic systems and vision → Event Cameras: Gallego G, Delbrück T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\nBasics of \'conventional\' neural networks:\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.']","Neuromorphic computing offers benefits in the context of green software practices primarily through its power-efficiency at scale. Neuromorphic systems use physical artificial neurons to perform computations, which can be more energy-efficient compared to traditional computing systems, especially those using GPUs for training modern, deep neural networks. This efficiency can lead to reduced energy consumption and lower carbon footprints, aligning with the goals of sustainable software engineering.","Neuromorphic computing offers several benefits in the context of green software practices. It is inspired by the structure and function of the human brain, making it highly energy-efficient compared to traditional computing methods. This efficiency is particularly relevant for sustainable software development, as it reduces the environmental impact of computing systems. Additionally, neuromorphic systems can process information in a way that mimics biological neural networks, which can lead to more efficient and sustainable computing solutions. These aspects make neuromorphic computing a promising approach for developers looking to implement green software practices.",0.49999999995,1.0,1.0,0.5,,0.974256536756406
What are the challenges in creating a dataset of government-developed OS software?,"['Study test generation energy consumption\n• Automatic test generation mainly focuses on making strong test suites \n• Do different techniques and configurations impact the energy consumption \nduring generation? \n• Preferably focus on non-LLM test generation methods (EvoSuite, Pyguin, \nDSpot)\n\nB3. Detailed energy profiling of build pipelines\n• Automatic builds have become a cornerstone of quality assurance. But how \nmuch energy do they even consume? \n• Create a tool that reports on the energy consumed during the (different stages \nof the) whole build (compile, build, test, package, …) \n• Should be integrated with build system(s), making setup for developers easy \n• For local setup (to enable true energy measurements)\n\nB4. Energy profiling of static analysis tools\n• What is the energy consumption of a ""typical run"" for a few OSS projects? \n• Does the type of analysis matter? Are certain analysis more expensive? Does \nthe number of rules that are activated in a static analysis tool important for the \nenergy consumption? \n• Differences between static analysis tools [lower priority]\n\nB5. Tool supporting SusAF workshop / process\n• Lead engineers through process & questions \n• Interface to create & document the two diagrams \n• (!) Should be easy to start using & set up \n• You may also create a simplified version / \u2028\nyour favorite sustainability framework\nEnvironmental\nEconomic\nTechnical\nIndividual\nSocial\nImmediate\nEnabling\nStructural\nAirbnb\ngreater \nearnings\nincrease \nin rents\ngentrification\ngreater racial \ndisparities\nrent \nrooms\n5\n1\n3\n4\n2\n \n \n \n \n \n \nVery \nunlikely \nVery \nlikely \n\nB6. Dataset of government-developed OS software\n• Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise \n• But studying government software is difficult b/c we don’t know what is out \nthere\u2028\n→ Lack of incentive to make popular \u2028\n→ Language barriers \n• Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation? \n• Could start with NL, but including your / other countries greatly appreciated!\n\nB7.', '• Deadline April 4.\n6\n\nStrategy\n• Week 0 \n• Decide project idea (today) \n• Define steering meeting schedule \n• Create working document of the \narticle: Problem statement and solution \nproposal! \n• Define and assign tasks for each week. \n• Week 1 \n• Implementation \n• Agreements with supervisor. \n• Week 2 \n• Implementation \n• Week 3 \n• Implementation, Full draft of article, \ndissemination. \n• Week 4 \n• Final refinements \n• Prepare presentation\n7\n\nProject ideas\n• A1. Prototype cross-machine comparable benchmarks \n• A2. Add energy metrics to LMstudio/Ollama \n• A3. Visualizations built-in with EnergiBridge \n• A4. Service-based version of EnergiBridge\n\nProject ideas\n• B1. Measure energy consumption of single JUnit tests \n• B2. Study test generation energy consumption \n• B3. Energy profiling of static analysis tools \n• B4. Detailed energy profiling of build pipelines \n• B5. Tool supporting SusAF workshop / process \n• B6. Dataset of government-developed OS software \n• B7. Queue - but better for the student / TA society\nQuality Assurance & Testing\nSocial & Individual \u2028\nSustainability\n\nProject ideas\n• C1. Compare energy consumption of docker images for ML workloads. \n• C2. Create a plugin to visualize Hugging Face carbon emissions in detail. \n• C3. Plugin for ChatGPT (footprint per chat window) \n• C4. NutriScore for software libraries. \n• C5. Add energy-awareness to existing software \n• C6. Green Shift Left \n• C7. Education for Sustainable SE\n\nA1. Prototype cross-machine comparable benchmarks\n• Energy-usage comparisons require running both baseline + software on the \nsame machine → limits how extensive our experiments can be \n• Research community is in need of benchmarks that make energy \nmeasurements comparable even if executed on different machines \n• Focus on a single task or model (i.e. computing vision, classification)\n\nA2.']","['<1-hop>\n\nB6. Dataset of government-developed OS software\n• Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise\n• But studying government software is diﬃcult b/c we don’t know what is out \nthere\u2028\n→ Lack of incentive to make popular \u2028\n→ Language barriers\n• Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation?\n• Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\n\nProject ideas\n• B1. Measure energy consumption of single JUnit tests \n• B2. Study test generation energy consumption\n• B3. Energy proﬁling of static analysis tools\n• B4. Detailed energy proﬁling of build pipelines\n• B5. Tool supporting SusAF workshop / process\n• B6. Dataset of government-developed OS software\n• B7. Queue - but better for the student / TA society\nQuality Assurance & Testing\nSocial & Individual \u2028\nSustainability', '<1-hop>\n\nB6. Dataset of government-developed OS software\n• Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise\n• But studying government software is diﬃcult b/c we don’t know what is out \nthere\u2028\n→ Lack of incentive to make popular \u2028\n→ Language barriers\n• Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation?\n• Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\n\nProject ideas\n• B1. Measure energy consumption of single JUnit tests \n• B2. Study test generation energy consumption\n• B3. Energy proﬁling of static analysis tools\n• B4. Detailed energy proﬁling of build pipelines\n• B5. Tool supporting SusAF workshop / process\n• B6. Dataset of government-developed OS software\n• B7. Queue - but better for the student / TA society\nQuality Assurance & Testing\nSocial & Individual \u2028\nSustainability', '<1-hop>\n\nB6. Dataset of government-developed OS software\n• Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise\n• But studying government software is diﬃcult b/c we don’t know what is out \nthere\u2028\n→ Lack of incentive to make popular \u2028\n→ Language barriers\n• Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation?\n• Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\n\nProject ideas\n• B1. Measure energy consumption of single JUnit tests \n• B2. Study test generation energy consumption\n• B3. Energy proﬁling of static analysis tools\n• B4. Detailed energy proﬁling of build pipelines\n• B5. Tool supporting SusAF workshop / process\n• B6. Dataset of government-developed OS software\n• B7. Queue - but better for the student / TA society\nQuality Assurance & Testing\nSocial & Individual \u2028\nSustainability']","The challenges in creating a dataset of government-developed open-source software include:

1. **Lack of Incentive to Make Popular**: There is often little motivation for governments to promote their software, leading to a lack of visibility and awareness.

2. **Language Barriers**: Government software may be developed in various languages, making it difficult to compile a comprehensive dataset that is accessible and understandable to a wider audience.

3. **Understanding the State of Software**: It is challenging to gather data that provides insights into whether the software is buildable, has an open development history, and includes requirements documentation.

These challenges make it difficult to study and compile a comprehensive dataset of government-developed open-source software.","Creating a dataset of government-developed OS software faces challenges such as a lack of incentive to make the software popular and language barriers, which hinder comprehensive understanding of the software's buildability, open development history, and requirements documentation.",0.9999999999,1.0,0.8888888888888888,1.0,0.6666666655555555,0.9696515751694866
"What are the practical applications of neuromorphic computing, and how does its biological inspiration influence its implementation in modern computing systems?","['McCulloch-Pitts neuron [1, 2]\n1940s: How do biological neurons compute basic \nlogic functions? (e.g. logic gates)\nNote: Ref. [3] gives a nice brief history on the ideas which lead to the \nMcCulloch-Pitts neuron.\n\nWhat is a neural network?\n1950s: How are neurons organized to perform \nsensory perception?\n\nWhat is a neural network?\n1950s: How are neurons organized to perform \nsensory perception?\nThe first ""neural network"": Perceptron (1958). [4]\nhttps://www.reddit.com/r/interestingasfuck/comments/e8a8oy/frank_rosenblatt_with_a_mark_i_perceptron/\nFrank Rosenblatt (Psychologist)\nwith a Mark I Perceptron computer in 1960\n\nWhat is a neural network?\n1950s: How are neurons organized to perform \nsensory perception?\nThe first ""neural network"": Perceptron (1958). [4]\nhttps://www.reddit.com/r/interestingasfuck/comments/e8a8oy/frank_rosenblatt_with_a_mark_i_perceptron/\nFrank Rosenblatt (Psychologist)\nwith a Mark I Perceptron computer in 1960\nPower-efficient\n\nNeuromorphic computing\nWhy neuromorphic computing?\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\n\nNeuromorphic computing\nWhy neuromorphic computing?\n‘Biological inspiration’ for artificial neural networks (ANNs) is not a new idea.\nEmulation (as opposed to simulation) of neural networks in hardware is not a new idea.\nQuestion: What can we gain from increasing biological realism in existing neural networks?\n\nPower-efficiency at scale\nModern, deep neural networks*\nare trained using GPUs.\n* It is estimated that ChatGPT was trained on \n10,000-20,000 GPUs and that it will require\n30,000 GPUs to keep running stably in the future.\n* It is estimated that ChatGPT has 10-20 billion \nparameters.', ""Neuromorphic computing\nCS4575\nSustainable Software Engineering\n05.03.2025\nNergis Tömen\n\nIntroduction\nComputer Vision Lab\nBiomorphic Intelligence Lab\nBiomedical Intervention Optimisation Lab\nNergis\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing\n\nHow many of you are familiar \nwith neural networks?\n\nSimple model of an artificial neuron\nhttps://medium.com/@cprasenjit32/perceptron-a-simple-yet-mighty-machine-learning-algorithm-9ff6b7d86a71\n\nWhat is a neural network?\nhttps://www.tibco.com/reference-center/what-is-a-neural-network\n\nWhy is it called a 'neural network'?\nhttps://www.tibco.com/reference-center/what-is-a-neural-network\n\nWhat is a neural network?\nWhy is it called a 'neural network'?\nMcCulloch-Pitts neuron [1, 2] (1943)\nWarren Sturgis McCulloch\n(Neurophysiologist)\nWalter Pitts\n(Logician)\n\nHow do biological neurons work?\nhttps://commons.wikimedia.org/wiki/File:Coincidence_detection_in_dendrites_of_pyramidal_neurons.gif\n'Input current' travels down the dendrites (top),\nget integrated (summed!) in the cell body\nwhich generates an 'output current' (bottom)\nwhich is chemically transmitted to the dendrites of \nother neurons.\n\nSimplified picture\nReal neuron\nArtificial neuron\nhttps://vajiramandravi.com/quest-upsc-notes/artificial-neural-network/\n\nWhat is a neural network?\nMcCulloch-Pitts neuron [1, 2]\n1940s: How do biological neurons compute basic \nlogic functions? (e.g. logic gates)\nNote: Ref. [3] gives a nice brief history on the ideas which lead to the \nMcCulloch-Pitts neuron.\n\nWhat is a neural network?""]","['<1-hop>\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\n\nReading materials\nMain reading:\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n- Neuromorphic computing:\n- Based on biology: Zenke F, Bohté SM, Clopath C, Comşa IM, Göltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \nhttps://www.nature.com/articles/s41586-019-1677-2\nExtra reading:\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\n- Also an important part of neuromorphic systems and vision → Event Cameras: Gallego G, Delbrück T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\nBasics of \'conventional\' neural networks:\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.', '<1-hop>\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\n\nReading materials\nMain reading:\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n- Neuromorphic computing:\n- Based on biology: Zenke F, Bohté SM, Clopath C, Comşa IM, Göltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \nhttps://www.nature.com/articles/s41586-019-1677-2\nExtra reading:\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\n- Also an important part of neuromorphic systems and vision → Event Cameras: Gallego G, Delbrück T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\nBasics of \'conventional\' neural networks:\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.']","Neuromorphic computing is inspired by the structure and function of the human brain, and it involves using physical artificial neurons to perform computations. This approach offers several practical applications and advantages:

1. **Power Efficiency**: Neuromorphic computing systems are designed to be power-efficient, which is particularly beneficial for large-scale computations. This efficiency is crucial for applications like deep neural networks, which require significant computational resources.

2. **Real-Time Processing**: Due to their architecture, neuromorphic systems can process information in real-time, making them suitable for applications that require immediate responses, such as autonomous vehicles and robotics.

3. **Sensory Perception**: Neuromorphic computing can enhance sensory perception tasks, such as image and speech recognition, by mimicking the way biological systems process sensory information.

4. **Scalability**: The emulation of neural networks in hardware allows for scalable solutions that can handle complex tasks more efficiently than traditional computing systems.

The biological inspiration behind neuromorphic computing influences its implementation by encouraging the development of systems that mimic the parallel and distributed nature of the brain's neural networks. This leads to computing systems that are not only more efficient but also capable of handling complex, dynamic tasks in a manner similar to biological organisms.","Neuromorphic computing, inspired by the human brain, offers practical applications such as event-based vision systems and efficient processing of sensory data. Its biological inspiration influences its implementation through the use of spiking neural networks and physical artificial neurons, enabling more energy-efficient and brain-like computation methods.",0.9999999999,1.0,0.8571428571428571,0.55,0.3333333327777777,0.9544250285082488
"What are the practical applications of neuromorphic computing, and how does its biological inspiration influence its implementation?","['McCulloch-Pitts neuron [1, 2]\n1940s: How do biological neurons compute basic \nlogic functions? (e.g. logic gates)\nNote: Ref. [3] gives a nice brief history on the ideas which lead to the \nMcCulloch-Pitts neuron.\n\nWhat is a neural network?\n1950s: How are neurons organized to perform \nsensory perception?\n\nWhat is a neural network?\n1950s: How are neurons organized to perform \nsensory perception?\nThe first ""neural network"": Perceptron (1958). [4]\nhttps://www.reddit.com/r/interestingasfuck/comments/e8a8oy/frank_rosenblatt_with_a_mark_i_perceptron/\nFrank Rosenblatt (Psychologist)\nwith a Mark I Perceptron computer in 1960\n\nWhat is a neural network?\n1950s: How are neurons organized to perform \nsensory perception?\nThe first ""neural network"": Perceptron (1958). [4]\nhttps://www.reddit.com/r/interestingasfuck/comments/e8a8oy/frank_rosenblatt_with_a_mark_i_perceptron/\nFrank Rosenblatt (Psychologist)\nwith a Mark I Perceptron computer in 1960\nPower-efficient\n\nNeuromorphic computing\nWhy neuromorphic computing?\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\n\nNeuromorphic computing\nWhy neuromorphic computing?\n‘Biological inspiration’ for artificial neural networks (ANNs) is not a new idea.\nEmulation (as opposed to simulation) of neural networks in hardware is not a new idea.\nQuestion: What can we gain from increasing biological realism in existing neural networks?\n\nPower-efficiency at scale\nModern, deep neural networks*\nare trained using GPUs.\n* It is estimated that ChatGPT was trained on \n10,000-20,000 GPUs and that it will require\n30,000 GPUs to keep running stably in the future.\n* It is estimated that ChatGPT has 10-20 billion \nparameters.', ""Neuromorphic computing\nCS4575\nSustainable Software Engineering\n05.03.2025\nNergis Tömen\n\nIntroduction\nComputer Vision Lab\nBiomorphic Intelligence Lab\nBiomedical Intervention Optimisation Lab\nNergis\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing\n\nHow many of you are familiar \nwith neural networks?\n\nSimple model of an artificial neuron\nhttps://medium.com/@cprasenjit32/perceptron-a-simple-yet-mighty-machine-learning-algorithm-9ff6b7d86a71\n\nWhat is a neural network?\nhttps://www.tibco.com/reference-center/what-is-a-neural-network\n\nWhy is it called a 'neural network'?\nhttps://www.tibco.com/reference-center/what-is-a-neural-network\n\nWhat is a neural network?\nWhy is it called a 'neural network'?\nMcCulloch-Pitts neuron [1, 2] (1943)\nWarren Sturgis McCulloch\n(Neurophysiologist)\nWalter Pitts\n(Logician)\n\nHow do biological neurons work?\nhttps://commons.wikimedia.org/wiki/File:Coincidence_detection_in_dendrites_of_pyramidal_neurons.gif\n'Input current' travels down the dendrites (top),\nget integrated (summed!) in the cell body\nwhich generates an 'output current' (bottom)\nwhich is chemically transmitted to the dendrites of \nother neurons.\n\nSimplified picture\nReal neuron\nArtificial neuron\nhttps://vajiramandravi.com/quest-upsc-notes/artificial-neural-network/\n\nWhat is a neural network?\nMcCulloch-Pitts neuron [1, 2]\n1940s: How do biological neurons compute basic \nlogic functions? (e.g. logic gates)\nNote: Ref. [3] gives a nice brief history on the ideas which lead to the \nMcCulloch-Pitts neuron.\n\nWhat is a neural network?""]","['<1-hop>\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\n\nReading materials\nMain reading:\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n- Neuromorphic computing:\n- Based on biology: Zenke F, Bohté SM, Clopath C, Comşa IM, Göltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \nhttps://www.nature.com/articles/s41586-019-1677-2\nExtra reading:\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\n- Also an important part of neuromorphic systems and vision → Event Cameras: Gallego G, Delbrück T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\nBasics of \'conventional\' neural networks:\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.']","The practical applications of neuromorphic computing include areas such as computer vision, robotics, and biomedical intervention optimization. Neuromorphic computing is particularly useful in scenarios where power efficiency is crucial, as it aims to emulate the brain's structure and function, which is highly efficient in processing information.

The biological inspiration for neuromorphic computing influences its implementation by using physical artificial neurons to perform computations, mimicking the way biological neurons process information. This approach allows for more efficient and scalable computing systems, as it leverages the brain's ability to perform complex tasks with minimal energy consumption. This emulation of neural networks in hardware is not a new idea, but it continues to offer potential for advancements in creating more efficient and powerful computing systems.","Neuromorphic computing, inspired by the human brain, offers practical applications through its ability to process information in a manner similar to biological neurons. This biological inspiration influences its implementation by enabling the development of systems that can efficiently handle tasks such as pattern recognition and adaptive learning, as detailed in resources like 'Computing with spiking neuron networks' and discussions on the convergence of deep learning with biological systems.",0.0,0.3333333333333333,0.8333333333333334,0.53,0.4285714279591837,0.9892387962727458
What are the practical applications of neuromorphic systems that leverage biological inspiration and advanced training methods for efficient computation?,"['McCulloch-Pitts neuron [1, 2]\n1940s: How do biological neurons compute basic \nlogic functions? (e.g. logic gates)\nNote: Ref. [3] gives a nice brief history on the ideas which lead to the \nMcCulloch-Pitts neuron.\n\nWhat is a neural network?\n1950s: How are neurons organized to perform \nsensory perception?\n\nWhat is a neural network?\n1950s: How are neurons organized to perform \nsensory perception?\nThe first ""neural network"": Perceptron (1958). [4]\nhttps://www.reddit.com/r/interestingasfuck/comments/e8a8oy/frank_rosenblatt_with_a_mark_i_perceptron/\nFrank Rosenblatt (Psychologist)\nwith a Mark I Perceptron computer in 1960\n\nWhat is a neural network?\n1950s: How are neurons organized to perform \nsensory perception?\nThe first ""neural network"": Perceptron (1958). [4]\nhttps://www.reddit.com/r/interestingasfuck/comments/e8a8oy/frank_rosenblatt_with_a_mark_i_perceptron/\nFrank Rosenblatt (Psychologist)\nwith a Mark I Perceptron computer in 1960\nPower-efficient\n\nNeuromorphic computing\nWhy neuromorphic computing?\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\n\nNeuromorphic computing\nWhy neuromorphic computing?\n‘Biological inspiration’ for artificial neural networks (ANNs) is not a new idea.\nEmulation (as opposed to simulation) of neural networks in hardware is not a new idea.\nQuestion: What can we gain from increasing biological realism in existing neural networks?\n\nPower-efficiency at scale\nModern, deep neural networks*\nare trained using GPUs.\n* It is estimated that ChatGPT was trained on \n10,000-20,000 GPUs and that it will require\n30,000 GPUs to keep running stably in the future.\n* It is estimated that ChatGPT has 10-20 billion \nparameters.', ""Neuromorphic computing\nCS4575\nSustainable Software Engineering\n05.03.2025\nNergis Tömen\n\nIntroduction\nComputer Vision Lab\nBiomorphic Intelligence Lab\nBiomedical Intervention Optimisation Lab\nNergis\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing\n\nHow many of you are familiar \nwith neural networks?\n\nSimple model of an artificial neuron\nhttps://medium.com/@cprasenjit32/perceptron-a-simple-yet-mighty-machine-learning-algorithm-9ff6b7d86a71\n\nWhat is a neural network?\nhttps://www.tibco.com/reference-center/what-is-a-neural-network\n\nWhy is it called a 'neural network'?\nhttps://www.tibco.com/reference-center/what-is-a-neural-network\n\nWhat is a neural network?\nWhy is it called a 'neural network'?\nMcCulloch-Pitts neuron [1, 2] (1943)\nWarren Sturgis McCulloch\n(Neurophysiologist)\nWalter Pitts\n(Logician)\n\nHow do biological neurons work?\nhttps://commons.wikimedia.org/wiki/File:Coincidence_detection_in_dendrites_of_pyramidal_neurons.gif\n'Input current' travels down the dendrites (top),\nget integrated (summed!) in the cell body\nwhich generates an 'output current' (bottom)\nwhich is chemically transmitted to the dendrites of \nother neurons.\n\nSimplified picture\nReal neuron\nArtificial neuron\nhttps://vajiramandravi.com/quest-upsc-notes/artificial-neural-network/\n\nWhat is a neural network?\nMcCulloch-Pitts neuron [1, 2]\n1940s: How do biological neurons compute basic \nlogic functions? (e.g. logic gates)\nNote: Ref. [3] gives a nice brief history on the ideas which lead to the \nMcCulloch-Pitts neuron.\n\nWhat is a neural network?""]","['<1-hop>\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\n\nReading materials\nMain reading:\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n- Neuromorphic computing:\n- Based on biology: Zenke F, Bohté SM, Clopath C, Comşa IM, Göltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \nhttps://www.nature.com/articles/s41586-019-1677-2\nExtra reading:\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\n- Also an important part of neuromorphic systems and vision → Event Cameras: Gallego G, Delbrück T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\nBasics of \'conventional\' neural networks:\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.', '<1-hop>\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\n\nReading materials\nMain reading:\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n- Neuromorphic computing:\n- Based on biology: Zenke F, Bohté SM, Clopath C, Comşa IM, Göltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \nhttps://www.nature.com/articles/s41586-019-1677-2\nExtra reading:\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\n- Also an important part of neuromorphic systems and vision → Event Cameras: Gallego G, Delbrück T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\nBasics of \'conventional\' neural networks:\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.', '<1-hop>\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\n\nReading materials\nMain reading:\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n- Neuromorphic computing:\n- Based on biology: Zenke F, Bohté SM, Clopath C, Comşa IM, Göltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \nhttps://www.nature.com/articles/s41586-019-1677-2\nExtra reading:\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\n- Also an important part of neuromorphic systems and vision → Event Cameras: Gallego G, Delbrück T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\nBasics of \'conventional\' neural networks:\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.']","The practical applications of neuromorphic systems, which leverage biological inspiration and advanced training methods for efficient computation, include:

1. **Power-Efficient Computing**: Neuromorphic systems are designed to mimic the brain's efficiency, making them highly power-efficient, especially at scale. This is particularly beneficial for applications requiring extensive computations, such as deep learning models.

2. **Real-Time Processing**: Due to their efficiency and speed, neuromorphic systems are well-suited for real-time processing tasks, such as those needed in autonomous vehicles and robotics.

3. **Sensory Perception**: These systems can be used for advanced sensory perception tasks, similar to how biological neurons are organized for sensory processing. This includes applications in computer vision and auditory processing.

4. **Adaptive Control Systems**: Neuromorphic systems can be used in adaptive control systems that require learning and adaptation in dynamic environments, such as in smart grid management or adaptive traffic control.

5. **Biomedical Applications**: They can be applied in biomedical fields for tasks like brain-machine interfaces and neural prosthetics, where efficient and real-time processing of neural signals is crucial.

These applications benefit from the neuromorphic approach's ability to perform computations in a manner similar to the human brain, offering advantages in efficiency and speed over traditional computing methods.","Neuromorphic systems, inspired by the human brain, have practical applications such as event-based vision, which is detailed in the reading materials. These systems use advanced training methods like surrogate gradient learning to optimize spiking neural networks, enabling efficient computation and real-world applications.",0.0,0.5,0.4444444444444444,0.47,0.0,0.9567597798554336
"What is the biological basis of neuromorphic computing, and how does it differ from conventional neural networks?","['McCulloch-Pitts neuron [1, 2]\n1940s: How do biological neurons compute basic \nlogic functions? (e.g. logic gates)\nNote: Ref. [3] gives a nice brief history on the ideas which lead to the \nMcCulloch-Pitts neuron.\n\nWhat is a neural network?\n1950s: How are neurons organized to perform \nsensory perception?\n\nWhat is a neural network?\n1950s: How are neurons organized to perform \nsensory perception?\nThe first ""neural network"": Perceptron (1958). [4]\nhttps://www.reddit.com/r/interestingasfuck/comments/e8a8oy/frank_rosenblatt_with_a_mark_i_perceptron/\nFrank Rosenblatt (Psychologist)\nwith a Mark I Perceptron computer in 1960\n\nWhat is a neural network?\n1950s: How are neurons organized to perform \nsensory perception?\nThe first ""neural network"": Perceptron (1958). [4]\nhttps://www.reddit.com/r/interestingasfuck/comments/e8a8oy/frank_rosenblatt_with_a_mark_i_perceptron/\nFrank Rosenblatt (Psychologist)\nwith a Mark I Perceptron computer in 1960\nPower-efficient\n\nNeuromorphic computing\nWhy neuromorphic computing?\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\n\nNeuromorphic computing\nWhy neuromorphic computing?\n‘Biological inspiration’ for artificial neural networks (ANNs) is not a new idea.\nEmulation (as opposed to simulation) of neural networks in hardware is not a new idea.\nQuestion: What can we gain from increasing biological realism in existing neural networks?\n\nPower-efficiency at scale\nModern, deep neural networks*\nare trained using GPUs.\n* It is estimated that ChatGPT was trained on \n10,000-20,000 GPUs and that it will require\n30,000 GPUs to keep running stably in the future.\n* It is estimated that ChatGPT has 10-20 billion \nparameters.', ""Neuromorphic computing\nCS4575\nSustainable Software Engineering\n05.03.2025\nNergis Tömen\n\nIntroduction\nComputer Vision Lab\nBiomorphic Intelligence Lab\nBiomedical Intervention Optimisation Lab\nNergis\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing\n\nHow many of you are familiar \nwith neural networks?\n\nSimple model of an artificial neuron\nhttps://medium.com/@cprasenjit32/perceptron-a-simple-yet-mighty-machine-learning-algorithm-9ff6b7d86a71\n\nWhat is a neural network?\nhttps://www.tibco.com/reference-center/what-is-a-neural-network\n\nWhy is it called a 'neural network'?\nhttps://www.tibco.com/reference-center/what-is-a-neural-network\n\nWhat is a neural network?\nWhy is it called a 'neural network'?\nMcCulloch-Pitts neuron [1, 2] (1943)\nWarren Sturgis McCulloch\n(Neurophysiologist)\nWalter Pitts\n(Logician)\n\nHow do biological neurons work?\nhttps://commons.wikimedia.org/wiki/File:Coincidence_detection_in_dendrites_of_pyramidal_neurons.gif\n'Input current' travels down the dendrites (top),\nget integrated (summed!) in the cell body\nwhich generates an 'output current' (bottom)\nwhich is chemically transmitted to the dendrites of \nother neurons.\n\nSimplified picture\nReal neuron\nArtificial neuron\nhttps://vajiramandravi.com/quest-upsc-notes/artificial-neural-network/\n\nWhat is a neural network?\nMcCulloch-Pitts neuron [1, 2]\n1940s: How do biological neurons compute basic \nlogic functions? (e.g. logic gates)\nNote: Ref. [3] gives a nice brief history on the ideas which lead to the \nMcCulloch-Pitts neuron.\n\nWhat is a neural network?""]","['<1-hop>\n\nNeuromorphic computing\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\n\nReading materials\nMain reading:\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n- Neuromorphic computing:\n- Based on biology: Zenke F, Bohté SM, Clopath C, Comşa IM, Göltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \nhttps://www.nature.com/articles/s41586-019-1677-2\nExtra reading:\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\n- Also an important part of neuromorphic systems and vision → Event Cameras: Gallego G, Delbrück T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\nBasics of \'conventional\' neural networks:\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.']","The biological basis of neuromorphic computing is its inspiration from the structure and function of the human brain. Neuromorphic computing involves using physical artificial neurons to perform computations, emulating the way biological neurons process information. This approach aims to mimic the brain's efficiency and adaptability.

In contrast, conventional neural networks, such as those used in deep learning, are typically implemented in software and run on general-purpose hardware like GPUs. These networks are inspired by the brain but do not emulate its physical structure. They are often less power-efficient compared to neuromorphic systems, which aim to achieve power-efficiency at scale by closely replicating the brain's architecture and processes.

In summary, neuromorphic computing seeks to emulate the brain's physical and functional characteristics in hardware, while conventional neural networks are software-based models inspired by the brain's information processing capabilities.","Neuromorphic computing is based on the biological principles of the human brain, utilizing physical artificial neurons to perform computations. Unlike conventional neural networks, which rely on traditional computing architectures, neuromorphic systems are designed to mimic the brain's structure and function, enabling more efficient and biologically inspired processing.",0.99999999995,0.5,1.0,,0.3333333327777777,0.9849766818157952
What is the difference between spiking neural networks and spiking neuron networks?,"[""ANN: Perceptron, threshold activation function:\n\nHow to model spiking neurons?\nANN: Perceptron, threshold activation function:\n\nHow to model spiking neurons?\nSpiking neural network (SNN): The 'input current' ε(t) is integrated over time.\nspike\n\nHow to model spiking neurons?\nEquivalence to perceptron: Computation at least as complex as a perceptron.\n\nNon-leaky integrate-and-fire (IF) neuron\nThe temporal profile of the input current ε(t) can be chosen differently, for different computations.\n\nQuestions?\n\nComputation with spiking neurons\nCoincidence detection\n\nComputation with spiking neurons\nCoincidence detection\nx, y ∈ {0, 1}n\nExample: n = 2\ninput = \nx1\nx2\ny1\ny2\n= \n0\n1\n1\n1\noutput? \n\nCoincidence detection\nComputation with spiking neurons\n\nCoincidence detection\nCan be trivially computed with a single spiking neuron! Requires at least n/log(n+1) hidden \nunits for a perceptron (proof in [7]).\nComputation with spiking neurons\n\nCoincidence detection\nCan be trivially computed with a single spiking neuron! Requires at least n/log(n+1) hidden \nunits for a perceptron (proof in [7]).\nBrains are energy efficient: \n1.\nHigh temporal resolution (more computation with less neurons)\nComputation with spiking neurons\n\nEncoding strategies\nWe considered single neurons with Boolean output (‘spike’=1 or ‘no spike’=0).\nHow should we encode information about 'features' in a large network with many spikes?\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\nElephant?\nhttps://www.nationalgeographic.com/anim\nals/mammals/facts/african-elephant\n\nFiring rates\nClassical view of the brain:\n- Each neuron is selective for one specific \nfeature in the input.\n- Higher firing rate (spikes per unit time) for \n'selected' feature.\nAdapted from Hubel & Wiesel, 1959\n\nFiring rates\nClassical view of the brain:\n- Each neuron is selective for one specific \nfeature in the input.\n- Higher firing rate (spikes per unit time) for \n'selected' feature."", 'Reading materials\nMain reading:\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n- Neuromorphic computing:\n- Based on biology: Zenke F, Bohté SM, Clopath C, Comşa IM, Göltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \nhttps://www.nature.com/articles/s41586-019-1677-2\nExtra reading:\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \nAcademy of Sciences. 2020.']","[""<1-hop>\n\nEncoding strategies\nWe considered single neurons with Boolean output (‘spike’=1 or ‘no spike’=0).\nHow should we encode information about 'features' in a large network with many spikes?\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\nElephant?\nhttps://www.nationalgeographic.com/anim\nals/mammals/facts/african-elephant"", '<2-hop>\n\nReferences\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch–Pitts neural networks. Journal of the History of the Behavioral \nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \n(not publicly available) doi:10.1037/h0042519\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\n6) https://github.com/idsc-frazzoli/retina\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\n10) Gütig R, Sompolinsky H. The tempotron: a neuron that learns spike timing–based decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568', ""<1-hop>\n\nEncoding strategies\nWe considered single neurons with Boolean output (‘spike’=1 or ‘no spike’=0).\nHow should we encode information about 'features' in a large network with many spikes?\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\nElephant?\nhttps://www.nationalgeographic.com/anim\nals/mammals/facts/african-elephant"", '<2-hop>\n\nReferences\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch–Pitts neural networks. Journal of the History of the Behavioral \nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \n(not publicly available) doi:10.1037/h0042519\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\n6) https://github.com/idsc-frazzoli/retina\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\n10) Gütig R, Sompolinsky H. The tempotron: a neuron that learns spike timing–based decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568']","The sources provided do not explicitly differentiate between ""spiking neural networks"" and ""spiking neuron networks."" However, based on general understanding, these terms are often used interchangeably to describe networks that use spiking neurons as their fundamental units. Both refer to a type of artificial neural network that mimics the way biological neurons communicate through discrete spikes or action potentials. If there is a specific context or distinction you are referring to, it might not be covered in the provided sources.","Spiking neural networks (SNNs) are a type of neural network model that processes information using spikes, similar to biological neurons. Spiking neuron networks refer to the individual components within these networks, focusing on how each neuron processes and communicates information through spikes.",0.0,0.7142857142857143,1.0,0.33,0.0,0.0
What is the foundational concepts of spiking neural networks and how they are used in encoding strategies?,"[""- Higher firing rate (spikes per unit time) for \n'selected' feature.\nAdapted from Hubel & Wiesel, 1959\n\nFiring rates\nClassical view of the brain:\n- Each neuron is selective for one specific \nfeature in the input.\n- Higher firing rate (spikes per unit time) for \n'selected' feature.\n- Link to modern ANNs: The scalar output of \nan artificial neuron is interpreted as the firing \nrate.\nAdapted from Hubel & Wiesel, 1959\n\nFiring rates\nClassical view of the brain:\n- Each neuron is selective for one specific \nfeature in the input.\n- Higher firing rate (spikes per unit time) for \n'selected' feature.\n- Link to modern ANNs: The scalar output of \nan artificial neuron is interpreted as the firing \nrate.\nAdapted from Hubel & Wiesel, 1959\nBut rate coding is inefficient and slow…\n(i.e. each neuron needs to fire many spikes to get good precision)\nboth in vivo and in silico.\n\nDifferent encoding strategies with spiking neurons [8]\n7 neurons\n7 time bins\n\nDifferent encoding strategies with spiking neurons\n7 neurons\n7 time bins\nn\n6\n(in the general \ncase a \nnumber \nbetween 0-7)\n\nDifferent encoding strategies with spiking neurons\n7 neurons\n7 time bins\nn\nTotal spike \ncount can \nencode 3 bits.\n\nDifferent encoding strategies with spiking neurons\n7 neurons\n7 time bins\nIndividual rates \ncan encode 7 \nbits.\n\nDifferent encoding strategies with spiking neurons\n7 neurons\n7 time bins\nLatency can \nencode ~3*7 or \n~19 bits.\n\nDifferent encoding strategies with spiking neurons\n7 neurons\n7 time bins\nRank order can \nencode ~12 bits.\n\nDifferent encoding strategies with spiking neurons\n\nDifferent encoding strategies with spiking neurons\nUsing latency or spatio-temporal codes\nour encoding is faster and more spike-efficient (sparser)!\n\nDifferent encoding strategies with spiking neurons\nUsing latency or spatio-temporal codes\nour encoding is faster and more spike-efficient (sparser)!\nQuestions?\n\nNeuromorphic computing\nWhat is the advantage for applications?\n-\nLess spikes = less energy consumption in specialized neuromorphic hardware\n(e.g."", ""ANN: Perceptron, threshold activation function:\n\nHow to model spiking neurons?\nANN: Perceptron, threshold activation function:\n\nHow to model spiking neurons?\nSpiking neural network (SNN): The 'input current' ε(t) is integrated over time.\nspike\n\nHow to model spiking neurons?\nEquivalence to perceptron: Computation at least as complex as a perceptron.\n\nNon-leaky integrate-and-fire (IF) neuron\nThe temporal profile of the input current ε(t) can be chosen differently, for different computations.\n\nQuestions?\n\nComputation with spiking neurons\nCoincidence detection\n\nComputation with spiking neurons\nCoincidence detection\nx, y ∈ {0, 1}n\nExample: n = 2\ninput = \nx1\nx2\ny1\ny2\n= \n0\n1\n1\n1\noutput? \n\nCoincidence detection\nComputation with spiking neurons\n\nCoincidence detection\nCan be trivially computed with a single spiking neuron! Requires at least n/log(n+1) hidden \nunits for a perceptron (proof in [7]).\nComputation with spiking neurons\n\nCoincidence detection\nCan be trivially computed with a single spiking neuron! Requires at least n/log(n+1) hidden \nunits for a perceptron (proof in [7]).\nBrains are energy efficient: \n1.\nHigh temporal resolution (more computation with less neurons)\nComputation with spiking neurons\n\nEncoding strategies\nWe considered single neurons with Boolean output (‘spike’=1 or ‘no spike’=0).\nHow should we encode information about 'features' in a large network with many spikes?\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\nElephant?\nhttps://www.nationalgeographic.com/anim\nals/mammals/facts/african-elephant\n\nFiring rates\nClassical view of the brain:\n- Each neuron is selective for one specific \nfeature in the input.\n- Higher firing rate (spikes per unit time) for \n'selected' feature.\nAdapted from Hubel & Wiesel, 1959\n\nFiring rates\nClassical view of the brain:\n- Each neuron is selective for one specific \nfeature in the input.\n- Higher firing rate (spikes per unit time) for \n'selected' feature.""]","[""<1-hop>\n\nEncoding strategies\nWe considered single neurons with Boolean output (‘spike’=1 or ‘no spike’=0).\nHow should we encode information about 'features' in a large network with many spikes?\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\nElephant?\nhttps://www.nationalgeographic.com/anim\nals/mammals/facts/african-elephant"", '<2-hop>\n\nReferences\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch–Pitts neural networks. Journal of the History of the Behavioral \nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \n(not publicly available) doi:10.1037/h0042519\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\n6) https://github.com/idsc-frazzoli/retina\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\n10) Gütig R, Sompolinsky H. The tempotron: a neuron that learns spike timing–based decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568', ""<1-hop>\n\nEncoding strategies\nWe considered single neurons with Boolean output (‘spike’=1 or ‘no spike’=0).\nHow should we encode information about 'features' in a large network with many spikes?\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\nElephant?\nhttps://www.nationalgeographic.com/anim\nals/mammals/facts/african-elephant"", '<2-hop>\n\nReferences\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch–Pitts neural networks. Journal of the History of the Behavioral \nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \n(not publicly available) doi:10.1037/h0042519\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\n6) https://github.com/idsc-frazzoli/retina\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\n10) Gütig R, Sompolinsky H. The tempotron: a neuron that learns spike timing–based decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568', ""<1-hop>\n\nEncoding strategies\nWe considered single neurons with Boolean output (‘spike’=1 or ‘no spike’=0).\nHow should we encode information about 'features' in a large network with many spikes?\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\nElephant?\nhttps://www.nationalgeographic.com/anim\nals/mammals/facts/african-elephant"", '<2-hop>\n\nReferences\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch–Pitts neural networks. Journal of the History of the Behavioral \nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \n(not publicly available) doi:10.1037/h0042519\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\n6) https://github.com/idsc-frazzoli/retina\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\n10) Gütig R, Sompolinsky H. The tempotron: a neuron that learns spike timing–based decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568', ""<1-hop>\n\nEncoding strategies\nWe considered single neurons with Boolean output (‘spike’=1 or ‘no spike’=0).\nHow should we encode information about 'features' in a large network with many spikes?\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\nElephant?\nhttps://www.nationalgeographic.com/anim\nals/mammals/facts/african-elephant"", '<2-hop>\n\nReferences\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch–Pitts neural networks. Journal of the History of the Behavioral \nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \n(not publicly available) doi:10.1037/h0042519\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\n6) https://github.com/idsc-frazzoli/retina\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\n10) Gütig R, Sompolinsky H. The tempotron: a neuron that learns spike timing–based decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568']","The foundational concept of spiking neural networks (SNNs) is that they model neurons as spiking units, where the 'input current' is integrated over time, leading to a spike when a threshold is reached. This is similar to the perceptron model but involves temporal dynamics.

In terms of encoding strategies, SNNs use different methods to encode information with spikes:

1. **Rate Coding**: This classical view involves neurons firing at higher rates for selected features. However, it is considered inefficient and slow as it requires many spikes for good precision.

2. **Latency Coding**: This method encodes information based on the timing of spikes, which can be more efficient and faster than rate coding.

3. **Spatio-Temporal Coding**: This involves using both spatial and temporal patterns of spikes to encode information, making it more spike-efficient and faster.

4. **Rank Order Coding**: This strategy encodes information based on the order in which neurons fire, allowing for a compact representation of information.

Overall, these encoding strategies in SNNs aim to achieve high temporal resolution and energy efficiency, allowing for more computation with fewer neurons and less energy consumption, especially in specialized neuromorphic hardware.","Spiking neural networks use spikes to encode information, with foundational concepts including the work of McCulloch and Pitts on logical calculus of nervous activity, and later developments in spiking neuron models.",0.0,0.0,1.0,0.5,0.0,0.9505452129990474
What is the project idea about government-developed OS software?,"['Study test generation energy consumption\n• Automatic test generation mainly focuses on making strong test suites \n• Do different techniques and configurations impact the energy consumption \nduring generation? \n• Preferably focus on non-LLM test generation methods (EvoSuite, Pyguin, \nDSpot)\n\nB3. Detailed energy profiling of build pipelines\n• Automatic builds have become a cornerstone of quality assurance. But how \nmuch energy do they even consume? \n• Create a tool that reports on the energy consumed during the (different stages \nof the) whole build (compile, build, test, package, …) \n• Should be integrated with build system(s), making setup for developers easy \n• For local setup (to enable true energy measurements)\n\nB4. Energy profiling of static analysis tools\n• What is the energy consumption of a ""typical run"" for a few OSS projects? \n• Does the type of analysis matter? Are certain analysis more expensive? Does \nthe number of rules that are activated in a static analysis tool important for the \nenergy consumption? \n• Differences between static analysis tools [lower priority]\n\nB5. Tool supporting SusAF workshop / process\n• Lead engineers through process & questions \n• Interface to create & document the two diagrams \n• (!) Should be easy to start using & set up \n• You may also create a simplified version / \u2028\nyour favorite sustainability framework\nEnvironmental\nEconomic\nTechnical\nIndividual\nSocial\nImmediate\nEnabling\nStructural\nAirbnb\ngreater \nearnings\nincrease \nin rents\ngentrification\ngreater racial \ndisparities\nrent \nrooms\n5\n1\n3\n4\n2\n \n \n \n \n \n \nVery \nunlikely \nVery \nlikely \n\nB6. Dataset of government-developed OS software\n• Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise \n• But studying government software is difficult b/c we don’t know what is out \nthere\u2028\n→ Lack of incentive to make popular \u2028\n→ Language barriers \n• Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation? \n• Could start with NL, but including your / other countries greatly appreciated!\n\nB7.', '• Deadline April 4.\n6\n\nStrategy\n• Week 0 \n• Decide project idea (today) \n• Define steering meeting schedule \n• Create working document of the \narticle: Problem statement and solution \nproposal! \n• Define and assign tasks for each week. \n• Week 1 \n• Implementation \n• Agreements with supervisor. \n• Week 2 \n• Implementation \n• Week 3 \n• Implementation, Full draft of article, \ndissemination. \n• Week 4 \n• Final refinements \n• Prepare presentation\n7\n\nProject ideas\n• A1. Prototype cross-machine comparable benchmarks \n• A2. Add energy metrics to LMstudio/Ollama \n• A3. Visualizations built-in with EnergiBridge \n• A4. Service-based version of EnergiBridge\n\nProject ideas\n• B1. Measure energy consumption of single JUnit tests \n• B2. Study test generation energy consumption \n• B3. Energy profiling of static analysis tools \n• B4. Detailed energy profiling of build pipelines \n• B5. Tool supporting SusAF workshop / process \n• B6. Dataset of government-developed OS software \n• B7. Queue - but better for the student / TA society\nQuality Assurance & Testing\nSocial & Individual \u2028\nSustainability\n\nProject ideas\n• C1. Compare energy consumption of docker images for ML workloads. \n• C2. Create a plugin to visualize Hugging Face carbon emissions in detail. \n• C3. Plugin for ChatGPT (footprint per chat window) \n• C4. NutriScore for software libraries. \n• C5. Add energy-awareness to existing software \n• C6. Green Shift Left \n• C7. Education for Sustainable SE\n\nA1. Prototype cross-machine comparable benchmarks\n• Energy-usage comparisons require running both baseline + software on the \nsame machine → limits how extensive our experiments can be \n• Research community is in need of benchmarks that make energy \nmeasurements comparable even if executed on different machines \n• Focus on a single task or model (i.e. computing vision, classification)\n\nA2.']","['<1-hop>\n\nB6. Dataset of government-developed OS software\n• Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise\n• But studying government software is diﬃcult b/c we don’t know what is out \nthere\u2028\n→ Lack of incentive to make popular \u2028\n→ Language barriers\n• Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation?\n• Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\n\nProject ideas\n• B1. Measure energy consumption of single JUnit tests \n• B2. Study test generation energy consumption\n• B3. Energy proﬁling of static analysis tools\n• B4. Detailed energy proﬁling of build pipelines\n• B5. Tool supporting SusAF workshop / process\n• B6. Dataset of government-developed OS software\n• B7. Queue - but better for the student / TA society\nQuality Assurance & Testing\nSocial & Individual \u2028\nSustainability']","The project idea about government-developed OS software involves creating a comprehensive dataset of open-source software developed and used by governments to support society. The dataset should include information to understand the state of the software, such as whether it is buildable, its open development history, and requirements documentation. The project could start with the Netherlands (NL) but would benefit from including other countries as well. The challenge lies in the lack of incentive to make such software popular and potential language barriers.","The project idea involves creating a comprehensive dataset of government-developed OS software to understand its state, including buildability, open development history, and requirements documentation. This addresses challenges like lack of incentives and language barriers.",0.99999999995,1.0,1.0,0.82,,0.9999999999999996
What is the project idea related to creating a dataset of goverment-developed OS software?,"['• Deadline April 4.\n6\n\nStrategy\n• Week 0 \n• Decide project idea (today) \n• Define steering meeting schedule \n• Create working document of the \narticle: Problem statement and solution \nproposal! \n• Define and assign tasks for each week. \n• Week 1 \n• Implementation \n• Agreements with supervisor. \n• Week 2 \n• Implementation \n• Week 3 \n• Implementation, Full draft of article, \ndissemination. \n• Week 4 \n• Final refinements \n• Prepare presentation\n7\n\nProject ideas\n• A1. Prototype cross-machine comparable benchmarks \n• A2. Add energy metrics to LMstudio/Ollama \n• A3. Visualizations built-in with EnergiBridge \n• A4. Service-based version of EnergiBridge\n\nProject ideas\n• B1. Measure energy consumption of single JUnit tests \n• B2. Study test generation energy consumption \n• B3. Energy profiling of static analysis tools \n• B4. Detailed energy profiling of build pipelines \n• B5. Tool supporting SusAF workshop / process \n• B6. Dataset of government-developed OS software \n• B7. Queue - but better for the student / TA society\nQuality Assurance & Testing\nSocial & Individual \u2028\nSustainability\n\nProject ideas\n• C1. Compare energy consumption of docker images for ML workloads. \n• C2. Create a plugin to visualize Hugging Face carbon emissions in detail. \n• C3. Plugin for ChatGPT (footprint per chat window) \n• C4. NutriScore for software libraries. \n• C5. Add energy-awareness to existing software \n• C6. Green Shift Left \n• C7. Education for Sustainable SE\n\nA1. Prototype cross-machine comparable benchmarks\n• Energy-usage comparisons require running both baseline + software on the \nsame machine → limits how extensive our experiments can be \n• Research community is in need of benchmarks that make energy \nmeasurements comparable even if executed on different machines \n• Focus on a single task or model (i.e. computing vision, classification)\n\nA2.', 'Study test generation energy consumption\n• Automatic test generation mainly focuses on making strong test suites \n• Do different techniques and configurations impact the energy consumption \nduring generation? \n• Preferably focus on non-LLM test generation methods (EvoSuite, Pyguin, \nDSpot)\n\nB3. Detailed energy profiling of build pipelines\n• Automatic builds have become a cornerstone of quality assurance. But how \nmuch energy do they even consume? \n• Create a tool that reports on the energy consumed during the (different stages \nof the) whole build (compile, build, test, package, …) \n• Should be integrated with build system(s), making setup for developers easy \n• For local setup (to enable true energy measurements)\n\nB4. Energy profiling of static analysis tools\n• What is the energy consumption of a ""typical run"" for a few OSS projects? \n• Does the type of analysis matter? Are certain analysis more expensive? Does \nthe number of rules that are activated in a static analysis tool important for the \nenergy consumption? \n• Differences between static analysis tools [lower priority]\n\nB5. Tool supporting SusAF workshop / process\n• Lead engineers through process & questions \n• Interface to create & document the two diagrams \n• (!) Should be easy to start using & set up \n• You may also create a simplified version / \u2028\nyour favorite sustainability framework\nEnvironmental\nEconomic\nTechnical\nIndividual\nSocial\nImmediate\nEnabling\nStructural\nAirbnb\ngreater \nearnings\nincrease \nin rents\ngentrification\ngreater racial \ndisparities\nrent \nrooms\n5\n1\n3\n4\n2\n \n \n \n \n \n \nVery \nunlikely \nVery \nlikely \n\nB6. Dataset of government-developed OS software\n• Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise \n• But studying government software is difficult b/c we don’t know what is out \nthere\u2028\n→ Lack of incentive to make popular \u2028\n→ Language barriers \n• Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation? \n• Could start with NL, but including your / other countries greatly appreciated!\n\nB7.']","['<1-hop>\n\nB6. Dataset of government-developed OS software\n• Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise\n• But studying government software is diﬃcult b/c we don’t know what is out \nthere\u2028\n→ Lack of incentive to make popular \u2028\n→ Language barriers\n• Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation?\n• Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\n\nProject ideas\n• B1. Measure energy consumption of single JUnit tests \n• B2. Study test generation energy consumption\n• B3. Energy proﬁling of static analysis tools\n• B4. Detailed energy proﬁling of build pipelines\n• B5. Tool supporting SusAF workshop / process\n• B6. Dataset of government-developed OS software\n• B7. Queue - but better for the student / TA society\nQuality Assurance & Testing\nSocial & Individual \u2028\nSustainability', '<1-hop>\n\nB6. Dataset of government-developed OS software\n• Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise\n• But studying government software is diﬃcult b/c we don’t know what is out \nthere\u2028\n→ Lack of incentive to make popular \u2028\n→ Language barriers\n• Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation?\n• Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\n\nProject ideas\n• B1. Measure energy consumption of single JUnit tests \n• B2. Study test generation energy consumption\n• B3. Energy proﬁling of static analysis tools\n• B4. Detailed energy proﬁling of build pipelines\n• B5. Tool supporting SusAF workshop / process\n• B6. Dataset of government-developed OS software\n• B7. Queue - but better for the student / TA society\nQuality Assurance & Testing\nSocial & Individual \u2028\nSustainability', '<1-hop>\n\nB6. Dataset of government-developed OS software\n• Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise\n• But studying government software is diﬃcult b/c we don’t know what is out \nthere\u2028\n→ Lack of incentive to make popular \u2028\n→ Language barriers\n• Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation?\n• Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\n\nProject ideas\n• B1. Measure energy consumption of single JUnit tests \n• B2. Study test generation energy consumption\n• B3. Energy proﬁling of static analysis tools\n• B4. Detailed energy proﬁling of build pipelines\n• B5. Tool supporting SusAF workshop / process\n• B6. Dataset of government-developed OS software\n• B7. Queue - but better for the student / TA society\nQuality Assurance & Testing\nSocial & Individual \u2028\nSustainability', '<1-hop>\n\nB6. Dataset of government-developed OS software\n• Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise\n• But studying government software is diﬃcult b/c we don’t know what is out \nthere\u2028\n→ Lack of incentive to make popular \u2028\n→ Language barriers\n• Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation?\n• Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\n\nProject ideas\n• B1. Measure energy consumption of single JUnit tests \n• B2. Study test generation energy consumption\n• B3. Energy proﬁling of static analysis tools\n• B4. Detailed energy proﬁling of build pipelines\n• B5. Tool supporting SusAF workshop / process\n• B6. Dataset of government-developed OS software\n• B7. Queue - but better for the student / TA society\nQuality Assurance & Testing\nSocial & Individual \u2028\nSustainability', '<1-hop>\n\nB6. Dataset of government-developed OS software\n• Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise\n• But studying government software is diﬃcult b/c we don’t know what is out \nthere\u2028\n→ Lack of incentive to make popular \u2028\n→ Language barriers\n• Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation?\n• Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\n\nProject ideas\n• B1. Measure energy consumption of single JUnit tests \n• B2. Study test generation energy consumption\n• B3. Energy proﬁling of static analysis tools\n• B4. Detailed energy proﬁling of build pipelines\n• B5. Tool supporting SusAF workshop / process\n• B6. Dataset of government-developed OS software\n• B7. Queue - but better for the student / TA society\nQuality Assurance & Testing\nSocial & Individual \u2028\nSustainability']","The project idea related to creating a dataset of government-developed OS software involves compiling a comprehensive dataset that includes data to understand the state of such software. This includes determining if the software is buildable, its open development history, and requirements documentation. The project acknowledges the challenges in studying government software due to a lack of incentive to make it popular and language barriers. It suggests starting with the Netherlands (NL) but encourages including other countries as well.","The project idea is to create a dataset of government-developed OS software, which would help in understanding the state of such software, including whether it is buildable, has open development history, and has requirements documentation.",0.99999999995,1.0,1.0,0.75,0.249999999375,0.9980923627914132
What is the project idea related to creating a dataset of government-developed OS software?,"['Study test generation energy consumption\n• Automatic test generation mainly focuses on making strong test suites \n• Do different techniques and configurations impact the energy consumption \nduring generation? \n• Preferably focus on non-LLM test generation methods (EvoSuite, Pyguin, \nDSpot)\n\nB3. Detailed energy profiling of build pipelines\n• Automatic builds have become a cornerstone of quality assurance. But how \nmuch energy do they even consume? \n• Create a tool that reports on the energy consumed during the (different stages \nof the) whole build (compile, build, test, package, …) \n• Should be integrated with build system(s), making setup for developers easy \n• For local setup (to enable true energy measurements)\n\nB4. Energy profiling of static analysis tools\n• What is the energy consumption of a ""typical run"" for a few OSS projects? \n• Does the type of analysis matter? Are certain analysis more expensive? Does \nthe number of rules that are activated in a static analysis tool important for the \nenergy consumption? \n• Differences between static analysis tools [lower priority]\n\nB5. Tool supporting SusAF workshop / process\n• Lead engineers through process & questions \n• Interface to create & document the two diagrams \n• (!) Should be easy to start using & set up \n• You may also create a simplified version / \u2028\nyour favorite sustainability framework\nEnvironmental\nEconomic\nTechnical\nIndividual\nSocial\nImmediate\nEnabling\nStructural\nAirbnb\ngreater \nearnings\nincrease \nin rents\ngentrification\ngreater racial \ndisparities\nrent \nrooms\n5\n1\n3\n4\n2\n \n \n \n \n \n \nVery \nunlikely \nVery \nlikely \n\nB6. Dataset of government-developed OS software\n• Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise \n• But studying government software is difficult b/c we don’t know what is out \nthere\u2028\n→ Lack of incentive to make popular \u2028\n→ Language barriers \n• Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation? \n• Could start with NL, but including your / other countries greatly appreciated!\n\nB7.', '• Deadline April 4.\n6\n\nStrategy\n• Week 0 \n• Decide project idea (today) \n• Define steering meeting schedule \n• Create working document of the \narticle: Problem statement and solution \nproposal! \n• Define and assign tasks for each week. \n• Week 1 \n• Implementation \n• Agreements with supervisor. \n• Week 2 \n• Implementation \n• Week 3 \n• Implementation, Full draft of article, \ndissemination. \n• Week 4 \n• Final refinements \n• Prepare presentation\n7\n\nProject ideas\n• A1. Prototype cross-machine comparable benchmarks \n• A2. Add energy metrics to LMstudio/Ollama \n• A3. Visualizations built-in with EnergiBridge \n• A4. Service-based version of EnergiBridge\n\nProject ideas\n• B1. Measure energy consumption of single JUnit tests \n• B2. Study test generation energy consumption \n• B3. Energy profiling of static analysis tools \n• B4. Detailed energy profiling of build pipelines \n• B5. Tool supporting SusAF workshop / process \n• B6. Dataset of government-developed OS software \n• B7. Queue - but better for the student / TA society\nQuality Assurance & Testing\nSocial & Individual \u2028\nSustainability\n\nProject ideas\n• C1. Compare energy consumption of docker images for ML workloads. \n• C2. Create a plugin to visualize Hugging Face carbon emissions in detail. \n• C3. Plugin for ChatGPT (footprint per chat window) \n• C4. NutriScore for software libraries. \n• C5. Add energy-awareness to existing software \n• C6. Green Shift Left \n• C7. Education for Sustainable SE\n\nA1. Prototype cross-machine comparable benchmarks\n• Energy-usage comparisons require running both baseline + software on the \nsame machine → limits how extensive our experiments can be \n• Research community is in need of benchmarks that make energy \nmeasurements comparable even if executed on different machines \n• Focus on a single task or model (i.e. computing vision, classification)\n\nA2.']","['<1-hop>\n\nB6. Dataset of government-developed OS software\n• Governments develop & use software for supporting society\u2028\nOpen-source development & policies are on the rise\n• But studying government software is diﬃcult b/c we don’t know what is out \nthere\u2028\n→ Lack of incentive to make popular \u2028\n→ Language barriers\n• Create a comprehensive dataset, incl. data to understand state of software: \nbuildable?, open dev. history?, requirements documentation?\n• Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\n\nProject ideas\n• B1. Measure energy consumption of single JUnit tests \n• B2. Study test generation energy consumption\n• B3. Energy proﬁling of static analysis tools\n• B4. Detailed energy proﬁling of build pipelines\n• B5. Tool supporting SusAF workshop / process\n• B6. Dataset of government-developed OS software\n• B7. Queue - but better for the student / TA society\nQuality Assurance & Testing\nSocial & Individual \u2028\nSustainability']","The project idea related to creating a dataset of government-developed OS software involves compiling a comprehensive dataset that includes data to understand the state of such software. This includes information on whether the software is buildable, its open development history, and requirements documentation. The project acknowledges the challenges in studying government software due to a lack of incentive to make it popular and language barriers. It suggests starting with the Netherlands (NL) but appreciates the inclusion of other countries.","The project idea is to create a comprehensive dataset of government-developed OS software to understand the state of such software, including its buildability, open development history, and requirements documentation. This addresses challenges like the lack of incentive to make software popular and language barriers.",0.99999999995,1.0,1.0,0.86,0.1666666663888888,1.0000000000000004
