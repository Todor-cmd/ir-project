user_input,retrieved_contexts,reference_contexts,response,reference,context_precision,context_recall,faithfulness,factual_correctness(mode=f1),context_entity_recall,answer_relevancy
How EcoAndroid reduce risk by reduce size?,"['8. Green AI\r\nSustainable Software Engineering\u2028\r\nCS4575\r\nSustainableSE 2025\r\nLuís Cruz \r\nL.Cruz@tudelft.nl \r\nCarolin Brandt \r\nC.E.Brandt@tudelft.nl \r\nEnrique Barba Roque \r\nE.BarbaRoque@tudelft.nl\r\n\r\n- Overview of Green AI \r\n- Large language models \r\n- Green data-centric AI \r\n- Model simplification \r\n- Hyper parameter tuning \r\n- Mixture of Experts and DeepSeek \r\n- Green AI at Meta \r\n\r\nAI\r\n• Artificial Intelligence (AI) is the branch of computer science that deals with \r\nautomating tasks that typically require human intelligence. \r\n• In the past years AI has been widely applied across different domains.\u2028\r\nE.g., health care, transportation, finance. \r\n• To deploy AI systems, we test them against benchmarks (or validation sets). \r\n• The goal is to outperform the previous existing models. \r\n• E.g., in Machine Learning we usually resort to accuracy metrics. The \r\nhighest the accuracy, the better the model.\r\n3\r\n\r\nSince 2012, the amount of computing used for AI \r\ntraining has been doubling every 6 months\r\n• https://epoch.ai/blog/compute-trends\r\n4\r\n\r\n• To create better AI systems we are currently adding \r\n• More data \r\n• More experiments \r\n• Larger models\r\n5\r\n\r\nThe Equation of Red AI\r\nCost(R) ∝E·D·H\r\nCost of a single (E)xample\r\nSize of (D)ataset\r\nNumber of (H)yperparameters\r\nBy Schwartz et al. (2020)\r\n\r\nIssues of Red AI\r\n• High costs (hardware, electricity, data access, etc.) \r\n• Limited reproducibility.  \r\n• Energy consumption. \r\n• Carbon emissions. \r\n• SMEs can hardly be competitive. \r\n• Groundbreaking AI research is mostly done by tech giants.\r\n7\r\n\r\nA few examples of Red AI\r\n• Google’s BERT-large \r\n• 350 million features \r\n• Trained for 2.5 days using 512 TPU chips, costing $60K+ \r\n• Open-GPT3 (now GPT-4/o1) \r\n• 550 tonnes CO2-eq (Patterson, 2021) \r\n• 175 billion features \r\n• API is open but no-pretrained model is available \r\n• AlphaGo \r\n• 1920 CPUs, 280 GPUs, costing $35M\r\n8\r\n\r\nRed AI in Large Language Models (LLMs)\r\n• OPT by Meta reports 75 tons CO2-eq (1/7 of OpenGPT’s footprint).\u2028\r\n(Also 175billion params) \r\n• However, Llama 3 reported 2,290 tons of CO2-eq (7.7M GPU hours \r\ntraining ) \r\n• Open science: release includes both the pretrained models and the code \r\nneeded to train and use them. \r\n• DeepSeek-V3 claims “only” 2.78M GPU hours \r\n• Bloom by Huggingface reports 25 tons, 51 when considering embodied \r\nand operational carbon footprint. (176billion params)\r\n9\r\n\r\nRed AI\r\nAccuracy: 0.999999999\r\nGreen AI\r\n• Energy \r\n• Time \r\n• Reproducibility \r\n• Reusage\r\n\r\nHow can we adopt Green AI\r\n• Check whether AI is needed. \r\n• Select green datacenters. \r\n• Run on low carbon intensity hours. \r\n• Opt for GPU-optimised solutions (?) \r\n• Opt for low-power hardware (e.g., Nvidia Jetson boards) \r\n• Or GPUs that provide energy metrics (e.g., NVIDIA GPUs via the nvidia-smi tool) \r\n• Report energy/carbon metrics (e.g., embed in MLFlow?)', '[…]\r\n13\r\n\r\nAvoid Extraneous Graphics and \r\nAnimations  \r\n• Context: Mobile apps that feature impressive graphics \r\nand animations. […] \r\n• Solution: Study the importance of graphics and \r\nanimations to the user experience and reduce them when \r\napplicable. […]\r\n• Example: Resort to low frame rates for animations when \r\npossible. \r\nDespite being important to improve user experience, graphics \r\nand animations are battery intensive and should be used with \r\nmoderation.  \r\n14\r\n\r\nEnergy Patterns are \r\nmore Frequent\r\nin Android Apps\r\n\r\nExample case: Nextcloud\r\nFOSS\r\n\r\nExample case: Nextcloud\r\n• Users complain that sometimes they go on a trip and  Nextcloud drains their \r\nbattery. Users consider uninstalling the app when battery life is essential. \r\n• File sync can be energy-greedy. Send large files to the server, long 3G/4G data connections. \r\n• It is mostly used for backup. No real-time collaboration is needed. \r\n• Energy requirements vary depending on context and user. Some days you really \r\nneed all the battery you can get. \r\n• https://github.com/nextcloud/android/commit/\r\n8bc432027e0d33e8043cf40192203203a40ca29c\r\nSolutions?\r\n17\r\n\r\nExample case: K-9 mail\r\n18\r\n\r\nExample case: K-9 mail\r\n19\r\nSolutions?\r\n• Some users noticed that K-9 mail was spending more energy than usual. 🙉 \r\n• A user that was having issues with a personal mail server  noticed that K-9 \r\nmail was the one of the most energy-greedy apps. IMAP IDLE protocol for real-time \r\nnotifications. \r\n• When a connection is not possible, the app automatically retries later. \r\n• https://github.com/k9mail/k-9/commit/\r\n86f3b28f79509d1a4d613eb39f60603e08579ea3\r\n\r\nEcoAndroid\r\n• Plugin for IntelliJ (Android Studio) \r\n• Dynamic Retry Delay \r\n• Push Over Poll \r\n• Reduce Size \r\n• Cache \r\n• Avoid Graphics and Animations\r\n20\r\n\r\nCarbon-Aware Computing for \r\nDatacenters \r\nhttps://sites.google.com/view/energy-efficiency-languages\r\n21\r\nAna Radovanovic ́, Ross Koningstein, Ian Schneider, Bokan \r\nChen, Alexandre Duarte, Binz Roy, Diyue Xiao, Maya \r\nHaridasan, Patrick Hung, Nick Care, Saurav Talukdar, Eric \r\nMullen, Kendal Smith, MariEllen Cottman, and Walfredo Cirne \r\n\r\n\r\n• Google’s Carbon-Intelligent Computing System (CICS) \r\n• Main idea: use carbon-intensity data to shift datacenter jobs in time \r\n• Typically, job schedulers use a metric of cluster capacity to schedule a job in \r\na particular cluster. \r\n• CICS overrides this metric with the virtual capacity curve (VCC) that \r\nfactors in Carbon intensity \r\n• When a new job comes in, the scheduler estimates its CPU load and power \r\nusage and assigns it to a cluster if the VCC is not exceeded.\r\n23\r\n\r\n• Jobs are divided between flexible and inflexible. \r\n• Flexible load is considered shapeable/shiftable as long as its total daily \r\ncompute (CPU) demand is preserved \r\n• The system needs to consider that, while running a job, the virtual capacity \r\ncurve (VCC) might drop. Hence, this job should not start in the first place. \r\n• They forecast VCC for the next day\r\n24\r\n\r\n\r\nVirtual Cluster Capacity (VCC)\r\n• Aims at reducing the peak load at carbon intensive hours but in total it should allow for the \r\nsome amount of daily computation!', '• Groundbreaking AI research is mostly done by tech giants.\r\n7\r\n\r\nA few examples of Red AI\r\n• Google’s BERT-large \r\n• 350 million features \r\n• Trained for 2.5 days using 512 TPU chips, costing $60K+ \r\n• Open-GPT3 (now GPT-4/o1) \r\n• 550 tonnes CO2-eq (Patterson, 2021) \r\n• 175 billion features \r\n• API is open but no-pretrained model is available \r\n• AlphaGo \r\n• 1920 CPUs, 280 GPUs, costing $35M\r\n8\r\n\r\nRed AI in Large Language Models (LLMs)\r\n• OPT by Meta reports 75 tons CO2-eq (1/7 of OpenGPT’s footprint).\u2028\r\n(Also 175billion params) \r\n• However, Llama 3 reported 2,290 tons of CO2-eq (7.7M GPU hours \r\ntraining ) \r\n• Open science: release includes both the pretrained models and the code \r\nneeded to train and use them. \r\n• DeepSeek-V3 claims “only” 2.78M GPU hours \r\n• Bloom by Huggingface reports 25 tons, 51 when considering embodied \r\nand operational carbon footprint. (176billion params)\r\n9\r\n\r\nRed AI\r\nAccuracy: 0.999999999\r\nGreen AI\r\n• Energy \r\n• Time \r\n• Reproducibility \r\n• Reusage\r\n\r\nHow can we adopt Green AI\r\n• Check whether AI is needed. \r\n• Select green datacenters. \r\n• Run on low carbon intensity hours. \r\n• Opt for GPU-optimised solutions (?) \r\n• Opt for low-power hardware (e.g., Nvidia Jetson boards) \r\n• Or GPUs that provide energy metrics (e.g., NVIDIA GPUs via the nvidia-smi tool) \r\n• Report energy/carbon metrics (e.g., embed in MLFlow?) \r\n• Use pre-trained models (Transfer Learning) \r\n• Preprocess dataset to reduce size. \r\n• Improve parameter-tuning strategy.\r\n11\r\n\r\nReporting energy/carbon footprint\r\n• We need benchmarks. \r\n• AllenAI leaderboard\u2028\r\nhttps://leaderboard.allenai.org \r\n• No carbon metrics, yet \r\n• Report comparable proxies for energy \r\nconsumption. \r\n• ⚠ Learning algorithms behave in a non-\r\ndeterministic \r\n• ⚠ Different data-points lead to different \r\nenergy consumption\r\n12\r\n\r\nReporting energy/carbon footprint\r\n• Reporting measured energy consumption \r\n• + Accurate \r\n• + Easy to map to carbon emissions \r\n• - Hard to measure \r\n• - Low replicability \r\n• Reporting time / estimation based on time & hardware \r\n• + Easy to measure \r\n• + Correlates with energy consumption in most cases. \r\n• -  Difficult to compare with measurements from other setups \r\n• E.g., floating point operations (FPOs) (?) \r\n• + comparable across different setups \r\n• + cheap \r\n• - does not factor in memory energy consumption \r\n• - does not reflect carbon emissions\r\n13\r\n\r\nData-centric AI\r\n\r\nData-centric AI\r\n• Emerging discipline that deals with systematically engineering data to build AI \r\nsystems. \r\n• Shift from improving the training strategy to improving the data. \r\n• It is better to have small but reliable datasets than large but noisy \r\ndatasets. \r\n• => Improve data collection, data labelling, and data preprocessing. \r\n• More about data-centric AI by Andrew Ng:\u2028\r\nhttps://www.youtube.com/watch?v=06-AZXmwHjo\r\n15\r\n\r\n\r\nGreen Data-centric AI\r\n• How do different ML algorithms compare \r\nin terms of energy consumption? \r\n• How does number of rows relate to the \r\nenergy consumption of ML models? \r\n• How does number of features relate to \r\nthe energy consumption of ML models? \r\n• What is the impact of reducing data in the \r\nperformance of the model?']","['<1-hop>\r\n\r\nShuffle \r\n🔀\r\n• It is not a mystery that energy consumption depends on so many factors that \r\nit is impossible to control all of them.\r\n• If you run 30 executions for version A and another batch for version B:\r\n• External conditions that change over time will have a diﬀerent bias in \r\nthe 2 versions (e.g., room temperature changes).\r\n• If you shuﬄe, you reduce this risk.\r\n11', '<2-hop>\r\n\r\nEcoAndroid\r\n• Plugin for IntelliJ (Android Studio)\r\n• Dynamic Retry Delay\r\n• Push Over Poll\r\n• Reduce Size\r\n• Cache\r\n• Avoid Graphics and Animations\r\n20', '<1-hop>\r\n\r\nShuffle \r\n🔀\r\n• It is not a mystery that energy consumption depends on so many factors that \r\nit is impossible to control all of them.\r\n• If you run 30 executions for version A and another batch for version B:\r\n• External conditions that change over time will have a diﬀerent bias in \r\nthe 2 versions (e.g., room temperature changes).\r\n• If you shuﬄe, you reduce this risk.\r\n11', '<2-hop>\r\n\r\nEcoAndroid\r\n• Plugin for IntelliJ (Android Studio)\r\n• Dynamic Retry Delay\r\n• Push Over Poll\r\n• Reduce Size\r\n• Cache\r\n• Avoid Graphics and Animations\r\n20']","EcoAndroid reduces risk by reducing size through several strategies aimed at minimizing energy consumption in mobile applications. These strategies include:

1. **Dynamic Retry Delay**: Adjusting the frequency of retries for network operations to reduce unnecessary energy use.
2. **Push Over Poll**: Favoring push notifications over polling to decrease energy consumption.
3. **Cache**: Implementing caching mechanisms to avoid redundant data fetching and processing.
4. **Avoid Graphics and Animations**: Reducing the use of graphics and animations, which are battery-intensive, to conserve energy.

By implementing these strategies, EcoAndroid helps in optimizing the app's performance while minimizing its energy footprint, thereby reducing the risk of excessive battery drain.",EcoAndroid reduce risk by using Reduce Size feature which help minimize energy consumption and reduce impact of external factors like temperature changes.,0.0,0.0,0.75,0.2,0.0,0.928332241227644
How can static code analysis in data science be used to improve the energy efficiency of data centers and contribute to sustainable software engineering?,"['Interviews?) & rich \r\ndescription of outcomes focus of grading\r\nhttps://eip.pages.ewi.tudelft.nl/eip-website/queue.html\r\n\r\nC1. Compare energy consumption of docker images for \r\nML workloads.\r\n• Similar to what we have seen in the lectures but for ML-specific workloads. \r\n• We can reuse existing experiment replication packages.\r\n22\r\n\r\nC2. Plugin to visualize Hugging Face carbon emissions.\r\n• https://huggingface.co/blog/leaderboard-emissions-\r\nanalysis \r\n•\r\nYoo, Taewon, et al. ""Visualizing the Carbon Intensity of Machine Learning Inference for Image Analysis on TensorFlow Hub.""\r\n\r\nC3. Plugin for ChatGPT\r\n• Users seldom know how much carbon they are emitting when they interact \r\nwith chat GPT. \r\n• Let’s make it transparent to the users. Browser plugin?\r\n24\r\nCO2\r\n\r\nC4. NutriScore for software libraries\r\n25\r\n• NutriScore labels are not perfect but they are a good starting point! \r\n• What if we could do the same thing for the energy efficiency of software. \r\n• (Also open to individual or social sustainability) \r\n• This work can be scoped in particular domains/ecossystems/use cases. \r\n• Libraries for stats? ML? Web Dev? Cloud?\r\n\r\nC5. Add energy-awareness to existing software\r\n26\r\n• Streamlit, Notebooks, JSFiddle, \r\nPyScript, zsh, etc. \r\n•\r\n\r\nC6 - Green Shift Left\r\n• Estimate energy efficiency using static code analysis. \r\n• We don’t need an accurate value. \r\n• It is useful to know which code is more likely to \r\nintroduce energy hotspots and that should be \r\nreviewed with more attention. \r\n• Can be scoped to a particular domain (react, php, data \r\nscience, web, etc., etc.)\r\n27\r\n\r\nC7 - Education\r\n• Educational game for Software \r\nSustainability practices \r\n• Purpose: use within software teams to \r\ndiscuss or learn about different sustainable \r\nIT practices: at the organisation level, \r\nsoftware, etc.\r\n28\r\nhttps://github.com/OttoKaaij/Ticket-To-Sustainability/?tab=readme-ov-file\r\n\r\nProject ideas (old)\r\n• Plugin from EnergiBridge (GUI, report generator, python library, etc.) \r\n• Plugin for ChatGPT (carbon emissions per chat window) \r\n• Seamless measurements for AI libraries \r\n• Energy patterns for Green AI \r\n• Sustainable SW dev gamification \r\n• Sustainability auditor for AI projects \r\n• Energy Profiling of screen colour filter tools (or display settings) \r\n• … you can also propose yours!\r\n\r\nhttps://edu.nl/64gpk\r\nedu.nl/64gpk', 'Energy profiling of static analysis tools\r\n• What is the energy consumption of a ""typical run"" for a few OSS projects? \r\n• Does the type of analysis matter? Are certain analysis more expensive? Does \r\nthe number of rules that are activated in a static analysis tool important for the \r\nenergy consumption? \r\n• Differences between static analysis tools [lower priority]\r\n\r\nB5. Tool supporting SusAF workshop / process\r\n• Lead engineers through process & questions \r\n• Interface to create & document the two diagrams \r\n• (!) Should be easy to start using & set up \r\n• You may also create a simplified version / \u2028\r\nyour favorite sustainability framework\r\nEnvironmental\r\nEconomic\r\nTechnical\r\nIndividual\r\nSocial\r\nImmediate\r\nEnabling\r\nStructural\r\nAirbnb\r\ngreater \r\nearnings\r\nincrease \r\nin rents\r\ngentrification\r\ngreater racial \r\ndisparities\r\nrent \r\nrooms\r\n5\r\n1\r\n3\r\n4\r\n2\r\n \r\n \r\n \r\n \r\n \r\n \r\nVery \r\nunlikely \r\nVery \r\nlikely \r\n\r\nB6. Dataset of government-developed OS software\r\n• Governments develop & use software for supporting society\u2028\r\nOpen-source development & policies are on the rise \r\n• But studying government software is difficult b/c we don’t know what is out \r\nthere\u2028\r\n→ Lack of incentive to make popular \u2028\r\n→ Language barriers \r\n• Create a comprehensive dataset, incl. data to understand state of software: \r\nbuildable?, open dev. history?, requirements documentation? \r\n• Could start with NL, but including your / other countries greatly appreciated!\r\n\r\nB7. Queue - but better for the student / TA society\r\n• Requirements analysis regarding social and individual sustainability effects \r\nof Queue \r\n• Other EIP / TUD used software also possible: e.g., Answers EWI \r\n• Non-technical project → proper process (workshops? Interviews?) & rich \r\ndescription of outcomes focus of grading\r\nhttps://eip.pages.ewi.tudelft.nl/eip-website/queue.html\r\n\r\nC1. Compare energy consumption of docker images for \r\nML workloads.\r\n• Similar to what we have seen in the lectures but for ML-specific workloads. \r\n• We can reuse existing experiment replication packages.\r\n22\r\n\r\nC2. Plugin to visualize Hugging Face carbon emissions.\r\n• https://huggingface.co/blog/leaderboard-emissions-\r\nanalysis \r\n•\r\nYoo, Taewon, et al. ""Visualizing the Carbon Intensity of Machine Learning Inference for Image Analysis on TensorFlow Hub.""\r\n\r\nC3. Plugin for ChatGPT\r\n• Users seldom know how much carbon they are emitting when they interact \r\nwith chat GPT. \r\n• Let’s make it transparent to the users. Browser plugin?\r\n24\r\nCO2\r\n\r\nC4. NutriScore for software libraries\r\n25\r\n• NutriScore labels are not perfect but they are a good starting point! \r\n• What if we could do the same thing for the energy efficiency of software. \r\n• (Also open to individual or social sustainability) \r\n• This work can be scoped in particular domains/ecossystems/use cases. \r\n• Libraries for stats? ML? Web Dev? Cloud?\r\n\r\nC5. Add energy-awareness to existing software\r\n26\r\n• Streamlit, Notebooks, JSFiddle, \r\nPyScript, zsh, etc. \r\n•\r\n\r\nC6 - Green Shift Left\r\n• Estimate energy efficiency using static code analysis. \r\n• We don’t need an accurate value. \r\n• It is useful to know which code is more likely to \r\nintroduce energy hotspots and that should be \r\nreviewed with more attention. \r\n• Can be scoped to a particular domain (react, php, data \r\nscience, web, etc., etc.)', 'Different \r\nscreen brightness, different results. \r\n• You need to reduce tasks to the bare minimum: \r\n• Set brightness to a fixed value; turn off notifications, kill all user-owned \r\nprocesses, turn off cellular data, bluetooth, location services, account \r\nsyncs; uninstall all unnecessary apps, etc.\r\n11\r\n\r\nWhen it comes to desktop/cloud software, the sources \r\nof noise are different but the same concerns apply. \r\n \r\nEach case is different – think it through!\r\n⚠\r\n\r\nEnergy Profilers\r\n• Simple setup! Quite reliable (if you choose the profiler wisely). \r\n• Recently, they are starting to rely on internal power sensors. \r\n• Still sensitive to noise from concurrent processes/tasks! ⚠\r\n13\r\n\r\nExamples of Energy Profilers\r\n\r\n15\r\nhttps://www.websitecarbon.com\r\n\r\n16\r\nhttps://mlco2.github.io/impact/\r\n\r\nIntel Power Monitor\r\n• Install: https://software.intel.com/content/www/us/en/\r\ndevelop/articles/intel-power-gadget.html \r\n• To collect: Logging > Log to File \r\n \r\n• It will store a CSV file with all the collected power data. \r\n(File location is specified in the settings) \r\n• Based on Intel RAPL. Works with Intel-based Windows \r\nand Macs. \r\n• Alternative-twin for M1-based Macs: Mx Power Gadget.\u2028\r\nhttps://www.seense.com/menubarstats/mxpg/\r\n17\r\n⚠ No longer supported by Intel ⚠  \r\n\r\n18\r\n🔗 https://luiscruz.github.io/\r\n2021/07/20/measuring-energy.html\r\n(Missing Apple m1 tools: mxpg, powermetrics)\r\n\r\n19\r\nEnergiBridge\r\nhttps://github.com/tdurieux/energibridge\r\n> target/release/energibridge -o results.csv --summary sleep 10\r\n\r\nHands-on 1\r\n• Install your energy profiler (EnergiBridge). \r\n• Collect the energy data of using Coral BodyPix for 30 seconds.\u2028\r\nhttps://storage.googleapis.com/tfjs-models/demos/body-pix/index.html \r\n• Report the total energy consumption. \r\n• Extra-mile: \r\n• Compare the energy consumption in different browsers. \r\n• Check the spikes and drops in Power and Temperature.\r\n20\r\n\r\nRetrospection\r\nHands-on 1\r\n• Are the measurements repeatable? \r\n• What were the confounding factors? \r\n• How can we automate this process?\r\n21\r\n\r\nEnergy testing\r\n(Different from energy monitoring)\r\n1. Create a reproducible scenario of the execution of your software. Preferably \r\nthis should be an automated script – e.g., using a bash or python script. \r\n2. Execute the scenario in a version of your software. Use the energy profiler to \r\nmeasure the energy consumption. \r\n3. Improve your software in parts of the code that you suspect have low \r\nperformance. \r\n4. Execute the same scenario with the new version. Compare the energy data \r\nin this version with the previous one.\u2028\r\nEnergy is lower, test passes; energy is higher test fails.\r\n22\r\n\r\nHands-on 2\r\n• Create a reproducible scenario. (Usually easier with command-line interfaces) \r\n• Automatically start/stop energy profiling.\r\n23\r\n\r\nProject 1\r\n• Deadline: March 1 \r\n•  Compare energy consumption in common software use cases. \r\n• Examples: \r\n• Different versions of the same app; \r\n• Same use case but different apps \r\n• Same version, same app, but different user settings (e.g., enable/disable GPU optimisation) \r\n• Same version, same app, but different running environment \r\n• Submission via PR (markdown). \r\n• Blog-style report (markdown, approx 2500 words). \r\n• Replication package. \r\n• Points if the experiment is automated.']","['<1-hop>\r\n\r\nC6 - Green Shift Left\r\n• Estimate energy eﬃciency using static code analysis.\r\n• We don’t need an accurate value.\r\n• It is useful to know which code is more likely to \r\nintroduce energy hotspots and that should be \r\nreviewed with more attention.\r\n• Can be scoped to a particular domain (react, php, data \r\nscience, web, etc., etc.)\r\n27', '<2-hop>\r\n\r\nWhy?\r\n• Throughout your career you might:\r\n• Design/maintain/contract data centers\r\n• Set up operations/devops\r\n• Develop AI for IoT devices\r\n• Be the next CEO/CTO of a software company\r\n• Sustainability can be your main role:\r\n• Green Software Developer\r\n• Sustainability Consultant\r\n• Green Advocate \r\n• Founder of a Green Tech startup (B2B?)\r\n28', '<1-hop>\r\n\r\nC6 - Green Shift Left\r\n• Estimate energy eﬃciency using static code analysis.\r\n• We don’t need an accurate value.\r\n• It is useful to know which code is more likely to \r\nintroduce energy hotspots and that should be \r\nreviewed with more attention.\r\n• Can be scoped to a particular domain (react, php, data \r\nscience, web, etc., etc.)\r\n27', '<2-hop>\r\n\r\nWhy?\r\n• Throughout your career you might:\r\n• Design/maintain/contract data centers\r\n• Set up operations/devops\r\n• Develop AI for IoT devices\r\n• Be the next CEO/CTO of a software company\r\n• Sustainability can be your main role:\r\n• Green Software Developer\r\n• Sustainability Consultant\r\n• Green Advocate \r\n• Founder of a Green Tech startup (B2B?)\r\n28', '<1-hop>\r\n\r\nC6 - Green Shift Left\r\n• Estimate energy eﬃciency using static code analysis.\r\n• We don’t need an accurate value.\r\n• It is useful to know which code is more likely to \r\nintroduce energy hotspots and that should be \r\nreviewed with more attention.\r\n• Can be scoped to a particular domain (react, php, data \r\nscience, web, etc., etc.)\r\n27', '<2-hop>\r\n\r\nWhy?\r\n• Throughout your career you might:\r\n• Design/maintain/contract data centers\r\n• Set up operations/devops\r\n• Develop AI for IoT devices\r\n• Be the next CEO/CTO of a software company\r\n• Sustainability can be your main role:\r\n• Green Software Developer\r\n• Sustainability Consultant\r\n• Green Advocate \r\n• Founder of a Green Tech startup (B2B?)\r\n28', '<1-hop>\r\n\r\nC6 - Green Shift Left\r\n• Estimate energy eﬃciency using static code analysis.\r\n• We don’t need an accurate value.\r\n• It is useful to know which code is more likely to \r\nintroduce energy hotspots and that should be \r\nreviewed with more attention.\r\n• Can be scoped to a particular domain (react, php, data \r\nscience, web, etc., etc.)\r\n27', '<2-hop>\r\n\r\nWhy?\r\n• Throughout your career you might:\r\n• Design/maintain/contract data centers\r\n• Set up operations/devops\r\n• Develop AI for IoT devices\r\n• Be the next CEO/CTO of a software company\r\n• Sustainability can be your main role:\r\n• Green Software Developer\r\n• Sustainability Consultant\r\n• Green Advocate \r\n• Founder of a Green Tech startup (B2B?)\r\n28']","Static code analysis in data science can be used to improve the energy efficiency of data centers and contribute to sustainable software engineering by identifying potential energy hotspots in the code. This process involves analyzing the code without executing it to estimate its energy efficiency. By doing so, developers can pinpoint sections of the code that are likely to consume more energy and should be reviewed with more attention. This approach, known as ""Green Shift Left,"" allows for early detection and mitigation of inefficient code patterns, which can lead to reduced energy consumption when the software is deployed in data centers. This proactive measure contributes to the overall sustainability of software engineering by optimizing resource usage and minimizing the carbon footprint associated with software operations.","Static code analysis in data science can be used to estimate energy efficiency by identifying potential energy hotspots in the code. This helps in reviewing and optimizing the code to reduce energy consumption. By applying this approach, developers can contribute to the design and maintenance of more energy-efficient data centers, aligning with sustainable software engineering practices.",0.99999999995,0.6666666666666666,1.0,0.94,,0.9541145642510434
How can static code analysis in data science contribute to the development of more sustainable data centers?,"['Interviews?) & rich \r\ndescription of outcomes focus of grading\r\nhttps://eip.pages.ewi.tudelft.nl/eip-website/queue.html\r\n\r\nC1. Compare energy consumption of docker images for \r\nML workloads.\r\n• Similar to what we have seen in the lectures but for ML-specific workloads. \r\n• We can reuse existing experiment replication packages.\r\n22\r\n\r\nC2. Plugin to visualize Hugging Face carbon emissions.\r\n• https://huggingface.co/blog/leaderboard-emissions-\r\nanalysis \r\n•\r\nYoo, Taewon, et al. ""Visualizing the Carbon Intensity of Machine Learning Inference for Image Analysis on TensorFlow Hub.""\r\n\r\nC3. Plugin for ChatGPT\r\n• Users seldom know how much carbon they are emitting when they interact \r\nwith chat GPT. \r\n• Let’s make it transparent to the users. Browser plugin?\r\n24\r\nCO2\r\n\r\nC4. NutriScore for software libraries\r\n25\r\n• NutriScore labels are not perfect but they are a good starting point! \r\n• What if we could do the same thing for the energy efficiency of software. \r\n• (Also open to individual or social sustainability) \r\n• This work can be scoped in particular domains/ecossystems/use cases. \r\n• Libraries for stats? ML? Web Dev? Cloud?\r\n\r\nC5. Add energy-awareness to existing software\r\n26\r\n• Streamlit, Notebooks, JSFiddle, \r\nPyScript, zsh, etc. \r\n•\r\n\r\nC6 - Green Shift Left\r\n• Estimate energy efficiency using static code analysis. \r\n• We don’t need an accurate value. \r\n• It is useful to know which code is more likely to \r\nintroduce energy hotspots and that should be \r\nreviewed with more attention. \r\n• Can be scoped to a particular domain (react, php, data \r\nscience, web, etc., etc.)\r\n27\r\n\r\nC7 - Education\r\n• Educational game for Software \r\nSustainability practices \r\n• Purpose: use within software teams to \r\ndiscuss or learn about different sustainable \r\nIT practices: at the organisation level, \r\nsoftware, etc.\r\n28\r\nhttps://github.com/OttoKaaij/Ticket-To-Sustainability/?tab=readme-ov-file\r\n\r\nProject ideas (old)\r\n• Plugin from EnergiBridge (GUI, report generator, python library, etc.) \r\n• Plugin for ChatGPT (carbon emissions per chat window) \r\n• Seamless measurements for AI libraries \r\n• Energy patterns for Green AI \r\n• Sustainable SW dev gamification \r\n• Sustainability auditor for AI projects \r\n• Energy Profiling of screen colour filter tools (or display settings) \r\n• … you can also propose yours!\r\n\r\nhttps://edu.nl/64gpk\r\nedu.nl/64gpk', 'Energy profiling of static analysis tools\r\n• What is the energy consumption of a ""typical run"" for a few OSS projects? \r\n• Does the type of analysis matter? Are certain analysis more expensive? Does \r\nthe number of rules that are activated in a static analysis tool important for the \r\nenergy consumption? \r\n• Differences between static analysis tools [lower priority]\r\n\r\nB5. Tool supporting SusAF workshop / process\r\n• Lead engineers through process & questions \r\n• Interface to create & document the two diagrams \r\n• (!) Should be easy to start using & set up \r\n• You may also create a simplified version / \u2028\r\nyour favorite sustainability framework\r\nEnvironmental\r\nEconomic\r\nTechnical\r\nIndividual\r\nSocial\r\nImmediate\r\nEnabling\r\nStructural\r\nAirbnb\r\ngreater \r\nearnings\r\nincrease \r\nin rents\r\ngentrification\r\ngreater racial \r\ndisparities\r\nrent \r\nrooms\r\n5\r\n1\r\n3\r\n4\r\n2\r\n \r\n \r\n \r\n \r\n \r\n \r\nVery \r\nunlikely \r\nVery \r\nlikely \r\n\r\nB6. Dataset of government-developed OS software\r\n• Governments develop & use software for supporting society\u2028\r\nOpen-source development & policies are on the rise \r\n• But studying government software is difficult b/c we don’t know what is out \r\nthere\u2028\r\n→ Lack of incentive to make popular \u2028\r\n→ Language barriers \r\n• Create a comprehensive dataset, incl. data to understand state of software: \r\nbuildable?, open dev. history?, requirements documentation? \r\n• Could start with NL, but including your / other countries greatly appreciated!\r\n\r\nB7. Queue - but better for the student / TA society\r\n• Requirements analysis regarding social and individual sustainability effects \r\nof Queue \r\n• Other EIP / TUD used software also possible: e.g., Answers EWI \r\n• Non-technical project → proper process (workshops? Interviews?) & rich \r\ndescription of outcomes focus of grading\r\nhttps://eip.pages.ewi.tudelft.nl/eip-website/queue.html\r\n\r\nC1. Compare energy consumption of docker images for \r\nML workloads.\r\n• Similar to what we have seen in the lectures but for ML-specific workloads. \r\n• We can reuse existing experiment replication packages.\r\n22\r\n\r\nC2. Plugin to visualize Hugging Face carbon emissions.\r\n• https://huggingface.co/blog/leaderboard-emissions-\r\nanalysis \r\n•\r\nYoo, Taewon, et al. ""Visualizing the Carbon Intensity of Machine Learning Inference for Image Analysis on TensorFlow Hub.""\r\n\r\nC3. Plugin for ChatGPT\r\n• Users seldom know how much carbon they are emitting when they interact \r\nwith chat GPT. \r\n• Let’s make it transparent to the users. Browser plugin?\r\n24\r\nCO2\r\n\r\nC4. NutriScore for software libraries\r\n25\r\n• NutriScore labels are not perfect but they are a good starting point! \r\n• What if we could do the same thing for the energy efficiency of software. \r\n• (Also open to individual or social sustainability) \r\n• This work can be scoped in particular domains/ecossystems/use cases. \r\n• Libraries for stats? ML? Web Dev? Cloud?\r\n\r\nC5. Add energy-awareness to existing software\r\n26\r\n• Streamlit, Notebooks, JSFiddle, \r\nPyScript, zsh, etc. \r\n•\r\n\r\nC6 - Green Shift Left\r\n• Estimate energy efficiency using static code analysis. \r\n• We don’t need an accurate value. \r\n• It is useful to know which code is more likely to \r\nintroduce energy hotspots and that should be \r\nreviewed with more attention. \r\n• Can be scoped to a particular domain (react, php, data \r\nscience, web, etc., etc.)', '•RQ3: Can we verify the \r\norigin of energy \r\nconsumption differences by \r\nrecreating the workload \r\nbehavior closely?\r\n37\r\n\r\nTracing\r\n• Find libc function running at the end of the benchmark \r\n• Tracing: Capture calls made to libc \r\n• Introduces non-linear overhead\r\n38\r\n\r\nSynchronization\r\n• Tracing slows down execution and increase energy consumption \r\n• Run separately and synchronize with logs\r\n39\r\n\r\nTracing Analysis\r\n40\r\n\r\nTracing Analysis\r\n41\r\n\r\nBenchmarking memcpy\r\n• Copy bytes from one part of memory to \r\nanother \r\n• Depending on function parameters, different \r\nassembly-level optimizations \r\n• Some not included in musl\r\n42\r\n\r\nBenchmarking memcpy\r\n• Copy 4 bytes from fixed \r\npointer to moving pointer \r\n• (100,300,500,600) x 1M \r\nrequests \r\n•  15% more power and 3X time\r\n43\r\n\r\nConclusions\r\n• Significant difference in energy consumption for memcpy \r\n• 8.6% difference for Redis workload and 13% in our benchmark \r\n• musl trades performance for a smaller codebase \r\n• No official documentation \r\n• No awareness about energy performance\r\n44\r\n\r\n5. Energy Efficiency vs Code \r\nQuality\r\n\r\nMeasuring Maintainability\r\n• According to ISO/IEC 25010, Maintainability is “the degree of \r\neffectiveness and efficiency with which a software product or \r\nsystem can be modified to improve it, correct it or adapt it to \r\nchanges in environment, and in requirements” \r\n• We use the code analysis tool Better Code Hub to assess \r\nmaintainability \r\n• Better Code Hub maps the ISO/IEC 25010 standard on \r\nmaintainability into a set of guidelines derived from static analysis\r\n46\r\n\r\nMaintainability of Energy Changes\r\n• What is the impact of making energy-oriented code changes on \r\nthe maintainability of mobile apps? \r\nMaintainability \r\nDifference \r\nvE-2\r\nvE-1\r\nvE\r\nEnergy \r\nCommit\r\nvE+1\r\nM(vE-1)\r\nM(vE)\r\nParent \r\nCommit\r\n∆M\r\n47\r\n\r\nThreshold Marks\r\n48\r\n\r\nBetter Code Hub\r\nMaintainability\r\nCombine\r\ndatasets\r\nEnergy\r\nCommits\r\nBaseline\r\nCommits\r\nBao et al. \r\n(2015)\r\nMoura et al. \r\n(2016)\r\nCruz et al. \r\n(2018)\r\nCruz et al. \r\n(2019)\r\nEnergy Code Changes \r\nDataset\r\n539 commits\u2028\r\nfrom 306 mobile apps\r\n539 baseline commits\u2028\r\nfrom 306 mobile apps\r\n49\r\n\r\nImpact of energy changes on \r\nmaintainability\r\n50\r\n\r\nWhich energy \r\npatterns are more \r\nlikely to affect \r\nmaintainability?\r\n51\r\n\r\nTypical maintainability issue I\r\nhttps://github.com/einmalfel/PodListen/commit/2ed5a65\r\n4 changed files with 28 additions and 0 deletions.\r\n…\r\n…\r\n52\r\n\r\nTypical maintainability issue II\r\nhttps://github.com/mozilla/MozStumbler/commit/6ea0268\r\n5 changed files with 66 additions and 14 deletions.\r\n53\r\n\r\n54']","['<1-hop>\r\n\r\nC6 - Green Shift Left\r\n• Estimate energy eﬃciency using static code analysis.\r\n• We don’t need an accurate value.\r\n• It is useful to know which code is more likely to \r\nintroduce energy hotspots and that should be \r\nreviewed with more attention.\r\n• Can be scoped to a particular domain (react, php, data \r\nscience, web, etc., etc.)\r\n27', '<2-hop>\r\n\r\nWhy?\r\n• Throughout your career you might:\r\n• Design/maintain/contract data centers\r\n• Set up operations/devops\r\n• Develop AI for IoT devices\r\n• Be the next CEO/CTO of a software company\r\n• Sustainability can be your main role:\r\n• Green Software Developer\r\n• Sustainability Consultant\r\n• Green Advocate \r\n• Founder of a Green Tech startup (B2B?)\r\n28']","Static code analysis in data science can contribute to the development of more sustainable data centers by identifying and estimating energy efficiency in code. This approach, known as ""Green Shift Left,"" allows developers to pinpoint code segments that are likely to introduce energy hotspots. By addressing these hotspots early in the development process, data centers can reduce unnecessary energy consumption, leading to more sustainable operations. This proactive measure helps in optimizing code for energy efficiency, which can significantly lower the overall energy footprint of data centers.","Static code analysis in data science can contribute to the development of more sustainable data centers by estimating energy efficiency and identifying potential energy hotspots within the code. This approach allows developers to focus on reviewing code segments that are more likely to introduce inefficiencies, thereby optimizing resource usage. By integrating such practices, especially in domains like data science, developers can help create more sustainable solutions, which aligns with broader career goals in sustainability, such as designing or maintaining energy-efficient data centers.",0.9999999999666668,0.6666666666666666,1.0,0.67,0.3749999995312499,0.9797426468827596
How can static code analysis in data science projects help in designing more sustainablility-focused data centers?,"['Interviews?) & rich \r\ndescription of outcomes focus of grading\r\nhttps://eip.pages.ewi.tudelft.nl/eip-website/queue.html\r\n\r\nC1. Compare energy consumption of docker images for \r\nML workloads.\r\n• Similar to what we have seen in the lectures but for ML-specific workloads. \r\n• We can reuse existing experiment replication packages.\r\n22\r\n\r\nC2. Plugin to visualize Hugging Face carbon emissions.\r\n• https://huggingface.co/blog/leaderboard-emissions-\r\nanalysis \r\n•\r\nYoo, Taewon, et al. ""Visualizing the Carbon Intensity of Machine Learning Inference for Image Analysis on TensorFlow Hub.""\r\n\r\nC3. Plugin for ChatGPT\r\n• Users seldom know how much carbon they are emitting when they interact \r\nwith chat GPT. \r\n• Let’s make it transparent to the users. Browser plugin?\r\n24\r\nCO2\r\n\r\nC4. NutriScore for software libraries\r\n25\r\n• NutriScore labels are not perfect but they are a good starting point! \r\n• What if we could do the same thing for the energy efficiency of software. \r\n• (Also open to individual or social sustainability) \r\n• This work can be scoped in particular domains/ecossystems/use cases. \r\n• Libraries for stats? ML? Web Dev? Cloud?\r\n\r\nC5. Add energy-awareness to existing software\r\n26\r\n• Streamlit, Notebooks, JSFiddle, \r\nPyScript, zsh, etc. \r\n•\r\n\r\nC6 - Green Shift Left\r\n• Estimate energy efficiency using static code analysis. \r\n• We don’t need an accurate value. \r\n• It is useful to know which code is more likely to \r\nintroduce energy hotspots and that should be \r\nreviewed with more attention. \r\n• Can be scoped to a particular domain (react, php, data \r\nscience, web, etc., etc.)\r\n27\r\n\r\nC7 - Education\r\n• Educational game for Software \r\nSustainability practices \r\n• Purpose: use within software teams to \r\ndiscuss or learn about different sustainable \r\nIT practices: at the organisation level, \r\nsoftware, etc.\r\n28\r\nhttps://github.com/OttoKaaij/Ticket-To-Sustainability/?tab=readme-ov-file\r\n\r\nProject ideas (old)\r\n• Plugin from EnergiBridge (GUI, report generator, python library, etc.) \r\n• Plugin for ChatGPT (carbon emissions per chat window) \r\n• Seamless measurements for AI libraries \r\n• Energy patterns for Green AI \r\n• Sustainable SW dev gamification \r\n• Sustainability auditor for AI projects \r\n• Energy Profiling of screen colour filter tools (or display settings) \r\n• … you can also propose yours!\r\n\r\nhttps://edu.nl/64gpk\r\nedu.nl/64gpk', 'Energy profiling of static analysis tools\r\n• What is the energy consumption of a ""typical run"" for a few OSS projects? \r\n• Does the type of analysis matter? Are certain analysis more expensive? Does \r\nthe number of rules that are activated in a static analysis tool important for the \r\nenergy consumption? \r\n• Differences between static analysis tools [lower priority]\r\n\r\nB5. Tool supporting SusAF workshop / process\r\n• Lead engineers through process & questions \r\n• Interface to create & document the two diagrams \r\n• (!) Should be easy to start using & set up \r\n• You may also create a simplified version / \u2028\r\nyour favorite sustainability framework\r\nEnvironmental\r\nEconomic\r\nTechnical\r\nIndividual\r\nSocial\r\nImmediate\r\nEnabling\r\nStructural\r\nAirbnb\r\ngreater \r\nearnings\r\nincrease \r\nin rents\r\ngentrification\r\ngreater racial \r\ndisparities\r\nrent \r\nrooms\r\n5\r\n1\r\n3\r\n4\r\n2\r\n \r\n \r\n \r\n \r\n \r\n \r\nVery \r\nunlikely \r\nVery \r\nlikely \r\n\r\nB6. Dataset of government-developed OS software\r\n• Governments develop & use software for supporting society\u2028\r\nOpen-source development & policies are on the rise \r\n• But studying government software is difficult b/c we don’t know what is out \r\nthere\u2028\r\n→ Lack of incentive to make popular \u2028\r\n→ Language barriers \r\n• Create a comprehensive dataset, incl. data to understand state of software: \r\nbuildable?, open dev. history?, requirements documentation? \r\n• Could start with NL, but including your / other countries greatly appreciated!\r\n\r\nB7. Queue - but better for the student / TA society\r\n• Requirements analysis regarding social and individual sustainability effects \r\nof Queue \r\n• Other EIP / TUD used software also possible: e.g., Answers EWI \r\n• Non-technical project → proper process (workshops? Interviews?) & rich \r\ndescription of outcomes focus of grading\r\nhttps://eip.pages.ewi.tudelft.nl/eip-website/queue.html\r\n\r\nC1. Compare energy consumption of docker images for \r\nML workloads.\r\n• Similar to what we have seen in the lectures but for ML-specific workloads. \r\n• We can reuse existing experiment replication packages.\r\n22\r\n\r\nC2. Plugin to visualize Hugging Face carbon emissions.\r\n• https://huggingface.co/blog/leaderboard-emissions-\r\nanalysis \r\n•\r\nYoo, Taewon, et al. ""Visualizing the Carbon Intensity of Machine Learning Inference for Image Analysis on TensorFlow Hub.""\r\n\r\nC3. Plugin for ChatGPT\r\n• Users seldom know how much carbon they are emitting when they interact \r\nwith chat GPT. \r\n• Let’s make it transparent to the users. Browser plugin?\r\n24\r\nCO2\r\n\r\nC4. NutriScore for software libraries\r\n25\r\n• NutriScore labels are not perfect but they are a good starting point! \r\n• What if we could do the same thing for the energy efficiency of software. \r\n• (Also open to individual or social sustainability) \r\n• This work can be scoped in particular domains/ecossystems/use cases. \r\n• Libraries for stats? ML? Web Dev? Cloud?\r\n\r\nC5. Add energy-awareness to existing software\r\n26\r\n• Streamlit, Notebooks, JSFiddle, \r\nPyScript, zsh, etc. \r\n•\r\n\r\nC6 - Green Shift Left\r\n• Estimate energy efficiency using static code analysis. \r\n• We don’t need an accurate value. \r\n• It is useful to know which code is more likely to \r\nintroduce energy hotspots and that should be \r\nreviewed with more attention. \r\n• Can be scoped to a particular domain (react, php, data \r\nscience, web, etc., etc.)', '•RQ3: Can we verify the \r\norigin of energy \r\nconsumption differences by \r\nrecreating the workload \r\nbehavior closely?\r\n37\r\n\r\nTracing\r\n• Find libc function running at the end of the benchmark \r\n• Tracing: Capture calls made to libc \r\n• Introduces non-linear overhead\r\n38\r\n\r\nSynchronization\r\n• Tracing slows down execution and increase energy consumption \r\n• Run separately and synchronize with logs\r\n39\r\n\r\nTracing Analysis\r\n40\r\n\r\nTracing Analysis\r\n41\r\n\r\nBenchmarking memcpy\r\n• Copy bytes from one part of memory to \r\nanother \r\n• Depending on function parameters, different \r\nassembly-level optimizations \r\n• Some not included in musl\r\n42\r\n\r\nBenchmarking memcpy\r\n• Copy 4 bytes from fixed \r\npointer to moving pointer \r\n• (100,300,500,600) x 1M \r\nrequests \r\n•  15% more power and 3X time\r\n43\r\n\r\nConclusions\r\n• Significant difference in energy consumption for memcpy \r\n• 8.6% difference for Redis workload and 13% in our benchmark \r\n• musl trades performance for a smaller codebase \r\n• No official documentation \r\n• No awareness about energy performance\r\n44\r\n\r\n5. Energy Efficiency vs Code \r\nQuality\r\n\r\nMeasuring Maintainability\r\n• According to ISO/IEC 25010, Maintainability is “the degree of \r\neffectiveness and efficiency with which a software product or \r\nsystem can be modified to improve it, correct it or adapt it to \r\nchanges in environment, and in requirements” \r\n• We use the code analysis tool Better Code Hub to assess \r\nmaintainability \r\n• Better Code Hub maps the ISO/IEC 25010 standard on \r\nmaintainability into a set of guidelines derived from static analysis\r\n46\r\n\r\nMaintainability of Energy Changes\r\n• What is the impact of making energy-oriented code changes on \r\nthe maintainability of mobile apps? \r\nMaintainability \r\nDifference \r\nvE-2\r\nvE-1\r\nvE\r\nEnergy \r\nCommit\r\nvE+1\r\nM(vE-1)\r\nM(vE)\r\nParent \r\nCommit\r\n∆M\r\n47\r\n\r\nThreshold Marks\r\n48\r\n\r\nBetter Code Hub\r\nMaintainability\r\nCombine\r\ndatasets\r\nEnergy\r\nCommits\r\nBaseline\r\nCommits\r\nBao et al. \r\n(2015)\r\nMoura et al. \r\n(2016)\r\nCruz et al. \r\n(2018)\r\nCruz et al. \r\n(2019)\r\nEnergy Code Changes \r\nDataset\r\n539 commits\u2028\r\nfrom 306 mobile apps\r\n539 baseline commits\u2028\r\nfrom 306 mobile apps\r\n49\r\n\r\nImpact of energy changes on \r\nmaintainability\r\n50\r\n\r\nWhich energy \r\npatterns are more \r\nlikely to affect \r\nmaintainability?\r\n51\r\n\r\nTypical maintainability issue I\r\nhttps://github.com/einmalfel/PodListen/commit/2ed5a65\r\n4 changed files with 28 additions and 0 deletions.\r\n…\r\n…\r\n52\r\n\r\nTypical maintainability issue II\r\nhttps://github.com/mozilla/MozStumbler/commit/6ea0268\r\n5 changed files with 66 additions and 14 deletions.\r\n53\r\n\r\n54']","['<1-hop>\r\n\r\nC6 - Green Shift Left\r\n• Estimate energy eﬃciency using static code analysis.\r\n• We don’t need an accurate value.\r\n• It is useful to know which code is more likely to \r\nintroduce energy hotspots and that should be \r\nreviewed with more attention.\r\n• Can be scoped to a particular domain (react, php, data \r\nscience, web, etc., etc.)\r\n27', '<2-hop>\r\n\r\nWhy?\r\n• Throughout your career you might:\r\n• Design/maintain/contract data centers\r\n• Set up operations/devops\r\n• Develop AI for IoT devices\r\n• Be the next CEO/CTO of a software company\r\n• Sustainability can be your main role:\r\n• Green Software Developer\r\n• Sustainability Consultant\r\n• Green Advocate \r\n• Founder of a Green Tech startup (B2B?)\r\n28', '<1-hop>\r\n\r\nC6 - Green Shift Left\r\n• Estimate energy eﬃciency using static code analysis.\r\n• We don’t need an accurate value.\r\n• It is useful to know which code is more likely to \r\nintroduce energy hotspots and that should be \r\nreviewed with more attention.\r\n• Can be scoped to a particular domain (react, php, data \r\nscience, web, etc., etc.)\r\n27', '<2-hop>\r\n\r\nWhy?\r\n• Throughout your career you might:\r\n• Design/maintain/contract data centers\r\n• Set up operations/devops\r\n• Develop AI for IoT devices\r\n• Be the next CEO/CTO of a software company\r\n• Sustainability can be your main role:\r\n• Green Software Developer\r\n• Sustainability Consultant\r\n• Green Advocate \r\n• Founder of a Green Tech startup (B2B?)\r\n28', '<1-hop>\r\n\r\nC6 - Green Shift Left\r\n• Estimate energy eﬃciency using static code analysis.\r\n• We don’t need an accurate value.\r\n• It is useful to know which code is more likely to \r\nintroduce energy hotspots and that should be \r\nreviewed with more attention.\r\n• Can be scoped to a particular domain (react, php, data \r\nscience, web, etc., etc.)\r\n27', '<2-hop>\r\n\r\nWhy?\r\n• Throughout your career you might:\r\n• Design/maintain/contract data centers\r\n• Set up operations/devops\r\n• Develop AI for IoT devices\r\n• Be the next CEO/CTO of a software company\r\n• Sustainability can be your main role:\r\n• Green Software Developer\r\n• Sustainability Consultant\r\n• Green Advocate \r\n• Founder of a Green Tech startup (B2B?)\r\n28']","Static code analysis in data science projects can help in designing more sustainability-focused data centers by identifying code that is likely to introduce energy hotspots. By estimating energy efficiency through static code analysis, developers can pinpoint parts of the code that consume more energy and should be reviewed with more attention. This proactive approach allows for optimizing code to reduce energy consumption, which in turn can lead to more energy-efficient data centers. This process is part of the ""Green Shift Left"" initiative, which emphasizes the importance of considering energy efficiency early in the software development lifecycle. By doing so, data centers can be designed to operate more sustainably, reducing their overall carbon footprint.","Static code analysis in data science projects can help in designing more sustainablility-focused data centers by identifying energy hotspots in the code. This allows developers to review and optimize code that may contribute to higher energy consumption, thereby supporting the development of more efficient and sustainable data centers.",0.99999999995,1.0,0.6666666666666666,0.8,,0.9690297750567164
How does the use of static code analysis in data science projects contribute to the design and optimization of more sustanable data centers?,"['Interviews?) & rich \r\ndescription of outcomes focus of grading\r\nhttps://eip.pages.ewi.tudelft.nl/eip-website/queue.html\r\n\r\nC1. Compare energy consumption of docker images for \r\nML workloads.\r\n• Similar to what we have seen in the lectures but for ML-specific workloads. \r\n• We can reuse existing experiment replication packages.\r\n22\r\n\r\nC2. Plugin to visualize Hugging Face carbon emissions.\r\n• https://huggingface.co/blog/leaderboard-emissions-\r\nanalysis \r\n•\r\nYoo, Taewon, et al. ""Visualizing the Carbon Intensity of Machine Learning Inference for Image Analysis on TensorFlow Hub.""\r\n\r\nC3. Plugin for ChatGPT\r\n• Users seldom know how much carbon they are emitting when they interact \r\nwith chat GPT. \r\n• Let’s make it transparent to the users. Browser plugin?\r\n24\r\nCO2\r\n\r\nC4. NutriScore for software libraries\r\n25\r\n• NutriScore labels are not perfect but they are a good starting point! \r\n• What if we could do the same thing for the energy efficiency of software. \r\n• (Also open to individual or social sustainability) \r\n• This work can be scoped in particular domains/ecossystems/use cases. \r\n• Libraries for stats? ML? Web Dev? Cloud?\r\n\r\nC5. Add energy-awareness to existing software\r\n26\r\n• Streamlit, Notebooks, JSFiddle, \r\nPyScript, zsh, etc. \r\n•\r\n\r\nC6 - Green Shift Left\r\n• Estimate energy efficiency using static code analysis. \r\n• We don’t need an accurate value. \r\n• It is useful to know which code is more likely to \r\nintroduce energy hotspots and that should be \r\nreviewed with more attention. \r\n• Can be scoped to a particular domain (react, php, data \r\nscience, web, etc., etc.)\r\n27\r\n\r\nC7 - Education\r\n• Educational game for Software \r\nSustainability practices \r\n• Purpose: use within software teams to \r\ndiscuss or learn about different sustainable \r\nIT practices: at the organisation level, \r\nsoftware, etc.\r\n28\r\nhttps://github.com/OttoKaaij/Ticket-To-Sustainability/?tab=readme-ov-file\r\n\r\nProject ideas (old)\r\n• Plugin from EnergiBridge (GUI, report generator, python library, etc.) \r\n• Plugin for ChatGPT (carbon emissions per chat window) \r\n• Seamless measurements for AI libraries \r\n• Energy patterns for Green AI \r\n• Sustainable SW dev gamification \r\n• Sustainability auditor for AI projects \r\n• Energy Profiling of screen colour filter tools (or display settings) \r\n• … you can also propose yours!\r\n\r\nhttps://edu.nl/64gpk\r\nedu.nl/64gpk', 'Energy profiling of static analysis tools\r\n• What is the energy consumption of a ""typical run"" for a few OSS projects? \r\n• Does the type of analysis matter? Are certain analysis more expensive? Does \r\nthe number of rules that are activated in a static analysis tool important for the \r\nenergy consumption? \r\n• Differences between static analysis tools [lower priority]\r\n\r\nB5. Tool supporting SusAF workshop / process\r\n• Lead engineers through process & questions \r\n• Interface to create & document the two diagrams \r\n• (!) Should be easy to start using & set up \r\n• You may also create a simplified version / \u2028\r\nyour favorite sustainability framework\r\nEnvironmental\r\nEconomic\r\nTechnical\r\nIndividual\r\nSocial\r\nImmediate\r\nEnabling\r\nStructural\r\nAirbnb\r\ngreater \r\nearnings\r\nincrease \r\nin rents\r\ngentrification\r\ngreater racial \r\ndisparities\r\nrent \r\nrooms\r\n5\r\n1\r\n3\r\n4\r\n2\r\n \r\n \r\n \r\n \r\n \r\n \r\nVery \r\nunlikely \r\nVery \r\nlikely \r\n\r\nB6. Dataset of government-developed OS software\r\n• Governments develop & use software for supporting society\u2028\r\nOpen-source development & policies are on the rise \r\n• But studying government software is difficult b/c we don’t know what is out \r\nthere\u2028\r\n→ Lack of incentive to make popular \u2028\r\n→ Language barriers \r\n• Create a comprehensive dataset, incl. data to understand state of software: \r\nbuildable?, open dev. history?, requirements documentation? \r\n• Could start with NL, but including your / other countries greatly appreciated!\r\n\r\nB7. Queue - but better for the student / TA society\r\n• Requirements analysis regarding social and individual sustainability effects \r\nof Queue \r\n• Other EIP / TUD used software also possible: e.g., Answers EWI \r\n• Non-technical project → proper process (workshops? Interviews?) & rich \r\ndescription of outcomes focus of grading\r\nhttps://eip.pages.ewi.tudelft.nl/eip-website/queue.html\r\n\r\nC1. Compare energy consumption of docker images for \r\nML workloads.\r\n• Similar to what we have seen in the lectures but for ML-specific workloads. \r\n• We can reuse existing experiment replication packages.\r\n22\r\n\r\nC2. Plugin to visualize Hugging Face carbon emissions.\r\n• https://huggingface.co/blog/leaderboard-emissions-\r\nanalysis \r\n•\r\nYoo, Taewon, et al. ""Visualizing the Carbon Intensity of Machine Learning Inference for Image Analysis on TensorFlow Hub.""\r\n\r\nC3. Plugin for ChatGPT\r\n• Users seldom know how much carbon they are emitting when they interact \r\nwith chat GPT. \r\n• Let’s make it transparent to the users. Browser plugin?\r\n24\r\nCO2\r\n\r\nC4. NutriScore for software libraries\r\n25\r\n• NutriScore labels are not perfect but they are a good starting point! \r\n• What if we could do the same thing for the energy efficiency of software. \r\n• (Also open to individual or social sustainability) \r\n• This work can be scoped in particular domains/ecossystems/use cases. \r\n• Libraries for stats? ML? Web Dev? Cloud?\r\n\r\nC5. Add energy-awareness to existing software\r\n26\r\n• Streamlit, Notebooks, JSFiddle, \r\nPyScript, zsh, etc. \r\n•\r\n\r\nC6 - Green Shift Left\r\n• Estimate energy efficiency using static code analysis. \r\n• We don’t need an accurate value. \r\n• It is useful to know which code is more likely to \r\nintroduce energy hotspots and that should be \r\nreviewed with more attention. \r\n• Can be scoped to a particular domain (react, php, data \r\nscience, web, etc., etc.)', '•RQ3: Can we verify the \r\norigin of energy \r\nconsumption differences by \r\nrecreating the workload \r\nbehavior closely?\r\n37\r\n\r\nTracing\r\n• Find libc function running at the end of the benchmark \r\n• Tracing: Capture calls made to libc \r\n• Introduces non-linear overhead\r\n38\r\n\r\nSynchronization\r\n• Tracing slows down execution and increase energy consumption \r\n• Run separately and synchronize with logs\r\n39\r\n\r\nTracing Analysis\r\n40\r\n\r\nTracing Analysis\r\n41\r\n\r\nBenchmarking memcpy\r\n• Copy bytes from one part of memory to \r\nanother \r\n• Depending on function parameters, different \r\nassembly-level optimizations \r\n• Some not included in musl\r\n42\r\n\r\nBenchmarking memcpy\r\n• Copy 4 bytes from fixed \r\npointer to moving pointer \r\n• (100,300,500,600) x 1M \r\nrequests \r\n•  15% more power and 3X time\r\n43\r\n\r\nConclusions\r\n• Significant difference in energy consumption for memcpy \r\n• 8.6% difference for Redis workload and 13% in our benchmark \r\n• musl trades performance for a smaller codebase \r\n• No official documentation \r\n• No awareness about energy performance\r\n44\r\n\r\n5. Energy Efficiency vs Code \r\nQuality\r\n\r\nMeasuring Maintainability\r\n• According to ISO/IEC 25010, Maintainability is “the degree of \r\neffectiveness and efficiency with which a software product or \r\nsystem can be modified to improve it, correct it or adapt it to \r\nchanges in environment, and in requirements” \r\n• We use the code analysis tool Better Code Hub to assess \r\nmaintainability \r\n• Better Code Hub maps the ISO/IEC 25010 standard on \r\nmaintainability into a set of guidelines derived from static analysis\r\n46\r\n\r\nMaintainability of Energy Changes\r\n• What is the impact of making energy-oriented code changes on \r\nthe maintainability of mobile apps? \r\nMaintainability \r\nDifference \r\nvE-2\r\nvE-1\r\nvE\r\nEnergy \r\nCommit\r\nvE+1\r\nM(vE-1)\r\nM(vE)\r\nParent \r\nCommit\r\n∆M\r\n47\r\n\r\nThreshold Marks\r\n48\r\n\r\nBetter Code Hub\r\nMaintainability\r\nCombine\r\ndatasets\r\nEnergy\r\nCommits\r\nBaseline\r\nCommits\r\nBao et al. \r\n(2015)\r\nMoura et al. \r\n(2016)\r\nCruz et al. \r\n(2018)\r\nCruz et al. \r\n(2019)\r\nEnergy Code Changes \r\nDataset\r\n539 commits\u2028\r\nfrom 306 mobile apps\r\n539 baseline commits\u2028\r\nfrom 306 mobile apps\r\n49\r\n\r\nImpact of energy changes on \r\nmaintainability\r\n50\r\n\r\nWhich energy \r\npatterns are more \r\nlikely to affect \r\nmaintainability?\r\n51\r\n\r\nTypical maintainability issue I\r\nhttps://github.com/einmalfel/PodListen/commit/2ed5a65\r\n4 changed files with 28 additions and 0 deletions.\r\n…\r\n…\r\n52\r\n\r\nTypical maintainability issue II\r\nhttps://github.com/mozilla/MozStumbler/commit/6ea0268\r\n5 changed files with 66 additions and 14 deletions.\r\n53\r\n\r\n54']","['<1-hop>\r\n\r\nC6 - Green Shift Left\r\n• Estimate energy eﬃciency using static code analysis.\r\n• We don’t need an accurate value.\r\n• It is useful to know which code is more likely to \r\nintroduce energy hotspots and that should be \r\nreviewed with more attention.\r\n• Can be scoped to a particular domain (react, php, data \r\nscience, web, etc., etc.)\r\n27', '<2-hop>\r\n\r\nWhy?\r\n• Throughout your career you might:\r\n• Design/maintain/contract data centers\r\n• Set up operations/devops\r\n• Develop AI for IoT devices\r\n• Be the next CEO/CTO of a software company\r\n• Sustainability can be your main role:\r\n• Green Software Developer\r\n• Sustainability Consultant\r\n• Green Advocate \r\n• Founder of a Green Tech startup (B2B?)\r\n28', '<1-hop>\r\n\r\nC6 - Green Shift Left\r\n• Estimate energy eﬃciency using static code analysis.\r\n• We don’t need an accurate value.\r\n• It is useful to know which code is more likely to \r\nintroduce energy hotspots and that should be \r\nreviewed with more attention.\r\n• Can be scoped to a particular domain (react, php, data \r\nscience, web, etc., etc.)\r\n27', '<2-hop>\r\n\r\nWhy?\r\n• Throughout your career you might:\r\n• Design/maintain/contract data centers\r\n• Set up operations/devops\r\n• Develop AI for IoT devices\r\n• Be the next CEO/CTO of a software company\r\n• Sustainability can be your main role:\r\n• Green Software Developer\r\n• Sustainability Consultant\r\n• Green Advocate \r\n• Founder of a Green Tech startup (B2B?)\r\n28']","The use of static code analysis in data science projects can contribute to the design and optimization of more sustainable data centers by identifying energy inefficiencies in the code. Static code analysis allows developers to estimate energy efficiency without needing precise values, which helps in pinpointing code segments likely to introduce energy hotspots. By addressing these hotspots, developers can optimize code to reduce energy consumption, leading to more energy-efficient software. This, in turn, can contribute to the overall sustainability of data centers by reducing their energy demands. This approach is part of the ""Green Shift Left"" initiative, which emphasizes early detection and mitigation of energy inefficiencies in the software development lifecycle.","The use of static code analysis in data science projects contributes to the design and optimization of more sustainable data centers by identifying energy hotspots within the code. This process allows developers to focus on reviewing and optimizing sections of code that are more likely to introduce energy inefficiencies. By doing so, it supports the broader goal of creating more energy-efficient systems, which is crucial for the sustainable operation of data centers. This approach aligns with the growing need for green software development practices, where professionals can play roles such as green software developers or sustainability consultants, ensuring that energy efficiency is a key consideration throughout the software lifecycle.",0.99999999995,0.5,1.0,0.8,0.0,0.98066355261324
How reduce risk and reduce size in software engineering?,"['8. Green AI\r\nSustainable Software Engineering\u2028\r\nCS4575\r\nSustainableSE 2025\r\nLuís Cruz \r\nL.Cruz@tudelft.nl \r\nCarolin Brandt \r\nC.E.Brandt@tudelft.nl \r\nEnrique Barba Roque \r\nE.BarbaRoque@tudelft.nl\r\n\r\n- Overview of Green AI \r\n- Large language models \r\n- Green data-centric AI \r\n- Model simplification \r\n- Hyper parameter tuning \r\n- Mixture of Experts and DeepSeek \r\n- Green AI at Meta \r\n\r\nAI\r\n• Artificial Intelligence (AI) is the branch of computer science that deals with \r\nautomating tasks that typically require human intelligence. \r\n• In the past years AI has been widely applied across different domains.\u2028\r\nE.g., health care, transportation, finance. \r\n• To deploy AI systems, we test them against benchmarks (or validation sets). \r\n• The goal is to outperform the previous existing models. \r\n• E.g., in Machine Learning we usually resort to accuracy metrics. The \r\nhighest the accuracy, the better the model.\r\n3\r\n\r\nSince 2012, the amount of computing used for AI \r\ntraining has been doubling every 6 months\r\n• https://epoch.ai/blog/compute-trends\r\n4\r\n\r\n• To create better AI systems we are currently adding \r\n• More data \r\n• More experiments \r\n• Larger models\r\n5\r\n\r\nThe Equation of Red AI\r\nCost(R) ∝E·D·H\r\nCost of a single (E)xample\r\nSize of (D)ataset\r\nNumber of (H)yperparameters\r\nBy Schwartz et al. (2020)\r\n\r\nIssues of Red AI\r\n• High costs (hardware, electricity, data access, etc.) \r\n• Limited reproducibility.  \r\n• Energy consumption. \r\n• Carbon emissions. \r\n• SMEs can hardly be competitive. \r\n• Groundbreaking AI research is mostly done by tech giants.\r\n7\r\n\r\nA few examples of Red AI\r\n• Google’s BERT-large \r\n• 350 million features \r\n• Trained for 2.5 days using 512 TPU chips, costing $60K+ \r\n• Open-GPT3 (now GPT-4/o1) \r\n• 550 tonnes CO2-eq (Patterson, 2021) \r\n• 175 billion features \r\n• API is open but no-pretrained model is available \r\n• AlphaGo \r\n• 1920 CPUs, 280 GPUs, costing $35M\r\n8\r\n\r\nRed AI in Large Language Models (LLMs)\r\n• OPT by Meta reports 75 tons CO2-eq (1/7 of OpenGPT’s footprint).\u2028\r\n(Also 175billion params) \r\n• However, Llama 3 reported 2,290 tons of CO2-eq (7.7M GPU hours \r\ntraining ) \r\n• Open science: release includes both the pretrained models and the code \r\nneeded to train and use them. \r\n• DeepSeek-V3 claims “only” 2.78M GPU hours \r\n• Bloom by Huggingface reports 25 tons, 51 when considering embodied \r\nand operational carbon footprint. (176billion params)\r\n9\r\n\r\nRed AI\r\nAccuracy: 0.999999999\r\nGreen AI\r\n• Energy \r\n• Time \r\n• Reproducibility \r\n• Reusage\r\n\r\nHow can we adopt Green AI\r\n• Check whether AI is needed. \r\n• Select green datacenters. \r\n• Run on low carbon intensity hours. \r\n• Opt for GPU-optimised solutions (?) \r\n• Opt for low-power hardware (e.g., Nvidia Jetson boards) \r\n• Or GPUs that provide energy metrics (e.g., NVIDIA GPUs via the nvidia-smi tool) \r\n• Report energy/carbon metrics (e.g., embed in MLFlow?)', ""• Our question: Can we save energy with alternative approaches? \r\n• We studied Grid Search, Random Search and Bayesian \r\nOptimisation.\r\n27\r\nTim's MSc Thesis, 2022 \r\nAccepted at CAIN’23\r\n\r\n\r\nResults\r\n29\r\nConclusions? \r\n• Bayesian converges faster. \r\n• No clear winner between Grid and Random\r\n\r\nDeepSeekMoE\r\n30\r\nPreprint at ArXiv, 2024 https://arxiv.org/pdf/\r\n2401.06066\r\n\r\nMixture of Experts\r\n• Llama3.1 has 405B parameters, DeepSeek V3 671B \r\n• Yet DeepSeek has quicker inference times and claims less energy \r\nconsumption (?) \r\n• Divide the model into smaller blocks of experts \r\n• Tokens get routed to certain experts based on the query \r\n• Only part of the network is active during inference \r\n• DeepSeek claims only 37B out of 671B parameters get active\r\n31\r\n\r\nDeepSeekMoE\r\n32\r\n\r\nDeepSeekMoE\r\n33\r\n• Comparable performance to \r\nLLaMA2 7B effectively using \r\nless half the parameters \r\n• Less computational power \r\n• Problems (?) \r\n• Still need to load all the \r\nparameters \r\n• High memory -> high \r\nembodied carbon\r\n\r\nGreen AI at FacebookMeta\r\n34\r\nSustainable AI: Environmental Implications, \r\nChallenges and Opportunities (2022)\r\n\r\nCarbon footprint mapped to the AI lifecycle\r\n• There are 4 main overarching stages where carbon emissions need to be isolated: data \r\ncollection, experimentation, training, inference. \r\n• At Facebook, recommendation systems split energy consumption evenly between \r\ntraining and inference; text translation models have a 35%/65% split. (Operational cost) \r\n• Operational/embodied cost split: 30%/70%\r\n35\r\n\r\nOpen issues according to Meta\r\n• A vast portion of projects only use GPUs at 30%.\u2028\r\nShould be higher to attenuate embodied carbon.\r\n36\r\nBased on 10K AI projects\r\n\r\nData Jan\r\nInformed Adaptation\r\nData Feb\r\nData Mar\r\nData Apr\r\nData May\r\n…\r\n~Data Dec\r\n~Data Dec\r\nData Jan\r\nData Feb\r\nData Mar\r\nData Apr\r\nData May\r\n…\r\nModel update Model update Model update\r\nCheck if \r\ndata change\r\nCheck if \r\ndata change\r\nCheck if \r\ndata change\r\nModel update\r\nBlind Adaptation\r\nThe AI Model will be \r\nupdated fewer times and \r\nonly when necessary.\r\nKnow when to retrain models\r\nNeither too early nor too late\r\n37\r\nAdaptation Techniques\r\n?\r\n\r\nGreen Architectural Tactics for ML-Enabled \r\nSystem\r\n38\r\nICSE-SEIS 2024\r\n\r\nArchitectural tactics\r\n39\r\n\r\nData-centric\r\n40\r\n• Reduce data size \r\n• Sampling \r\n• Dimensionality reduction \r\n• Quantization\r\n\r\nAlgorithm Design\r\n41\r\n• Carefully select your algorithm \r\n• You don’t need the fanciest \r\ntechniques\r\n\r\nModel Optimization\r\n42\r\n• Add energy to training parameters \r\n• Reduce FLOPs \r\n• Pruning, sparsity \r\n•  Take advantage of existing models\r\n\r\nModel Training\r\n43\r\n• Quantization\r\n• SAVE TRAINING PROGRESS\r\n\r\nModel Deployment\r\n44\r\n• Distributed deployment \r\n• Energy efficient hardware and \r\nconfigurations\r\n\r\nModel Management\r\n45\r\n• Reuse the model as much as possible \r\n\r\nRethinking the Architecture: Spiking Neural Networks\r\n46\r\n\r\nRethinking the Architecture: Spiking Neural Networks\r\n47\r\n\r\nSELF Lab\r\n48\r\n\r\nrecap"", 'Luís Cruz \r\nL.Cruz@tudelft.nl \r\n7. Green SE – Research\r\nSustainable Software Engineering\u2028\r\nCS4575\r\nSustainableSE 2025\r\nCarolin Brandt \r\nC.E.Brandt@tudelft.nl \r\nEnrique Barba Roque \r\nE.BarbaRoque@tudelft.nl\r\n\r\n1. Energy patterns for mobile apps \r\n2. Carbon-aware datacenters \r\n3. Energy Regression Testing \r\n4. Debugging Energy with Docker images \r\n5. Energy Efficiency vs Code Quality \r\n\r\n• While learning about these works, try to be critical about them and find their \r\npitfalls.\r\n3\r\n\r\n• We have seen that measuring energy consumption is not trivial \r\n• It is not practical considering that developers have other priorities above \r\nenergy efficiency \r\n• At the same time, every now and then there are some efforts to improve \r\nenergy efficiency in some cases. This is time consuming and requires \r\nexpertise. \r\n• How can we reuse these efforts?\r\n4\r\n\r\nEnergy Patterns for Mobile \r\nApps\r\nhttps://tqrg.github.io/energy-patterns/\r\n\r\nMethodology\r\n5. Catalog of Energy Patterns\r\n22 \r\npatterns\r\nF-droid\r\nCurated Lists\r\n1. App Collection\r\n1783 \r\napps\r\n3. Manual Refinement of Subjects of \r\nInterest\r\n1563 \r\nchanges\r\n4. Thematic Analysis\r\n431 \r\nreusable \r\nchanges\r\n2. Collect Changes With Potential Interest\r\n/.*(energy|power|battery).*/\r\n6028 \r\nchanges\r\n\r\nThematic Analysis\r\n1. Familiarization with data \r\n2. Generating initial labels \r\n3. Reviewing themes\r\n4. Defining and naming themes\r\n\r\n• Energy Pattern: design pattern to improve energy efficiency.. \r\n• 22 energy patterns. \r\n• Each pattern is described by Context, Solution, Example, References from \r\nliterature, and Occurences (links to code changes from git repositories).\r\n8\r\n\r\nhttps://tqrg.github.io/energy-patterns\r\n\r\nDark UI Colors \r\nProvide a dark UI color theme to \r\nsave battery on devices with \r\nAMOLED screens.\r\n• Context: […] Apps that require heavy usage of screen can \r\nhave a substantial negative impact on battery life. \r\n• Solution: Provide a UI theme with dark background colors. […]\r\n• Example: In a reading app, provide a theme with a dark \r\nbackground using light colors to display text. […] \r\n10\r\n\r\nDark UI Colors \r\nProvide a dark UI color theme to \r\nsave battery on devices with \r\nAMOLED screens.\r\n• Context: […] Apps that require heavy usage of screen can \r\nhave a substantial negative impact on battery life. \r\n• Solution: Provide a UI theme with dark background colors. […]\r\n• Example: In a reading app, provide a theme with a dark \r\nbackground using light colors to display text. […] \r\n11\r\n\r\nDynamic Retry Delay\r\n• Context: […] In a mobile app, when a given resource is \r\nunavailable, the app will unnecessarily try to connect the resource \r\nfor a number of times, leading to unnecessary power consumption. \r\n• Solution: Increase retry interval after each failed connection. […]\r\n• Example: Consider a mobile app that provides a news feed and \r\nthe app is not able to reach the server to collect updates. […] use \r\nthe Fibonacci series to increase the time between attempts.\r\nWhenever an attempt to access a \r\nresource fails, increase the time \r\ninterval before retrying.\r\n12\r\n\r\nBatch Operations \r\n• Context: Executing operations separately leads to extraneous \r\ntail energy consumptions  \r\n• Solution: Bundle multiple operations in a single one. […]\r\n• Example: Use system provided APIs to schedule background \r\ntasks. These APIs, guarantee that device will exit sleep mode \r\nonly when there is a reasonable amount of work to do or when a \r\ngiven task is urgent.']","['<1-hop>\r\n\r\nShuffle \r\n🔀\r\n• It is not a mystery that energy consumption depends on so many factors that \r\nit is impossible to control all of them.\r\n• If you run 30 executions for version A and another batch for version B:\r\n• External conditions that change over time will have a diﬀerent bias in \r\nthe 2 versions (e.g., room temperature changes).\r\n• If you shuﬄe, you reduce this risk.\r\n11', '<2-hop>\r\n\r\nEcoAndroid\r\n• Plugin for IntelliJ (Android Studio)\r\n• Dynamic Retry Delay\r\n• Push Over Poll\r\n• Reduce Size\r\n• Cache\r\n• Avoid Graphics and Animations\r\n20', '<1-hop>\r\n\r\nShuffle \r\n🔀\r\n• It is not a mystery that energy consumption depends on so many factors that \r\nit is impossible to control all of them.\r\n• If you run 30 executions for version A and another batch for version B:\r\n• External conditions that change over time will have a diﬀerent bias in \r\nthe 2 versions (e.g., room temperature changes).\r\n• If you shuﬄe, you reduce this risk.\r\n11', '<2-hop>\r\n\r\nEcoAndroid\r\n• Plugin for IntelliJ (Android Studio)\r\n• Dynamic Retry Delay\r\n• Push Over Poll\r\n• Reduce Size\r\n• Cache\r\n• Avoid Graphics and Animations\r\n20']","To reduce risk and size in software engineering, you can adopt several strategies:

1. **Model Simplification**: Simplify AI models by reducing the number of parameters and using techniques like pruning and sparsity to decrease the computational load and energy consumption.

2. **Data-Centric Approaches**: Reduce data size through sampling, dimensionality reduction, and quantization. This helps in managing the complexity and size of the software.

3. **Algorithm Selection**: Carefully select algorithms that are efficient and not necessarily the most complex or advanced, which can help in reducing the risk of overfitting and unnecessary complexity.

4. **Energy Patterns**: Implement energy-efficient design patterns, such as batching operations to reduce extraneous energy consumption and using dark UI themes for AMOLED screens to save battery life.

5. **Green AI Practices**: Adopt Green AI practices by checking if AI is necessary, using energy-efficient hardware, and optimizing for low-power solutions.

6. **Reuse and Modular Design**: Reuse existing models and components as much as possible to avoid reinventing the wheel and to reduce development time and risk.

These strategies help in managing the complexity, size, and energy consumption of software systems, thereby reducing risks associated with software engineering projects.","To reduce risk, shuffling executions helps control external factors like temperature changes. To reduce size, EcoAndroid uses features like dynamic retry delay and push over poll.",0.0,0.0,0.75,,0.0,0.980885315377629
How shuffling reduce risk and EcoAndroid reduce size?,"['• In a scientific project, the magic number is 30.\r\n7\r\n\r\n\r\nRest ⏸\r\n• It is common practice to do a pause/sleep between executions/\r\nmeasurements. \r\n• Prevent tail energy consumption from previous measurements. ? \r\n• Prevent collateral tasks of previous measurement from affecting the next \r\nmeasurement. \r\n• There is no golden rule but one minute should be enough. It can be more or \r\nless depending on your hardware or the duration of your energy test.\r\n9\r\n\r\nt\r\nP\r\nTail Energy Consumption\r\n\r\nShuffle 🔀\r\n• It is not a mystery that energy consumption depends on so many factors that \r\nit is impossible to control all of them. \r\n• If you run 30 executions for version A and another batch for version B: \r\n• External conditions that change over time will have a different bias in \r\nthe 2 versions (e.g., room temperature changes). \r\n• If you shuffle, you reduce this risk.\r\n11\r\n\r\nKeep it cool 🌡\r\n• Always make sure there is a stable room temperature. \r\n• Tricky because, some times, experiments may have to run over a few days. \r\n• If you cannot control room temperature: collect temperature data and filter \r\nout measurements where the room temperature is clearly deviating.\r\n12\r\n\r\nAutomate Executions 🤖\r\n• (Already mentioned in the previous classes) \r\n• One cannot run 30 shuffled experiments per version without automation…\r\n13\r\n\r\nData analysis\r\n14\r\n\r\n1. Exploratory Analysis\r\n• Plot the data and inspect outliers or \r\nunexpected biases. \r\n• Violin+box plots are usually handy. (?) \r\n• It’s a nice way of combining the 30 \r\nexperiments, and of showing \r\ndescriptive statistics. (?) \r\n• Shows the shape of the distribution \r\nof the data.\r\n15\r\n\r\n1. Exploratory Analysis (II)\r\n• Data should be Normal. Unless there’s a \r\ngood reason. \r\n• E.g., somewhere amongst the 30 executions, \r\nthere might be 1 or 2 that failed to finish due \r\nto some unexpected error. \r\n• (It happens and that’s ok!)– consequently, \r\nthe execution is shorter and spends less \r\nenergy – falsely appearing as more \r\nenergy efficient. \r\n• If data is not Normal there might be some \r\nissues affecting the measurements that \r\nmight be ruining results. It is important to \r\ninvestigate this.\r\n16\r\n\r\nEnergy data is not normal. Why?\r\n• It might be caused by one of the following reasons: \r\n• There was an error in some of the executions. If not detected and fixed it \r\nmight ruin results. \r\n• Your tests are not fully replicable or are not deterministic. Quite frequent \r\nwhen you have internet requests or random-based algorithms. \r\n• There is an unusual task being run by the system during some experiments. \r\n• The computer entered a different power mode. \r\n• External physical conditions have changed. E.g., someone opened a \r\nwindow.\r\n17\r\n\r\nEnergy data is not normal. How to fix?\r\n• We have 2+1 options: \r\n1. Remove outliers. If there are only a few points that deviate from the normal \r\ndistribution, it is okay to simply remove them. \r\n• Use the z-score outlier removal. (?) \r\n• Remove all data points that deviate from the mean more than 3 standard \r\ndeviations: \r\n \r\n2. Fix the issue and rerun experiments. \r\n3. Conclude that nothing can be done about it and data will never be normal. (e.g., \r\nin AI, executions are rarely deterministic). ⚠ Only after ruling out the previous \r\npoints.\r\n| ¯x −x| > 3s\r\n18\r\n\r\nHow do we know whether data is Normal?', 'Luís Cruz \r\nL.Cruz@tudelft.nl \r\n3. Scientific Guide for Reliable \r\nEnergy Experiments\r\nSustainable Software Engineering\u2028\r\nCS4575\r\nSustainableSE 2025\r\nCarolin Brandt \r\nC.E.Brandt@tudelft.nl \r\nEnrique Barba Roque \r\nE.BarbaRoque@tudelft.nl\r\n\r\n1. Scientific guide for energy measurements \r\n2. Energy consumption data analysis \r\n\r\nEnergy tests are flaky\r\n• Multiple runs might yield different results \r\n• There are many confounding factors that need to be controlled/minimized.\r\n3\r\n?\r\n\r\nZen mode 🧘\r\n• Close all applications. \r\n• Turn off notifications. \r\n• Only the required hardware should be connected (avoid USB drives, external \r\ndisks, external displays, etc.). \r\n• Kill unnecessary services running in the background (e.g., web server, file \r\nsharing, etc.). \r\n• If you do not need an internet or intranet connection, switch off your network. \r\n• Prefer cable over wireless – the energy consumption from a cable connection \r\nis more stable than from a wireless connection.\r\n4\r\n\r\nFreeze and report your settings 🥶\r\n• Fix display brightness; switch off auto brightness \r\n• If Wifi is on, it should always be on, connected to the same network/\r\nendpoint.…\r\n5\r\n\r\nWarm-up 📶\r\n• Energy consumption is highly affected by the temperature of your hardware. \r\n• Higher the temperature -> higher the resistance of electrical conductors ->\u2028\r\n-> higher dissipation -> higher energy consumption \r\n• The first execution will appear more efficient because the hardware is still \r\ncold. \r\n• Run a CPU-intensive task before measuring energy consumption. E.g., \r\nFibonacci sequence. At least 1min; 5min recommended. \r\n•\r\n6\r\n\r\nRepeat 🔁\r\n• The best way to make sure a measurement is valid is by repeating it. \r\n• In a scientific project, the magic number is 30.\r\n7\r\n\r\n\r\nRest ⏸\r\n• It is common practice to do a pause/sleep between executions/\r\nmeasurements. \r\n• Prevent tail energy consumption from previous measurements. ? \r\n• Prevent collateral tasks of previous measurement from affecting the next \r\nmeasurement. \r\n• There is no golden rule but one minute should be enough. It can be more or \r\nless depending on your hardware or the duration of your energy test.\r\n9\r\n\r\nt\r\nP\r\nTail Energy Consumption\r\n\r\nShuffle 🔀\r\n• It is not a mystery that energy consumption depends on so many factors that \r\nit is impossible to control all of them. \r\n• If you run 30 executions for version A and another batch for version B: \r\n• External conditions that change over time will have a different bias in \r\nthe 2 versions (e.g., room temperature changes). \r\n• If you shuffle, you reduce this risk.\r\n11\r\n\r\nKeep it cool 🌡\r\n• Always make sure there is a stable room temperature. \r\n• Tricky because, some times, experiments may have to run over a few days. \r\n• If you cannot control room temperature: collect temperature data and filter \r\nout measurements where the room temperature is clearly deviating.\r\n12\r\n\r\nAutomate Executions 🤖\r\n• (Already mentioned in the previous classes) \r\n• One cannot run 30 shuffled experiments per version without automation…\r\n13\r\n\r\nData analysis\r\n14\r\n\r\n1. Exploratory Analysis\r\n• Plot the data and inspect outliers or \r\nunexpected biases. \r\n• Violin+box plots are usually handy. (?) \r\n• It’s a nice way of combining the 30 \r\nexperiments, and of showing \r\ndescriptive statistics. (?) \r\n• Shows the shape of the distribution \r\nof the data.\r\n15\r\n\r\n1. Exploratory Analysis (II)\r\n• Data should be Normal.', '[…]\r\n13\r\n\r\nAvoid Extraneous Graphics and \r\nAnimations  \r\n• Context: Mobile apps that feature impressive graphics \r\nand animations. […] \r\n• Solution: Study the importance of graphics and \r\nanimations to the user experience and reduce them when \r\napplicable. […]\r\n• Example: Resort to low frame rates for animations when \r\npossible. \r\nDespite being important to improve user experience, graphics \r\nand animations are battery intensive and should be used with \r\nmoderation.  \r\n14\r\n\r\nEnergy Patterns are \r\nmore Frequent\r\nin Android Apps\r\n\r\nExample case: Nextcloud\r\nFOSS\r\n\r\nExample case: Nextcloud\r\n• Users complain that sometimes they go on a trip and  Nextcloud drains their \r\nbattery. Users consider uninstalling the app when battery life is essential. \r\n• File sync can be energy-greedy. Send large files to the server, long 3G/4G data connections. \r\n• It is mostly used for backup. No real-time collaboration is needed. \r\n• Energy requirements vary depending on context and user. Some days you really \r\nneed all the battery you can get. \r\n• https://github.com/nextcloud/android/commit/\r\n8bc432027e0d33e8043cf40192203203a40ca29c\r\nSolutions?\r\n17\r\n\r\nExample case: K-9 mail\r\n18\r\n\r\nExample case: K-9 mail\r\n19\r\nSolutions?\r\n• Some users noticed that K-9 mail was spending more energy than usual. 🙉 \r\n• A user that was having issues with a personal mail server  noticed that K-9 \r\nmail was the one of the most energy-greedy apps. IMAP IDLE protocol for real-time \r\nnotifications. \r\n• When a connection is not possible, the app automatically retries later. \r\n• https://github.com/k9mail/k-9/commit/\r\n86f3b28f79509d1a4d613eb39f60603e08579ea3\r\n\r\nEcoAndroid\r\n• Plugin for IntelliJ (Android Studio) \r\n• Dynamic Retry Delay \r\n• Push Over Poll \r\n• Reduce Size \r\n• Cache \r\n• Avoid Graphics and Animations\r\n20\r\n\r\nCarbon-Aware Computing for \r\nDatacenters \r\nhttps://sites.google.com/view/energy-efficiency-languages\r\n21\r\nAna Radovanovic ́, Ross Koningstein, Ian Schneider, Bokan \r\nChen, Alexandre Duarte, Binz Roy, Diyue Xiao, Maya \r\nHaridasan, Patrick Hung, Nick Care, Saurav Talukdar, Eric \r\nMullen, Kendal Smith, MariEllen Cottman, and Walfredo Cirne \r\n\r\n\r\n• Google’s Carbon-Intelligent Computing System (CICS) \r\n• Main idea: use carbon-intensity data to shift datacenter jobs in time \r\n• Typically, job schedulers use a metric of cluster capacity to schedule a job in \r\na particular cluster. \r\n• CICS overrides this metric with the virtual capacity curve (VCC) that \r\nfactors in Carbon intensity \r\n• When a new job comes in, the scheduler estimates its CPU load and power \r\nusage and assigns it to a cluster if the VCC is not exceeded.\r\n23\r\n\r\n• Jobs are divided between flexible and inflexible. \r\n• Flexible load is considered shapeable/shiftable as long as its total daily \r\ncompute (CPU) demand is preserved \r\n• The system needs to consider that, while running a job, the virtual capacity \r\ncurve (VCC) might drop. Hence, this job should not start in the first place. \r\n• They forecast VCC for the next day\r\n24\r\n\r\n\r\nVirtual Cluster Capacity (VCC)\r\n• Aims at reducing the peak load at carbon intensive hours but in total it should allow for the \r\nsome amount of daily computation!']","['<1-hop>\r\n\r\nShuffle \r\n🔀\r\n• It is not a mystery that energy consumption depends on so many factors that \r\nit is impossible to control all of them.\r\n• If you run 30 executions for version A and another batch for version B:\r\n• External conditions that change over time will have a diﬀerent bias in \r\nthe 2 versions (e.g., room temperature changes).\r\n• If you shuﬄe, you reduce this risk.\r\n11', '<2-hop>\r\n\r\nEcoAndroid\r\n• Plugin for IntelliJ (Android Studio)\r\n• Dynamic Retry Delay\r\n• Push Over Poll\r\n• Reduce Size\r\n• Cache\r\n• Avoid Graphics and Animations\r\n20', '<1-hop>\r\n\r\nShuffle \r\n🔀\r\n• It is not a mystery that energy consumption depends on so many factors that \r\nit is impossible to control all of them.\r\n• If you run 30 executions for version A and another batch for version B:\r\n• External conditions that change over time will have a diﬀerent bias in \r\nthe 2 versions (e.g., room temperature changes).\r\n• If you shuﬄe, you reduce this risk.\r\n11', '<2-hop>\r\n\r\nEcoAndroid\r\n• Plugin for IntelliJ (Android Studio)\r\n• Dynamic Retry Delay\r\n• Push Over Poll\r\n• Reduce Size\r\n• Cache\r\n• Avoid Graphics and Animations\r\n20', '<1-hop>\r\n\r\nShuffle \r\n🔀\r\n• It is not a mystery that energy consumption depends on so many factors that \r\nit is impossible to control all of them.\r\n• If you run 30 executions for version A and another batch for version B:\r\n• External conditions that change over time will have a diﬀerent bias in \r\nthe 2 versions (e.g., room temperature changes).\r\n• If you shuﬄe, you reduce this risk.\r\n11', '<2-hop>\r\n\r\nEcoAndroid\r\n• Plugin for IntelliJ (Android Studio)\r\n• Dynamic Retry Delay\r\n• Push Over Poll\r\n• Reduce Size\r\n• Cache\r\n• Avoid Graphics and Animations\r\n20']","Shuffling reduces risk by minimizing the bias introduced by external conditions that change over time during experiments. By running executions in a shuffled order, you ensure that any changes in conditions (e.g., room temperature) affect both versions being tested (e.g., version A and version B) more evenly, thereby reducing the risk of skewed results due to these external factors (Source 11).

EcoAndroid reduces size by implementing strategies such as dynamic retry delay, push over poll, caching, and avoiding graphics and animations. These strategies help in optimizing the app's performance and energy consumption, which indirectly contributes to reducing the app's overall resource usage and size (Source 20).",Shuffling reduce risk by controlling external factors like room temperature changes. EcoAndroid reduce size by using features like caching and avoiding graphics and animations.,0.0,1.0,0.7142857142857143,0.67,0.0,0.9895330037142868
How shuffling reduce risk when compare to EcoAndroid reduce size?,"['Luís Cruz \r\nL.Cruz@tudelft.nl \r\n3. Scientific Guide for Reliable \r\nEnergy Experiments\r\nSustainable Software Engineering\u2028\r\nCS4575\r\nSustainableSE 2025\r\nCarolin Brandt \r\nC.E.Brandt@tudelft.nl \r\nEnrique Barba Roque \r\nE.BarbaRoque@tudelft.nl\r\n\r\n1. Scientific guide for energy measurements \r\n2. Energy consumption data analysis \r\n\r\nEnergy tests are flaky\r\n• Multiple runs might yield different results \r\n• There are many confounding factors that need to be controlled/minimized.\r\n3\r\n?\r\n\r\nZen mode 🧘\r\n• Close all applications. \r\n• Turn off notifications. \r\n• Only the required hardware should be connected (avoid USB drives, external \r\ndisks, external displays, etc.). \r\n• Kill unnecessary services running in the background (e.g., web server, file \r\nsharing, etc.). \r\n• If you do not need an internet or intranet connection, switch off your network. \r\n• Prefer cable over wireless – the energy consumption from a cable connection \r\nis more stable than from a wireless connection.\r\n4\r\n\r\nFreeze and report your settings 🥶\r\n• Fix display brightness; switch off auto brightness \r\n• If Wifi is on, it should always be on, connected to the same network/\r\nendpoint.…\r\n5\r\n\r\nWarm-up 📶\r\n• Energy consumption is highly affected by the temperature of your hardware. \r\n• Higher the temperature -> higher the resistance of electrical conductors ->\u2028\r\n-> higher dissipation -> higher energy consumption \r\n• The first execution will appear more efficient because the hardware is still \r\ncold. \r\n• Run a CPU-intensive task before measuring energy consumption. E.g., \r\nFibonacci sequence. At least 1min; 5min recommended. \r\n•\r\n6\r\n\r\nRepeat 🔁\r\n• The best way to make sure a measurement is valid is by repeating it. \r\n• In a scientific project, the magic number is 30.\r\n7\r\n\r\n\r\nRest ⏸\r\n• It is common practice to do a pause/sleep between executions/\r\nmeasurements. \r\n• Prevent tail energy consumption from previous measurements. ? \r\n• Prevent collateral tasks of previous measurement from affecting the next \r\nmeasurement. \r\n• There is no golden rule but one minute should be enough. It can be more or \r\nless depending on your hardware or the duration of your energy test.\r\n9\r\n\r\nt\r\nP\r\nTail Energy Consumption\r\n\r\nShuffle 🔀\r\n• It is not a mystery that energy consumption depends on so many factors that \r\nit is impossible to control all of them. \r\n• If you run 30 executions for version A and another batch for version B: \r\n• External conditions that change over time will have a different bias in \r\nthe 2 versions (e.g., room temperature changes). \r\n• If you shuffle, you reduce this risk.\r\n11\r\n\r\nKeep it cool 🌡\r\n• Always make sure there is a stable room temperature. \r\n• Tricky because, some times, experiments may have to run over a few days. \r\n• If you cannot control room temperature: collect temperature data and filter \r\nout measurements where the room temperature is clearly deviating.\r\n12\r\n\r\nAutomate Executions 🤖\r\n• (Already mentioned in the previous classes) \r\n• One cannot run 30 shuffled experiments per version without automation…\r\n13\r\n\r\nData analysis\r\n14\r\n\r\n1. Exploratory Analysis\r\n• Plot the data and inspect outliers or \r\nunexpected biases. \r\n• Violin+box plots are usually handy. (?) \r\n• It’s a nice way of combining the 30 \r\nexperiments, and of showing \r\ndescriptive statistics. (?) \r\n• Shows the shape of the distribution \r\nof the data.\r\n15\r\n\r\n1. Exploratory Analysis (II)\r\n• Data should be Normal.', '• In a scientific project, the magic number is 30.\r\n7\r\n\r\n\r\nRest ⏸\r\n• It is common practice to do a pause/sleep between executions/\r\nmeasurements. \r\n• Prevent tail energy consumption from previous measurements. ? \r\n• Prevent collateral tasks of previous measurement from affecting the next \r\nmeasurement. \r\n• There is no golden rule but one minute should be enough. It can be more or \r\nless depending on your hardware or the duration of your energy test.\r\n9\r\n\r\nt\r\nP\r\nTail Energy Consumption\r\n\r\nShuffle 🔀\r\n• It is not a mystery that energy consumption depends on so many factors that \r\nit is impossible to control all of them. \r\n• If you run 30 executions for version A and another batch for version B: \r\n• External conditions that change over time will have a different bias in \r\nthe 2 versions (e.g., room temperature changes). \r\n• If you shuffle, you reduce this risk.\r\n11\r\n\r\nKeep it cool 🌡\r\n• Always make sure there is a stable room temperature. \r\n• Tricky because, some times, experiments may have to run over a few days. \r\n• If you cannot control room temperature: collect temperature data and filter \r\nout measurements where the room temperature is clearly deviating.\r\n12\r\n\r\nAutomate Executions 🤖\r\n• (Already mentioned in the previous classes) \r\n• One cannot run 30 shuffled experiments per version without automation…\r\n13\r\n\r\nData analysis\r\n14\r\n\r\n1. Exploratory Analysis\r\n• Plot the data and inspect outliers or \r\nunexpected biases. \r\n• Violin+box plots are usually handy. (?) \r\n• It’s a nice way of combining the 30 \r\nexperiments, and of showing \r\ndescriptive statistics. (?) \r\n• Shows the shape of the distribution \r\nof the data.\r\n15\r\n\r\n1. Exploratory Analysis (II)\r\n• Data should be Normal. Unless there’s a \r\ngood reason. \r\n• E.g., somewhere amongst the 30 executions, \r\nthere might be 1 or 2 that failed to finish due \r\nto some unexpected error. \r\n• (It happens and that’s ok!)– consequently, \r\nthe execution is shorter and spends less \r\nenergy – falsely appearing as more \r\nenergy efficient. \r\n• If data is not Normal there might be some \r\nissues affecting the measurements that \r\nmight be ruining results. It is important to \r\ninvestigate this.\r\n16\r\n\r\nEnergy data is not normal. Why?\r\n• It might be caused by one of the following reasons: \r\n• There was an error in some of the executions. If not detected and fixed it \r\nmight ruin results. \r\n• Your tests are not fully replicable or are not deterministic. Quite frequent \r\nwhen you have internet requests or random-based algorithms. \r\n• There is an unusual task being run by the system during some experiments. \r\n• The computer entered a different power mode. \r\n• External physical conditions have changed. E.g., someone opened a \r\nwindow.\r\n17\r\n\r\nEnergy data is not normal. How to fix?\r\n• We have 2+1 options: \r\n1. Remove outliers. If there are only a few points that deviate from the normal \r\ndistribution, it is okay to simply remove them. \r\n• Use the z-score outlier removal. (?) \r\n• Remove all data points that deviate from the mean more than 3 standard \r\ndeviations: \r\n \r\n2. Fix the issue and rerun experiments. \r\n3. Conclude that nothing can be done about it and data will never be normal. (e.g., \r\nin AI, executions are rarely deterministic). ⚠ Only after ruling out the previous \r\npoints.\r\n| ¯x −x| > 3s\r\n18\r\n\r\nHow do we know whether data is Normal?', '[…]\r\n13\r\n\r\nAvoid Extraneous Graphics and \r\nAnimations  \r\n• Context: Mobile apps that feature impressive graphics \r\nand animations. […] \r\n• Solution: Study the importance of graphics and \r\nanimations to the user experience and reduce them when \r\napplicable. […]\r\n• Example: Resort to low frame rates for animations when \r\npossible. \r\nDespite being important to improve user experience, graphics \r\nand animations are battery intensive and should be used with \r\nmoderation.  \r\n14\r\n\r\nEnergy Patterns are \r\nmore Frequent\r\nin Android Apps\r\n\r\nExample case: Nextcloud\r\nFOSS\r\n\r\nExample case: Nextcloud\r\n• Users complain that sometimes they go on a trip and  Nextcloud drains their \r\nbattery. Users consider uninstalling the app when battery life is essential. \r\n• File sync can be energy-greedy. Send large files to the server, long 3G/4G data connections. \r\n• It is mostly used for backup. No real-time collaboration is needed. \r\n• Energy requirements vary depending on context and user. Some days you really \r\nneed all the battery you can get. \r\n• https://github.com/nextcloud/android/commit/\r\n8bc432027e0d33e8043cf40192203203a40ca29c\r\nSolutions?\r\n17\r\n\r\nExample case: K-9 mail\r\n18\r\n\r\nExample case: K-9 mail\r\n19\r\nSolutions?\r\n• Some users noticed that K-9 mail was spending more energy than usual. 🙉 \r\n• A user that was having issues with a personal mail server  noticed that K-9 \r\nmail was the one of the most energy-greedy apps. IMAP IDLE protocol for real-time \r\nnotifications. \r\n• When a connection is not possible, the app automatically retries later. \r\n• https://github.com/k9mail/k-9/commit/\r\n86f3b28f79509d1a4d613eb39f60603e08579ea3\r\n\r\nEcoAndroid\r\n• Plugin for IntelliJ (Android Studio) \r\n• Dynamic Retry Delay \r\n• Push Over Poll \r\n• Reduce Size \r\n• Cache \r\n• Avoid Graphics and Animations\r\n20\r\n\r\nCarbon-Aware Computing for \r\nDatacenters \r\nhttps://sites.google.com/view/energy-efficiency-languages\r\n21\r\nAna Radovanovic ́, Ross Koningstein, Ian Schneider, Bokan \r\nChen, Alexandre Duarte, Binz Roy, Diyue Xiao, Maya \r\nHaridasan, Patrick Hung, Nick Care, Saurav Talukdar, Eric \r\nMullen, Kendal Smith, MariEllen Cottman, and Walfredo Cirne \r\n\r\n\r\n• Google’s Carbon-Intelligent Computing System (CICS) \r\n• Main idea: use carbon-intensity data to shift datacenter jobs in time \r\n• Typically, job schedulers use a metric of cluster capacity to schedule a job in \r\na particular cluster. \r\n• CICS overrides this metric with the virtual capacity curve (VCC) that \r\nfactors in Carbon intensity \r\n• When a new job comes in, the scheduler estimates its CPU load and power \r\nusage and assigns it to a cluster if the VCC is not exceeded.\r\n23\r\n\r\n• Jobs are divided between flexible and inflexible. \r\n• Flexible load is considered shapeable/shiftable as long as its total daily \r\ncompute (CPU) demand is preserved \r\n• The system needs to consider that, while running a job, the virtual capacity \r\ncurve (VCC) might drop. Hence, this job should not start in the first place. \r\n• They forecast VCC for the next day\r\n24\r\n\r\n\r\nVirtual Cluster Capacity (VCC)\r\n• Aims at reducing the peak load at carbon intensive hours but in total it should allow for the \r\nsome amount of daily computation!']","['<1-hop>\r\n\r\nShuffle \r\n🔀\r\n• It is not a mystery that energy consumption depends on so many factors that \r\nit is impossible to control all of them.\r\n• If you run 30 executions for version A and another batch for version B:\r\n• External conditions that change over time will have a diﬀerent bias in \r\nthe 2 versions (e.g., room temperature changes).\r\n• If you shuﬄe, you reduce this risk.\r\n11', '<2-hop>\r\n\r\nEcoAndroid\r\n• Plugin for IntelliJ (Android Studio)\r\n• Dynamic Retry Delay\r\n• Push Over Poll\r\n• Reduce Size\r\n• Cache\r\n• Avoid Graphics and Animations\r\n20']","Shuffling reduces risk in energy experiments by minimizing the impact of external conditions that change over time, such as room temperature, which could introduce bias when comparing different versions of a software. By shuffling the order of executions, you ensure that these external factors affect all versions equally, thus reducing the risk of skewed results.

On the other hand, EcoAndroid's ""reduce size"" strategy focuses on minimizing the energy consumption of mobile apps by reducing the size of data being processed or transmitted. This approach directly targets the energy efficiency of the app itself, rather than addressing external factors.

In summary, shuffling is about controlling external experimental conditions to ensure fair comparisons, while reducing size in EcoAndroid is about optimizing the app's internal operations to save energy.",Shuffling reduce risk by balance external factor like room temperature change while EcoAndroid reduce size by optimize resource use.,0.0,1.0,0.625,0.67,0.249999999375,0.9136261590381838
How shuffling reduce risk when reduce size in software engineering?,"['Luís Cruz \r\nL.Cruz@tudelft.nl \r\n3. Scientific Guide for Reliable \r\nEnergy Experiments\r\nSustainable Software Engineering\u2028\r\nCS4575\r\nSustainableSE 2025\r\nCarolin Brandt \r\nC.E.Brandt@tudelft.nl \r\nEnrique Barba Roque \r\nE.BarbaRoque@tudelft.nl\r\n\r\n1. Scientific guide for energy measurements \r\n2. Energy consumption data analysis \r\n\r\nEnergy tests are flaky\r\n• Multiple runs might yield different results \r\n• There are many confounding factors that need to be controlled/minimized.\r\n3\r\n?\r\n\r\nZen mode 🧘\r\n• Close all applications. \r\n• Turn off notifications. \r\n• Only the required hardware should be connected (avoid USB drives, external \r\ndisks, external displays, etc.). \r\n• Kill unnecessary services running in the background (e.g., web server, file \r\nsharing, etc.). \r\n• If you do not need an internet or intranet connection, switch off your network. \r\n• Prefer cable over wireless – the energy consumption from a cable connection \r\nis more stable than from a wireless connection.\r\n4\r\n\r\nFreeze and report your settings 🥶\r\n• Fix display brightness; switch off auto brightness \r\n• If Wifi is on, it should always be on, connected to the same network/\r\nendpoint.…\r\n5\r\n\r\nWarm-up 📶\r\n• Energy consumption is highly affected by the temperature of your hardware. \r\n• Higher the temperature -> higher the resistance of electrical conductors ->\u2028\r\n-> higher dissipation -> higher energy consumption \r\n• The first execution will appear more efficient because the hardware is still \r\ncold. \r\n• Run a CPU-intensive task before measuring energy consumption. E.g., \r\nFibonacci sequence. At least 1min; 5min recommended. \r\n•\r\n6\r\n\r\nRepeat 🔁\r\n• The best way to make sure a measurement is valid is by repeating it. \r\n• In a scientific project, the magic number is 30.\r\n7\r\n\r\n\r\nRest ⏸\r\n• It is common practice to do a pause/sleep between executions/\r\nmeasurements. \r\n• Prevent tail energy consumption from previous measurements. ? \r\n• Prevent collateral tasks of previous measurement from affecting the next \r\nmeasurement. \r\n• There is no golden rule but one minute should be enough. It can be more or \r\nless depending on your hardware or the duration of your energy test.\r\n9\r\n\r\nt\r\nP\r\nTail Energy Consumption\r\n\r\nShuffle 🔀\r\n• It is not a mystery that energy consumption depends on so many factors that \r\nit is impossible to control all of them. \r\n• If you run 30 executions for version A and another batch for version B: \r\n• External conditions that change over time will have a different bias in \r\nthe 2 versions (e.g., room temperature changes). \r\n• If you shuffle, you reduce this risk.\r\n11\r\n\r\nKeep it cool 🌡\r\n• Always make sure there is a stable room temperature. \r\n• Tricky because, some times, experiments may have to run over a few days. \r\n• If you cannot control room temperature: collect temperature data and filter \r\nout measurements where the room temperature is clearly deviating.\r\n12\r\n\r\nAutomate Executions 🤖\r\n• (Already mentioned in the previous classes) \r\n• One cannot run 30 shuffled experiments per version without automation…\r\n13\r\n\r\nData analysis\r\n14\r\n\r\n1. Exploratory Analysis\r\n• Plot the data and inspect outliers or \r\nunexpected biases. \r\n• Violin+box plots are usually handy. (?) \r\n• It’s a nice way of combining the 30 \r\nexperiments, and of showing \r\ndescriptive statistics. (?) \r\n• Shows the shape of the distribution \r\nof the data.\r\n15\r\n\r\n1. Exploratory Analysis (II)\r\n• Data should be Normal.', ""• Our question: Can we save energy with alternative approaches? \r\n• We studied Grid Search, Random Search and Bayesian \r\nOptimisation.\r\n27\r\nTim's MSc Thesis, 2022 \r\nAccepted at CAIN’23\r\n\r\n\r\nResults\r\n29\r\nConclusions? \r\n• Bayesian converges faster. \r\n• No clear winner between Grid and Random\r\n\r\nDeepSeekMoE\r\n30\r\nPreprint at ArXiv, 2024 https://arxiv.org/pdf/\r\n2401.06066\r\n\r\nMixture of Experts\r\n• Llama3.1 has 405B parameters, DeepSeek V3 671B \r\n• Yet DeepSeek has quicker inference times and claims less energy \r\nconsumption (?) \r\n• Divide the model into smaller blocks of experts \r\n• Tokens get routed to certain experts based on the query \r\n• Only part of the network is active during inference \r\n• DeepSeek claims only 37B out of 671B parameters get active\r\n31\r\n\r\nDeepSeekMoE\r\n32\r\n\r\nDeepSeekMoE\r\n33\r\n• Comparable performance to \r\nLLaMA2 7B effectively using \r\nless half the parameters \r\n• Less computational power \r\n• Problems (?) \r\n• Still need to load all the \r\nparameters \r\n• High memory -> high \r\nembodied carbon\r\n\r\nGreen AI at FacebookMeta\r\n34\r\nSustainable AI: Environmental Implications, \r\nChallenges and Opportunities (2022)\r\n\r\nCarbon footprint mapped to the AI lifecycle\r\n• There are 4 main overarching stages where carbon emissions need to be isolated: data \r\ncollection, experimentation, training, inference. \r\n• At Facebook, recommendation systems split energy consumption evenly between \r\ntraining and inference; text translation models have a 35%/65% split. (Operational cost) \r\n• Operational/embodied cost split: 30%/70%\r\n35\r\n\r\nOpen issues according to Meta\r\n• A vast portion of projects only use GPUs at 30%.\u2028\r\nShould be higher to attenuate embodied carbon.\r\n36\r\nBased on 10K AI projects\r\n\r\nData Jan\r\nInformed Adaptation\r\nData Feb\r\nData Mar\r\nData Apr\r\nData May\r\n…\r\n~Data Dec\r\n~Data Dec\r\nData Jan\r\nData Feb\r\nData Mar\r\nData Apr\r\nData May\r\n…\r\nModel update Model update Model update\r\nCheck if \r\ndata change\r\nCheck if \r\ndata change\r\nCheck if \r\ndata change\r\nModel update\r\nBlind Adaptation\r\nThe AI Model will be \r\nupdated fewer times and \r\nonly when necessary.\r\nKnow when to retrain models\r\nNeither too early nor too late\r\n37\r\nAdaptation Techniques\r\n?\r\n\r\nGreen Architectural Tactics for ML-Enabled \r\nSystem\r\n38\r\nICSE-SEIS 2024\r\n\r\nArchitectural tactics\r\n39\r\n\r\nData-centric\r\n40\r\n• Reduce data size \r\n• Sampling \r\n• Dimensionality reduction \r\n• Quantization\r\n\r\nAlgorithm Design\r\n41\r\n• Carefully select your algorithm \r\n• You don’t need the fanciest \r\ntechniques\r\n\r\nModel Optimization\r\n42\r\n• Add energy to training parameters \r\n• Reduce FLOPs \r\n• Pruning, sparsity \r\n•  Take advantage of existing models\r\n\r\nModel Training\r\n43\r\n• Quantization\r\n• SAVE TRAINING PROGRESS\r\n\r\nModel Deployment\r\n44\r\n• Distributed deployment \r\n• Energy efficient hardware and \r\nconfigurations\r\n\r\nModel Management\r\n45\r\n• Reuse the model as much as possible \r\n\r\nRethinking the Architecture: Spiking Neural Networks\r\n46\r\n\r\nRethinking the Architecture: Spiking Neural Networks\r\n47\r\n\r\nSELF Lab\r\n48\r\n\r\nrecap"", '[…]\r\n13\r\n\r\nAvoid Extraneous Graphics and \r\nAnimations  \r\n• Context: Mobile apps that feature impressive graphics \r\nand animations. […] \r\n• Solution: Study the importance of graphics and \r\nanimations to the user experience and reduce them when \r\napplicable. […]\r\n• Example: Resort to low frame rates for animations when \r\npossible. \r\nDespite being important to improve user experience, graphics \r\nand animations are battery intensive and should be used with \r\nmoderation.  \r\n14\r\n\r\nEnergy Patterns are \r\nmore Frequent\r\nin Android Apps\r\n\r\nExample case: Nextcloud\r\nFOSS\r\n\r\nExample case: Nextcloud\r\n• Users complain that sometimes they go on a trip and  Nextcloud drains their \r\nbattery. Users consider uninstalling the app when battery life is essential. \r\n• File sync can be energy-greedy. Send large files to the server, long 3G/4G data connections. \r\n• It is mostly used for backup. No real-time collaboration is needed. \r\n• Energy requirements vary depending on context and user. Some days you really \r\nneed all the battery you can get. \r\n• https://github.com/nextcloud/android/commit/\r\n8bc432027e0d33e8043cf40192203203a40ca29c\r\nSolutions?\r\n17\r\n\r\nExample case: K-9 mail\r\n18\r\n\r\nExample case: K-9 mail\r\n19\r\nSolutions?\r\n• Some users noticed that K-9 mail was spending more energy than usual. 🙉 \r\n• A user that was having issues with a personal mail server  noticed that K-9 \r\nmail was the one of the most energy-greedy apps. IMAP IDLE protocol for real-time \r\nnotifications. \r\n• When a connection is not possible, the app automatically retries later. \r\n• https://github.com/k9mail/k-9/commit/\r\n86f3b28f79509d1a4d613eb39f60603e08579ea3\r\n\r\nEcoAndroid\r\n• Plugin for IntelliJ (Android Studio) \r\n• Dynamic Retry Delay \r\n• Push Over Poll \r\n• Reduce Size \r\n• Cache \r\n• Avoid Graphics and Animations\r\n20\r\n\r\nCarbon-Aware Computing for \r\nDatacenters \r\nhttps://sites.google.com/view/energy-efficiency-languages\r\n21\r\nAna Radovanovic ́, Ross Koningstein, Ian Schneider, Bokan \r\nChen, Alexandre Duarte, Binz Roy, Diyue Xiao, Maya \r\nHaridasan, Patrick Hung, Nick Care, Saurav Talukdar, Eric \r\nMullen, Kendal Smith, MariEllen Cottman, and Walfredo Cirne \r\n\r\n\r\n• Google’s Carbon-Intelligent Computing System (CICS) \r\n• Main idea: use carbon-intensity data to shift datacenter jobs in time \r\n• Typically, job schedulers use a metric of cluster capacity to schedule a job in \r\na particular cluster. \r\n• CICS overrides this metric with the virtual capacity curve (VCC) that \r\nfactors in Carbon intensity \r\n• When a new job comes in, the scheduler estimates its CPU load and power \r\nusage and assigns it to a cluster if the VCC is not exceeded.\r\n23\r\n\r\n• Jobs are divided between flexible and inflexible. \r\n• Flexible load is considered shapeable/shiftable as long as its total daily \r\ncompute (CPU) demand is preserved \r\n• The system needs to consider that, while running a job, the virtual capacity \r\ncurve (VCC) might drop. Hence, this job should not start in the first place. \r\n• They forecast VCC for the next day\r\n24\r\n\r\n\r\nVirtual Cluster Capacity (VCC)\r\n• Aims at reducing the peak load at carbon intensive hours but in total it should allow for the \r\nsome amount of daily computation!']","['<1-hop>\r\n\r\nShuffle \r\n🔀\r\n• It is not a mystery that energy consumption depends on so many factors that \r\nit is impossible to control all of them.\r\n• If you run 30 executions for version A and another batch for version B:\r\n• External conditions that change over time will have a diﬀerent bias in \r\nthe 2 versions (e.g., room temperature changes).\r\n• If you shuﬄe, you reduce this risk.\r\n11', '<2-hop>\r\n\r\nEcoAndroid\r\n• Plugin for IntelliJ (Android Studio)\r\n• Dynamic Retry Delay\r\n• Push Over Poll\r\n• Reduce Size\r\n• Cache\r\n• Avoid Graphics and Animations\r\n20', '<1-hop>\r\n\r\nShuffle \r\n🔀\r\n• It is not a mystery that energy consumption depends on so many factors that \r\nit is impossible to control all of them.\r\n• If you run 30 executions for version A and another batch for version B:\r\n• External conditions that change over time will have a diﬀerent bias in \r\nthe 2 versions (e.g., room temperature changes).\r\n• If you shuﬄe, you reduce this risk.\r\n11', '<2-hop>\r\n\r\nEcoAndroid\r\n• Plugin for IntelliJ (Android Studio)\r\n• Dynamic Retry Delay\r\n• Push Over Poll\r\n• Reduce Size\r\n• Cache\r\n• Avoid Graphics and Animations\r\n20']","The concept of shuffling in the context of energy measurements in software engineering is about reducing the risk of bias due to external conditions that change over time. When you run multiple executions of different versions of software, shuffling the order of these executions helps ensure that any external factors, such as room temperature changes, affect all versions equally. This reduces the risk of one version being unfairly advantaged or disadvantaged by these external conditions.

However, the concept of ""reduce size"" in software engineering typically refers to reducing the data size or the size of the software itself, which is not directly related to shuffling. Reducing size can involve techniques like data sampling, dimensionality reduction, or code optimization to make the software more efficient and less resource-intensive.

In summary, shuffling helps mitigate bias in experimental conditions, while reducing size is a separate optimization strategy aimed at improving software efficiency. They are related in the broader context of optimizing software performance and energy consumption but address different aspects of the process.","Shuffling reduce risk by controlling external factors like room temperature changes, which help in reducing size in software engineering.",0.0,0.0,1.0,0.31,,0.9584482104163689
How spiking neural networks relate to McCulloch-Pitts model?,"['Different encoding strategies with spiking neurons\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\nQuestions?\r\n\r\nNeuromorphic computing\r\nWhat is the advantage for applications?\r\n-\r\nLess spikes = less energy consumption in specialized neuromorphic hardware\r\n(e.g. Intel Loihi [12])\r\nBrains are energy efficient: \r\n2. Sparse encoding\r\n\r\nMultiply-accumulate (MAC) operations:\r\nNormal neuron: Multiplies input with \r\nweights, then adds.\r\nSpiking neuron: Consider binary input (e.g. input currents are piecewise \r\nconstant and assume values {0,1}). There is no multiplication, only addition.\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\r\nNormal neuron: nin x nout multiplications,\r\n(nin– 1) x nout additions\r\nSpiking neuron: 0 multiplications, (nactive– 1) x nout \r\nadditions, with nactive ≤ nin\r\nEnergy consumption:\r\nEnormal = M nin nout + A (nin– 1) nout = 17.5 pJ\r\nEnergy consumption:\r\nEspiking = A (nactive– 1) nout = 2.7 pJ\r\nMultiply-accumulate (MAC) operations:\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\r\nNormal neuron: nin x nout multiplications,\r\n(nin– 1) x nout additions\r\nSpiking neuron: 0 multiplications, (nactive– 1) x nout \r\nadditions, with nactive ≤ nin\r\nEnergy consumption:\r\nEnormal = M nin nout + A (nin– 1) nout = 17.5 pJ\r\nEnergy consumption:\r\nEspiking = A (nactive– 1) nout = 2.7 pJ\r\nMultiply-accumulate (MAC) operations:\r\nSNN challenge: how to \r\ncompute with the least \r\namount of spikes!\r\n\r\nOften, we observe a \r\nsparsity (energy)-\r\ntask accuracy trade-off\r\n(Left: results for image \r\nclassification)\r\nSNN challenge: how to \r\ncompute with the least \r\namount of spikes!\r\nhttps://arxiv.org/pdf/2409.08290\r\n\r\nIn practice, energy \r\nconsumption computations \r\nare complex. \r\nNeed to take into account\r\n- memory access,\r\n- addressing,\r\n- auxiliary operations,\r\nin addition to MACs.\r\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \r\nand video classification respectively). FNN’s are conventional feed-forward neural networks.\r\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.\r\nhttps://arxiv.org/pdf/2210.13107\r\nComputing energy consumption\r\n\r\nIn practice, energy \r\nconsumption computations \r\nare complex. \r\nNeed to take into account\r\n- memory access,\r\n- addressing,\r\n- auxiliary operations,\r\nin addition to MACs.\r\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \r\nand video classification respectively). FNN’s are conventional feed-forward neural networks.\r\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.', ""- Higher firing rate (spikes per unit time) for \r\n'selected' feature.\r\nAdapted from Hubel & Wiesel, 1959\r\n\r\nFiring rates\r\nClassical view of the brain:\r\n- Each neuron is selective for one specific \r\nfeature in the input.\r\n- Higher firing rate (spikes per unit time) for \r\n'selected' feature.\r\n- Link to modern ANNs: The scalar output of \r\nan artificial neuron is interpreted as the firing \r\nrate.\r\nAdapted from Hubel & Wiesel, 1959\r\n\r\nFiring rates\r\nClassical view of the brain:\r\n- Each neuron is selective for one specific \r\nfeature in the input.\r\n- Higher firing rate (spikes per unit time) for \r\n'selected' feature.\r\n- Link to modern ANNs: The scalar output of \r\nan artificial neuron is interpreted as the firing \r\nrate.\r\nAdapted from Hubel & Wiesel, 1959\r\nBut rate coding is inefficient and slow…\r\n(i.e. each neuron needs to fire many spikes to get good precision)\r\nboth in vivo and in silico.\r\n\r\nDifferent encoding strategies with spiking neurons [8]\r\n7 neurons\r\n7 time bins\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nn\r\n6\r\n(in the general \r\ncase a \r\nnumber \r\nbetween 0-7)\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nn\r\nTotal spike \r\ncount can \r\nencode 3 bits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nIndividual rates \r\ncan encode 7 \r\nbits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nLatency can \r\nencode ~3*7 or \r\n~19 bits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nRank order can \r\nencode ~12 bits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\nQuestions?\r\n\r\nNeuromorphic computing\r\nWhat is the advantage for applications?\r\n-\r\nLess spikes = less energy consumption in specialized neuromorphic hardware\r\n(e.g. Intel Loihi [12])\r\nBrains are energy efficient: \r\n2. Sparse encoding\r\n\r\nMultiply-accumulate (MAC) operations:\r\nNormal neuron: Multiplies input with \r\nweights, then adds.\r\nSpiking neuron: Consider binary input (e.g. input currents are piecewise \r\nconstant and assume values {0,1}). There is no multiplication, only addition.\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\r\nNormal neuron: nin x nout multiplications,\r\n(nin– 1) x nout additions\r\nSpiking neuron: 0 multiplications, (nactive– 1) x nout \r\nadditions, with nactive ≤ nin\r\nEnergy consumption:\r\nEnormal = M nin nout + A (nin– 1) nout = 17.5 pJ\r\nEnergy consumption:\r\nEspiking = A (nactive– 1) nout = 2.7 pJ\r\nMultiply-accumulate (MAC) operations:\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ)."", 'Normal neuron: nin x nout multiplications,\r\n(nin– 1) x nout additions\r\nSpiking neuron: 0 multiplications, (nactive– 1) x nout \r\nadditions, with nactive ≤ nin\r\nEnergy consumption:\r\nEnormal = M nin nout + A (nin– 1) nout = 17.5 pJ\r\nEnergy consumption:\r\nEspiking = A (nactive– 1) nout = 2.7 pJ\r\nMultiply-accumulate (MAC) operations:\r\nSNN challenge: how to \r\ncompute with the least \r\namount of spikes!\r\n\r\nOften, we observe a \r\nsparsity (energy)-\r\ntask accuracy trade-off\r\n(Left: results for image \r\nclassification)\r\nSNN challenge: how to \r\ncompute with the least \r\namount of spikes!\r\nhttps://arxiv.org/pdf/2409.08290\r\n\r\nIn practice, energy \r\nconsumption computations \r\nare complex. \r\nNeed to take into account\r\n- memory access,\r\n- addressing,\r\n- auxiliary operations,\r\nin addition to MACs.\r\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \r\nand video classification respectively). FNN’s are conventional feed-forward neural networks.\r\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.\r\nhttps://arxiv.org/pdf/2210.13107\r\nComputing energy consumption\r\n\r\nIn practice, energy \r\nconsumption computations \r\nare complex. \r\nNeed to take into account\r\n- memory access,\r\n- addressing,\r\n- auxiliary operations,\r\nin addition to MACs.\r\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \r\nand video classification respectively). FNN’s are conventional feed-forward neural networks.\r\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.\r\nComputing energy consumption\r\nQuestions?\r\n\r\nReading materials\r\nMain reading:\r\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \r\nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\r\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\r\n- Neuromorphic computing:\r\n- Based on biology: Zenke F, Bohté SM, Clopath C, Comşa IM, Göltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \r\nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\r\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \r\noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\r\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019.']","[""<1-hop>\r\n\r\nEncoding strategies\r\nWe considered single neurons with Boolean output (‘spike’=1 or ‘no spike’=0).\r\nHow should we encode information about 'features' in a large network with many spikes?\r\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\r\nElephant?\r\nhttps://www.nationalgeographic.com/anim\r\nals/mammals/facts/african-elephant"", '<2-hop>\r\n\r\nReferences\r\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \r\nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\r\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \r\n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\r\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch–Pitts neural networks. Journal of the History of the Behavioral \r\nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\r\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \r\n(not publicly available) doi:10.1037/h0042519\r\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \r\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\r\n6) https://github.com/idsc-frazzoli/retina\r\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \r\nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\r\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \r\nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\r\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \r\nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\r\n10) Gütig R, Sompolinsky H. The tempotron: a neuron that learns spike timing–based decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \r\nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\r\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \r\nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\r\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\r\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\r\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \r\n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568', ""<1-hop>\r\n\r\nEncoding strategies\r\nWe considered single neurons with Boolean output (‘spike’=1 or ‘no spike’=0).\r\nHow should we encode information about 'features' in a large network with many spikes?\r\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\r\nElephant?\r\nhttps://www.nationalgeographic.com/anim\r\nals/mammals/facts/african-elephant"", '<2-hop>\r\n\r\nReferences\r\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \r\nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\r\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \r\n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\r\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch–Pitts neural networks. Journal of the History of the Behavioral \r\nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\r\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \r\n(not publicly available) doi:10.1037/h0042519\r\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \r\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\r\n6) https://github.com/idsc-frazzoli/retina\r\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \r\nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\r\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \r\nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\r\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \r\nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\r\n10) Gütig R, Sompolinsky H. The tempotron: a neuron that learns spike timing–based decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \r\nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\r\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \r\nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\r\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\r\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\r\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \r\n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568', ""<1-hop>\r\n\r\nEncoding strategies\r\nWe considered single neurons with Boolean output (‘spike’=1 or ‘no spike’=0).\r\nHow should we encode information about 'features' in a large network with many spikes?\r\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\r\nElephant?\r\nhttps://www.nationalgeographic.com/anim\r\nals/mammals/facts/african-elephant"", '<2-hop>\r\n\r\nReferences\r\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \r\nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\r\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \r\n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\r\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch–Pitts neural networks. Journal of the History of the Behavioral \r\nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\r\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \r\n(not publicly available) doi:10.1037/h0042519\r\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \r\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\r\n6) https://github.com/idsc-frazzoli/retina\r\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \r\nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\r\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \r\nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\r\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \r\nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\r\n10) Gütig R, Sompolinsky H. The tempotron: a neuron that learns spike timing–based decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \r\nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\r\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \r\nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\r\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\r\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\r\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \r\n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568']","The McCulloch-Pitts model is one of the earliest models of artificial neurons, which uses binary inputs and outputs to simulate simple neural processing. Spiking neural networks (SNNs), on the other hand, are a more advanced model that incorporates the concept of time and spikes, mimicking the way biological neurons communicate through discrete events (spikes) over time.

While the McCulloch-Pitts model operates on binary signals and is primarily concerned with logical operations, SNNs use spikes to encode information in a more biologically realistic manner. SNNs do not rely on continuous activation levels like traditional artificial neural networks but instead use the timing of spikes to convey information, which can lead to more efficient and sparse computations.

In summary, SNNs can be seen as an evolution of the McCulloch-Pitts model, incorporating temporal dynamics and spike-based communication to better emulate the behavior of biological neural networks.","Spiking neural networks relate to the McCulloch-Pitts model by building upon the foundational concepts of neural computation, where the McCulloch-Pitts model introduced the idea of binary neuron states, which is extended in spiking networks to model more complex information processing through spike timing and patterns.",0.0,0.0,0.6923076923076923,1.0,0.0,0.9353285959508354
How spiking neural networks used in encoding strategies?,"[""- Higher firing rate (spikes per unit time) for \r\n'selected' feature.\r\nAdapted from Hubel & Wiesel, 1959\r\n\r\nFiring rates\r\nClassical view of the brain:\r\n- Each neuron is selective for one specific \r\nfeature in the input.\r\n- Higher firing rate (spikes per unit time) for \r\n'selected' feature.\r\n- Link to modern ANNs: The scalar output of \r\nan artificial neuron is interpreted as the firing \r\nrate.\r\nAdapted from Hubel & Wiesel, 1959\r\n\r\nFiring rates\r\nClassical view of the brain:\r\n- Each neuron is selective for one specific \r\nfeature in the input.\r\n- Higher firing rate (spikes per unit time) for \r\n'selected' feature.\r\n- Link to modern ANNs: The scalar output of \r\nan artificial neuron is interpreted as the firing \r\nrate.\r\nAdapted from Hubel & Wiesel, 1959\r\nBut rate coding is inefficient and slow…\r\n(i.e. each neuron needs to fire many spikes to get good precision)\r\nboth in vivo and in silico.\r\n\r\nDifferent encoding strategies with spiking neurons [8]\r\n7 neurons\r\n7 time bins\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nn\r\n6\r\n(in the general \r\ncase a \r\nnumber \r\nbetween 0-7)\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nn\r\nTotal spike \r\ncount can \r\nencode 3 bits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nIndividual rates \r\ncan encode 7 \r\nbits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nLatency can \r\nencode ~3*7 or \r\n~19 bits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nRank order can \r\nencode ~12 bits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\nQuestions?\r\n\r\nNeuromorphic computing\r\nWhat is the advantage for applications?\r\n-\r\nLess spikes = less energy consumption in specialized neuromorphic hardware\r\n(e.g. Intel Loihi [12])\r\nBrains are energy efficient: \r\n2. Sparse encoding\r\n\r\nMultiply-accumulate (MAC) operations:\r\nNormal neuron: Multiplies input with \r\nweights, then adds.\r\nSpiking neuron: Consider binary input (e.g. input currents are piecewise \r\nconstant and assume values {0,1}). There is no multiplication, only addition.\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\r\nNormal neuron: nin x nout multiplications,\r\n(nin– 1) x nout additions\r\nSpiking neuron: 0 multiplications, (nactive– 1) x nout \r\nadditions, with nactive ≤ nin\r\nEnergy consumption:\r\nEnormal = M nin nout + A (nin– 1) nout = 17.5 pJ\r\nEnergy consumption:\r\nEspiking = A (nactive– 1) nout = 2.7 pJ\r\nMultiply-accumulate (MAC) operations:\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ)."", ""Spiking neural network (SNN): The 'input current' ε(t) is integrated over time.\r\nspike\r\n\r\nHow to model spiking neurons?\r\nEquivalence to perceptron: Computation at least as complex as a perceptron.\r\n\r\nNon-leaky integrate-and-fire (IF) neuron\r\nThe temporal profile of the input current ε(t) can be chosen differently, for different computations.\r\n\r\nQuestions?\r\n\r\nComputation with spiking neurons\r\nCoincidence detection\r\n\r\nComputation with spiking neurons\r\nCoincidence detection\r\nx, y ∈ {0, 1}n\r\nExample: n = 2\r\ninput = \r\nx1\r\nx2\r\ny1\r\ny2\r\n= \r\n0\r\n1\r\n1\r\n1\r\noutput? \r\n\r\nCoincidence detection\r\nComputation with spiking neurons\r\n\r\nCoincidence detection\r\nCan be trivially computed with a single spiking neuron! Requires at least n/log(n+1) hidden \r\nunits for a perceptron (proof in [7]).\r\nComputation with spiking neurons\r\n\r\nCoincidence detection\r\nCan be trivially computed with a single spiking neuron! Requires at least n/log(n+1) hidden \r\nunits for a perceptron (proof in [7]).\r\nBrains are energy efficient: \r\n1.\r\nHigh temporal resolution (more computation with less neurons)\r\nComputation with spiking neurons\r\n\r\nEncoding strategies\r\nWe considered single neurons with Boolean output (‘spike’=1 or ‘no spike’=0).\r\nHow should we encode information about 'features' in a large network with many spikes?\r\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\r\nElephant?\r\nhttps://www.nationalgeographic.com/anim\r\nals/mammals/facts/african-elephant\r\n\r\nFiring rates\r\nClassical view of the brain:\r\n- Each neuron is selective for one specific \r\nfeature in the input.\r\n- Higher firing rate (spikes per unit time) for \r\n'selected' feature.\r\nAdapted from Hubel & Wiesel, 1959\r\n\r\nFiring rates\r\nClassical view of the brain:\r\n- Each neuron is selective for one specific \r\nfeature in the input.\r\n- Higher firing rate (spikes per unit time) for \r\n'selected' feature.\r\n- Link to modern ANNs: The scalar output of \r\nan artificial neuron is interpreted as the firing \r\nrate.\r\nAdapted from Hubel & Wiesel, 1959\r\n\r\nFiring rates\r\nClassical view of the brain:\r\n- Each neuron is selective for one specific \r\nfeature in the input.\r\n- Higher firing rate (spikes per unit time) for \r\n'selected' feature.\r\n- Link to modern ANNs: The scalar output of \r\nan artificial neuron is interpreted as the firing \r\nrate.\r\nAdapted from Hubel & Wiesel, 1959\r\nBut rate coding is inefficient and slow…\r\n(i.e. each neuron needs to fire many spikes to get good precision)\r\nboth in vivo and in silico.\r\n\r\nDifferent encoding strategies with spiking neurons [8]\r\n7 neurons\r\n7 time bins\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nn\r\n6\r\n(in the general \r\ncase a \r\nnumber \r\nbetween 0-7)\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nn\r\nTotal spike \r\ncount can \r\nencode 3 bits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nIndividual rates \r\ncan encode 7 \r\nbits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nLatency can \r\nencode ~3*7 or \r\n~19 bits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nRank order can \r\nencode ~12 bits."", 'Different encoding strategies with spiking neurons\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\nQuestions?\r\n\r\nNeuromorphic computing\r\nWhat is the advantage for applications?\r\n-\r\nLess spikes = less energy consumption in specialized neuromorphic hardware\r\n(e.g. Intel Loihi [12])\r\nBrains are energy efficient: \r\n2. Sparse encoding\r\n\r\nMultiply-accumulate (MAC) operations:\r\nNormal neuron: Multiplies input with \r\nweights, then adds.\r\nSpiking neuron: Consider binary input (e.g. input currents are piecewise \r\nconstant and assume values {0,1}). There is no multiplication, only addition.\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\r\nNormal neuron: nin x nout multiplications,\r\n(nin– 1) x nout additions\r\nSpiking neuron: 0 multiplications, (nactive– 1) x nout \r\nadditions, with nactive ≤ nin\r\nEnergy consumption:\r\nEnormal = M nin nout + A (nin– 1) nout = 17.5 pJ\r\nEnergy consumption:\r\nEspiking = A (nactive– 1) nout = 2.7 pJ\r\nMultiply-accumulate (MAC) operations:\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\r\nNormal neuron: nin x nout multiplications,\r\n(nin– 1) x nout additions\r\nSpiking neuron: 0 multiplications, (nactive– 1) x nout \r\nadditions, with nactive ≤ nin\r\nEnergy consumption:\r\nEnormal = M nin nout + A (nin– 1) nout = 17.5 pJ\r\nEnergy consumption:\r\nEspiking = A (nactive– 1) nout = 2.7 pJ\r\nMultiply-accumulate (MAC) operations:\r\nSNN challenge: how to \r\ncompute with the least \r\namount of spikes!\r\n\r\nOften, we observe a \r\nsparsity (energy)-\r\ntask accuracy trade-off\r\n(Left: results for image \r\nclassification)\r\nSNN challenge: how to \r\ncompute with the least \r\namount of spikes!\r\nhttps://arxiv.org/pdf/2409.08290\r\n\r\nIn practice, energy \r\nconsumption computations \r\nare complex. \r\nNeed to take into account\r\n- memory access,\r\n- addressing,\r\n- auxiliary operations,\r\nin addition to MACs.\r\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \r\nand video classification respectively). FNN’s are conventional feed-forward neural networks.\r\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.\r\nhttps://arxiv.org/pdf/2210.13107\r\nComputing energy consumption\r\n\r\nIn practice, energy \r\nconsumption computations \r\nare complex. \r\nNeed to take into account\r\n- memory access,\r\n- addressing,\r\n- auxiliary operations,\r\nin addition to MACs.\r\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \r\nand video classification respectively). FNN’s are conventional feed-forward neural networks.\r\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.']","[""<1-hop>\r\n\r\nEncoding strategies\r\nWe considered single neurons with Boolean output (‘spike’=1 or ‘no spike’=0).\r\nHow should we encode information about 'features' in a large network with many spikes?\r\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\r\nElephant?\r\nhttps://www.nationalgeographic.com/anim\r\nals/mammals/facts/african-elephant"", '<2-hop>\r\n\r\nReferences\r\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \r\nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\r\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \r\n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\r\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch–Pitts neural networks. Journal of the History of the Behavioral \r\nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\r\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \r\n(not publicly available) doi:10.1037/h0042519\r\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \r\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\r\n6) https://github.com/idsc-frazzoli/retina\r\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \r\nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\r\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \r\nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\r\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \r\nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\r\n10) Gütig R, Sompolinsky H. The tempotron: a neuron that learns spike timing–based decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \r\nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\r\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \r\nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\r\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\r\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\r\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \r\n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568']","Spiking neural networks (SNNs) use various encoding strategies to efficiently process information. These strategies include:

1. **Rate Coding**: This classical view involves neurons firing at higher rates for selected features. However, it is considered inefficient and slow because it requires many spikes to achieve good precision.

2. **Latency Coding**: This method encodes information based on the timing of spikes. It is more efficient, as latency can encode approximately 19 bits, making it faster and more spike-efficient.

3. **Rank Order Coding**: This strategy uses the order in which neurons fire to encode information, capable of encoding about 12 bits.

4. **Total Spike Count**: This method encodes information based on the total number of spikes, which can encode around 3 bits.

5. **Spatio-Temporal Coding**: By using both spatial and temporal dimensions, this approach allows for faster and more efficient encoding, reducing the number of spikes needed.

These encoding strategies make SNNs more energy-efficient and suitable for applications in neuromorphic computing, where less energy consumption is crucial.",Spiking neural networks use single neurons with Boolean output (‘spike’=1 or ‘no spike’=0) to encode information about 'features' in a large network with many spikes. This approach is supported by foundational works such as McCulloch and Pitts' logical calculus of nervous activity and later developments in spiking neuron models.,0.49999999995,0.5,1.0,0.0,0.0,0.9418523118196584
What are the benefits of neuromorphic computing in the context of green software practices?,"['Different encoding strategies with spiking neurons\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\nQuestions?\r\n\r\nNeuromorphic computing\r\nWhat is the advantage for applications?\r\n-\r\nLess spikes = less energy consumption in specialized neuromorphic hardware\r\n(e.g. Intel Loihi [12])\r\nBrains are energy efficient: \r\n2. Sparse encoding\r\n\r\nMultiply-accumulate (MAC) operations:\r\nNormal neuron: Multiplies input with \r\nweights, then adds.\r\nSpiking neuron: Consider binary input (e.g. input currents are piecewise \r\nconstant and assume values {0,1}). There is no multiplication, only addition.\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\r\nNormal neuron: nin x nout multiplications,\r\n(nin– 1) x nout additions\r\nSpiking neuron: 0 multiplications, (nactive– 1) x nout \r\nadditions, with nactive ≤ nin\r\nEnergy consumption:\r\nEnormal = M nin nout + A (nin– 1) nout = 17.5 pJ\r\nEnergy consumption:\r\nEspiking = A (nactive– 1) nout = 2.7 pJ\r\nMultiply-accumulate (MAC) operations:\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\r\nNormal neuron: nin x nout multiplications,\r\n(nin– 1) x nout additions\r\nSpiking neuron: 0 multiplications, (nactive– 1) x nout \r\nadditions, with nactive ≤ nin\r\nEnergy consumption:\r\nEnormal = M nin nout + A (nin– 1) nout = 17.5 pJ\r\nEnergy consumption:\r\nEspiking = A (nactive– 1) nout = 2.7 pJ\r\nMultiply-accumulate (MAC) operations:\r\nSNN challenge: how to \r\ncompute with the least \r\namount of spikes!\r\n\r\nOften, we observe a \r\nsparsity (energy)-\r\ntask accuracy trade-off\r\n(Left: results for image \r\nclassification)\r\nSNN challenge: how to \r\ncompute with the least \r\namount of spikes!\r\nhttps://arxiv.org/pdf/2409.08290\r\n\r\nIn practice, energy \r\nconsumption computations \r\nare complex. \r\nNeed to take into account\r\n- memory access,\r\n- addressing,\r\n- auxiliary operations,\r\nin addition to MACs.\r\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \r\nand video classification respectively). FNN’s are conventional feed-forward neural networks.\r\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.\r\nhttps://arxiv.org/pdf/2210.13107\r\nComputing energy consumption\r\n\r\nIn practice, energy \r\nconsumption computations \r\nare complex. \r\nNeed to take into account\r\n- memory access,\r\n- addressing,\r\n- auxiliary operations,\r\nin addition to MACs.\r\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \r\nand video classification respectively). FNN’s are conventional feed-forward neural networks.\r\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.', ""- Higher firing rate (spikes per unit time) for \r\n'selected' feature.\r\nAdapted from Hubel & Wiesel, 1959\r\n\r\nFiring rates\r\nClassical view of the brain:\r\n- Each neuron is selective for one specific \r\nfeature in the input.\r\n- Higher firing rate (spikes per unit time) for \r\n'selected' feature.\r\n- Link to modern ANNs: The scalar output of \r\nan artificial neuron is interpreted as the firing \r\nrate.\r\nAdapted from Hubel & Wiesel, 1959\r\n\r\nFiring rates\r\nClassical view of the brain:\r\n- Each neuron is selective for one specific \r\nfeature in the input.\r\n- Higher firing rate (spikes per unit time) for \r\n'selected' feature.\r\n- Link to modern ANNs: The scalar output of \r\nan artificial neuron is interpreted as the firing \r\nrate.\r\nAdapted from Hubel & Wiesel, 1959\r\nBut rate coding is inefficient and slow…\r\n(i.e. each neuron needs to fire many spikes to get good precision)\r\nboth in vivo and in silico.\r\n\r\nDifferent encoding strategies with spiking neurons [8]\r\n7 neurons\r\n7 time bins\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nn\r\n6\r\n(in the general \r\ncase a \r\nnumber \r\nbetween 0-7)\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nn\r\nTotal spike \r\ncount can \r\nencode 3 bits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nIndividual rates \r\ncan encode 7 \r\nbits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nLatency can \r\nencode ~3*7 or \r\n~19 bits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nRank order can \r\nencode ~12 bits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\nQuestions?\r\n\r\nNeuromorphic computing\r\nWhat is the advantage for applications?\r\n-\r\nLess spikes = less energy consumption in specialized neuromorphic hardware\r\n(e.g. Intel Loihi [12])\r\nBrains are energy efficient: \r\n2. Sparse encoding\r\n\r\nMultiply-accumulate (MAC) operations:\r\nNormal neuron: Multiplies input with \r\nweights, then adds.\r\nSpiking neuron: Consider binary input (e.g. input currents are piecewise \r\nconstant and assume values {0,1}). There is no multiplication, only addition.\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\r\nNormal neuron: nin x nout multiplications,\r\n(nin– 1) x nout additions\r\nSpiking neuron: 0 multiplications, (nactive– 1) x nout \r\nadditions, with nactive ≤ nin\r\nEnergy consumption:\r\nEnormal = M nin nout + A (nin– 1) nout = 17.5 pJ\r\nEnergy consumption:\r\nEspiking = A (nactive– 1) nout = 2.7 pJ\r\nMultiply-accumulate (MAC) operations:\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ)."", ""• Our question: Can we save energy with alternative approaches? \r\n• We studied Grid Search, Random Search and Bayesian \r\nOptimisation.\r\n27\r\nTim's MSc Thesis, 2022 \r\nAccepted at CAIN’23\r\n\r\n\r\nResults\r\n29\r\nConclusions? \r\n• Bayesian converges faster. \r\n• No clear winner between Grid and Random\r\n\r\nDeepSeekMoE\r\n30\r\nPreprint at ArXiv, 2024 https://arxiv.org/pdf/\r\n2401.06066\r\n\r\nMixture of Experts\r\n• Llama3.1 has 405B parameters, DeepSeek V3 671B \r\n• Yet DeepSeek has quicker inference times and claims less energy \r\nconsumption (?) \r\n• Divide the model into smaller blocks of experts \r\n• Tokens get routed to certain experts based on the query \r\n• Only part of the network is active during inference \r\n• DeepSeek claims only 37B out of 671B parameters get active\r\n31\r\n\r\nDeepSeekMoE\r\n32\r\n\r\nDeepSeekMoE\r\n33\r\n• Comparable performance to \r\nLLaMA2 7B effectively using \r\nless half the parameters \r\n• Less computational power \r\n• Problems (?) \r\n• Still need to load all the \r\nparameters \r\n• High memory -> high \r\nembodied carbon\r\n\r\nGreen AI at FacebookMeta\r\n34\r\nSustainable AI: Environmental Implications, \r\nChallenges and Opportunities (2022)\r\n\r\nCarbon footprint mapped to the AI lifecycle\r\n• There are 4 main overarching stages where carbon emissions need to be isolated: data \r\ncollection, experimentation, training, inference. \r\n• At Facebook, recommendation systems split energy consumption evenly between \r\ntraining and inference; text translation models have a 35%/65% split. (Operational cost) \r\n• Operational/embodied cost split: 30%/70%\r\n35\r\n\r\nOpen issues according to Meta\r\n• A vast portion of projects only use GPUs at 30%.\u2028\r\nShould be higher to attenuate embodied carbon.\r\n36\r\nBased on 10K AI projects\r\n\r\nData Jan\r\nInformed Adaptation\r\nData Feb\r\nData Mar\r\nData Apr\r\nData May\r\n…\r\n~Data Dec\r\n~Data Dec\r\nData Jan\r\nData Feb\r\nData Mar\r\nData Apr\r\nData May\r\n…\r\nModel update Model update Model update\r\nCheck if \r\ndata change\r\nCheck if \r\ndata change\r\nCheck if \r\ndata change\r\nModel update\r\nBlind Adaptation\r\nThe AI Model will be \r\nupdated fewer times and \r\nonly when necessary.\r\nKnow when to retrain models\r\nNeither too early nor too late\r\n37\r\nAdaptation Techniques\r\n?\r\n\r\nGreen Architectural Tactics for ML-Enabled \r\nSystem\r\n38\r\nICSE-SEIS 2024\r\n\r\nArchitectural tactics\r\n39\r\n\r\nData-centric\r\n40\r\n• Reduce data size \r\n• Sampling \r\n• Dimensionality reduction \r\n• Quantization\r\n\r\nAlgorithm Design\r\n41\r\n• Carefully select your algorithm \r\n• You don’t need the fanciest \r\ntechniques\r\n\r\nModel Optimization\r\n42\r\n• Add energy to training parameters \r\n• Reduce FLOPs \r\n• Pruning, sparsity \r\n•  Take advantage of existing models\r\n\r\nModel Training\r\n43\r\n• Quantization\r\n• SAVE TRAINING PROGRESS\r\n\r\nModel Deployment\r\n44\r\n• Distributed deployment \r\n• Energy efficient hardware and \r\nconfigurations\r\n\r\nModel Management\r\n45\r\n• Reuse the model as much as possible \r\n\r\nRethinking the Architecture: Spiking Neural Networks\r\n46\r\n\r\nRethinking the Architecture: Spiking Neural Networks\r\n47\r\n\r\nSELF Lab\r\n48\r\n\r\nrecap""]","['<1-hop>\r\n\r\nNeuromorphic computing\r\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\r\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\r\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\r\n\r\nReading materials\r\nMain reading:\r\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \r\nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\r\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\r\n- Neuromorphic computing:\r\n- Based on biology: Zenke F, Bohté SM, Clopath C, Comşa IM, Göltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \r\nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\r\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \r\noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\r\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \r\nhttps://www.nature.com/articles/s41586-019-1677-2\r\nExtra reading:\r\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \r\nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\r\n- Also an important part of neuromorphic systems and vision → Event Cameras: Gallego G, Delbrück T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \r\nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \r\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\r\nBasics of \'conventional\' neural networks:\r\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\r\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.', '<1-hop>\r\n\r\nNeuromorphic computing\r\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\r\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\r\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\r\n\r\nReading materials\r\nMain reading:\r\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \r\nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\r\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\r\n- Neuromorphic computing:\r\n- Based on biology: Zenke F, Bohté SM, Clopath C, Comşa IM, Göltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \r\nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\r\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \r\noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\r\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \r\nhttps://www.nature.com/articles/s41586-019-1677-2\r\nExtra reading:\r\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \r\nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\r\n- Also an important part of neuromorphic systems and vision → Event Cameras: Gallego G, Delbrück T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \r\nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \r\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\r\nBasics of \'conventional\' neural networks:\r\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\r\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.', '<1-hop>\r\n\r\nNeuromorphic computing\r\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\r\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\r\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\r\n\r\nReading materials\r\nMain reading:\r\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \r\nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\r\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\r\n- Neuromorphic computing:\r\n- Based on biology: Zenke F, Bohté SM, Clopath C, Comşa IM, Göltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \r\nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\r\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \r\noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\r\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \r\nhttps://www.nature.com/articles/s41586-019-1677-2\r\nExtra reading:\r\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \r\nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\r\n- Also an important part of neuromorphic systems and vision → Event Cameras: Gallego G, Delbrück T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \r\nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \r\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\r\nBasics of \'conventional\' neural networks:\r\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\r\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.']","Neuromorphic computing offers several benefits in the context of green software practices:

1. **Energy Efficiency**: Neuromorphic computing, particularly using spiking neural networks (SNNs), is significantly more energy-efficient compared to traditional neural networks. SNNs are 6 to 8 times more energy-efficient than conventional feed-forward neural networks (FNNs) due to their sparse encoding and reduced number of spikes, which translates to less energy consumption in specialized neuromorphic hardware.

2. **Reduced Computational Power**: Spiking neurons perform fewer operations compared to normal neurons. They do not require multiplications, only additions, which further reduces energy consumption. For example, the energy consumption of spiking neurons is significantly lower (2.7 pJ) compared to normal neurons (17.5 pJ) for the same task.

3. **Sparsity and Efficiency**: Neuromorphic systems aim to compute with the least amount of spikes, which not only saves energy but also aligns with the goal of reducing computational overhead and improving efficiency.

These characteristics make neuromorphic computing a promising approach for developing more sustainable and environmentally friendly AI systems.","Neuromorphic computing offers several benefits in the context of green software practices. It is inspired by the structure and function of the human brain, making it highly energy-efficient compared to traditional computing methods. This efficiency is particularly relevant for sustainable software development, as it reduces the environmental impact of computing systems. Additionally, neuromorphic systems can process information in a way that mimics biological neural networks, which can lead to more efficient and sustainable computing solutions. These aspects make neuromorphic computing a promising approach for developers looking to implement green software practices.",0.99999999995,1.0,1.0,0.44,0.0,1.0000000000000009
What are the challenges in creating a dataset of government-developed OS software?,"['Energy profiling of static analysis tools\r\n• What is the energy consumption of a ""typical run"" for a few OSS projects? \r\n• Does the type of analysis matter? Are certain analysis more expensive? Does \r\nthe number of rules that are activated in a static analysis tool important for the \r\nenergy consumption? \r\n• Differences between static analysis tools [lower priority]\r\n\r\nB5. Tool supporting SusAF workshop / process\r\n• Lead engineers through process & questions \r\n• Interface to create & document the two diagrams \r\n• (!) Should be easy to start using & set up \r\n• You may also create a simplified version / \u2028\r\nyour favorite sustainability framework\r\nEnvironmental\r\nEconomic\r\nTechnical\r\nIndividual\r\nSocial\r\nImmediate\r\nEnabling\r\nStructural\r\nAirbnb\r\ngreater \r\nearnings\r\nincrease \r\nin rents\r\ngentrification\r\ngreater racial \r\ndisparities\r\nrent \r\nrooms\r\n5\r\n1\r\n3\r\n4\r\n2\r\n \r\n \r\n \r\n \r\n \r\n \r\nVery \r\nunlikely \r\nVery \r\nlikely \r\n\r\nB6. Dataset of government-developed OS software\r\n• Governments develop & use software for supporting society\u2028\r\nOpen-source development & policies are on the rise \r\n• But studying government software is difficult b/c we don’t know what is out \r\nthere\u2028\r\n→ Lack of incentive to make popular \u2028\r\n→ Language barriers \r\n• Create a comprehensive dataset, incl. data to understand state of software: \r\nbuildable?, open dev. history?, requirements documentation? \r\n• Could start with NL, but including your / other countries greatly appreciated!\r\n\r\nB7. Queue - but better for the student / TA society\r\n• Requirements analysis regarding social and individual sustainability effects \r\nof Queue \r\n• Other EIP / TUD used software also possible: e.g., Answers EWI \r\n• Non-technical project → proper process (workshops? Interviews?) & rich \r\ndescription of outcomes focus of grading\r\nhttps://eip.pages.ewi.tudelft.nl/eip-website/queue.html\r\n\r\nC1. Compare energy consumption of docker images for \r\nML workloads.\r\n• Similar to what we have seen in the lectures but for ML-specific workloads. \r\n• We can reuse existing experiment replication packages.\r\n22\r\n\r\nC2. Plugin to visualize Hugging Face carbon emissions.\r\n• https://huggingface.co/blog/leaderboard-emissions-\r\nanalysis \r\n•\r\nYoo, Taewon, et al. ""Visualizing the Carbon Intensity of Machine Learning Inference for Image Analysis on TensorFlow Hub.""\r\n\r\nC3. Plugin for ChatGPT\r\n• Users seldom know how much carbon they are emitting when they interact \r\nwith chat GPT. \r\n• Let’s make it transparent to the users. Browser plugin?\r\n24\r\nCO2\r\n\r\nC4. NutriScore for software libraries\r\n25\r\n• NutriScore labels are not perfect but they are a good starting point! \r\n• What if we could do the same thing for the energy efficiency of software. \r\n• (Also open to individual or social sustainability) \r\n• This work can be scoped in particular domains/ecossystems/use cases. \r\n• Libraries for stats? ML? Web Dev? Cloud?\r\n\r\nC5. Add energy-awareness to existing software\r\n26\r\n• Streamlit, Notebooks, JSFiddle, \r\nPyScript, zsh, etc. \r\n•\r\n\r\nC6 - Green Shift Left\r\n• Estimate energy efficiency using static code analysis. \r\n• We don’t need an accurate value. \r\n• It is useful to know which code is more likely to \r\nintroduce energy hotspots and that should be \r\nreviewed with more attention. \r\n• Can be scoped to a particular domain (react, php, data \r\nscience, web, etc., etc.)', 'computing vision, classification)\r\n\r\nA2. Adding energy metrics to LMstudio/Ollama\r\n• Make energy consumption visible to users \r\nwithin local chat-interface for LLMs \r\n• LMstudio/Ollama are tools for easy \r\ndeployment of LLMs \r\n• Do not show energy metrics \r\n• Add energy metrics to LMstudio-python \r\nor Ollama\r\nhttps://lmstudio.ai/\r\n\r\nA3. Visualizations built-in with EnergiBridge\r\n• EnergiBridge simplifies energy measurement, but analysis & visualization is \r\nstill left to the user \r\n• Extend the tool with well-chosen visualizations and analyses directly from the \r\nprovided data\r\n\r\nA4. Service-based version of EnergiBridge\r\n• Simplify interaction & setup with EnergiBridge \r\n• Service that runs independently, start/stop signals over RPC to manage \r\nexperiments \r\n• Potential: create EnergiBridge interface for other prog. lang\r\n\r\nB1. Measure energy consumption of single JUnit tests\r\n• We’d like to identify energy anti patterns in unit tests \r\n• As a first step, we need tooling to measure and compare the energy \r\nconsumption of single unit tests \r\n• Ideally including preliminary analysis looking at potential reasons for high-\r\nenergy-consuming tests\r\n\r\nB2. Study test generation energy consumption\r\n• Automatic test generation mainly focuses on making strong test suites \r\n• Do different techniques and configurations impact the energy consumption \r\nduring generation? \r\n• Preferably focus on non-LLM test generation methods (EvoSuite, Pyguin, \r\nDSpot)\r\n\r\nB3. Detailed energy profiling of build pipelines\r\n• Automatic builds have become a cornerstone of quality assurance. But how \r\nmuch energy do they even consume? \r\n• Create a tool that reports on the energy consumed during the (different stages \r\nof the) whole build (compile, build, test, package, …) \r\n• Should be integrated with build system(s), making setup for developers easy \r\n• For local setup (to enable true energy measurements)\r\n\r\nB4. Energy profiling of static analysis tools\r\n• What is the energy consumption of a ""typical run"" for a few OSS projects? \r\n• Does the type of analysis matter? Are certain analysis more expensive? Does \r\nthe number of rules that are activated in a static analysis tool important for the \r\nenergy consumption? \r\n• Differences between static analysis tools [lower priority]\r\n\r\nB5. Tool supporting SusAF workshop / process\r\n• Lead engineers through process & questions \r\n• Interface to create & document the two diagrams \r\n• (!) Should be easy to start using & set up \r\n• You may also create a simplified version / \u2028\r\nyour favorite sustainability framework\r\nEnvironmental\r\nEconomic\r\nTechnical\r\nIndividual\r\nSocial\r\nImmediate\r\nEnabling\r\nStructural\r\nAirbnb\r\ngreater \r\nearnings\r\nincrease \r\nin rents\r\ngentrification\r\ngreater racial \r\ndisparities\r\nrent \r\nrooms\r\n5\r\n1\r\n3\r\n4\r\n2\r\n \r\n \r\n \r\n \r\n \r\n \r\nVery \r\nunlikely \r\nVery \r\nlikely \r\n\r\nB6. Dataset of government-developed OS software\r\n• Governments develop & use software for supporting society\u2028\r\nOpen-source development & policies are on the rise \r\n• But studying government software is difficult b/c we don’t know what is out \r\nthere\u2028\r\n→ Lack of incentive to make popular \u2028\r\n→ Language barriers \r\n• Create a comprehensive dataset, incl. data to understand state of software: \r\nbuildable?, open dev. history?, requirements documentation? \r\n• Could start with NL, but including your / other countries greatly appreciated!\r\n\r\nB7. Queue - but better for the student / TA society\r\n• Requirements analysis regarding social and individual sustainability effects \r\nof Queue \r\n• Other EIP / TUD used software also possible: e.g., Answers EWI \r\n• Non-technical project → proper process (workshops?', '• Deadline April 4.\r\n6\r\n\r\nStrategy\r\n• Week 0 \r\n• Decide project idea (today) \r\n• Define steering meeting schedule \r\n• Create working document of the \r\narticle: Problem statement and solution \r\nproposal! \r\n• Define and assign tasks for each week. \r\n• Week 1 \r\n• Implementation \r\n• Agreements with supervisor. \r\n• Week 2 \r\n• Implementation \r\n• Week 3 \r\n• Implementation, Full draft of article, \r\ndissemination. \r\n• Week 4 \r\n• Final refinements \r\n• Prepare presentation\r\n7\r\n\r\nProject ideas\r\n• A1. Prototype cross-machine comparable benchmarks \r\n• A2. Add energy metrics to LMstudio/Ollama \r\n• A3. Visualizations built-in with EnergiBridge \r\n• A4. Service-based version of EnergiBridge\r\n\r\nProject ideas\r\n• B1. Measure energy consumption of single JUnit tests \r\n• B2. Study test generation energy consumption \r\n• B3. Energy profiling of static analysis tools \r\n• B4. Detailed energy profiling of build pipelines \r\n• B5. Tool supporting SusAF workshop / process \r\n• B6. Dataset of government-developed OS software \r\n• B7. Queue - but better for the student / TA society\r\nQuality Assurance & Testing\r\nSocial & Individual \u2028\r\nSustainability\r\n\r\nProject ideas\r\n• C1. Compare energy consumption of docker images for ML workloads. \r\n• C2. Create a plugin to visualize Hugging Face carbon emissions in detail. \r\n• C3. Plugin for ChatGPT (footprint per chat window) \r\n• C4. NutriScore for software libraries. \r\n• C5. Add energy-awareness to existing software \r\n• C6. Green Shift Left \r\n• C7. Education for Sustainable SE\r\n\r\nA1. Prototype cross-machine comparable benchmarks\r\n• Energy-usage comparisons require running both baseline + software on the \r\nsame machine → limits how extensive our experiments can be \r\n• Research community is in need of benchmarks that make energy \r\nmeasurements comparable even if executed on different machines \r\n• Focus on a single task or model (i.e. computing vision, classification)\r\n\r\nA2. Adding energy metrics to LMstudio/Ollama\r\n• Make energy consumption visible to users \r\nwithin local chat-interface for LLMs \r\n• LMstudio/Ollama are tools for easy \r\ndeployment of LLMs \r\n• Do not show energy metrics \r\n• Add energy metrics to LMstudio-python \r\nor Ollama\r\nhttps://lmstudio.ai/\r\n\r\nA3. Visualizations built-in with EnergiBridge\r\n• EnergiBridge simplifies energy measurement, but analysis & visualization is \r\nstill left to the user \r\n• Extend the tool with well-chosen visualizations and analyses directly from the \r\nprovided data\r\n\r\nA4. Service-based version of EnergiBridge\r\n• Simplify interaction & setup with EnergiBridge \r\n• Service that runs independently, start/stop signals over RPC to manage \r\nexperiments \r\n• Potential: create EnergiBridge interface for other prog. lang\r\n\r\nB1. Measure energy consumption of single JUnit tests\r\n• We’d like to identify energy anti patterns in unit tests \r\n• As a first step, we need tooling to measure and compare the energy \r\nconsumption of single unit tests \r\n• Ideally including preliminary analysis looking at potential reasons for high-\r\nenergy-consuming tests\r\n\r\nB2. Study test generation energy consumption\r\n• Automatic test generation mainly focuses on making strong test suites \r\n• Do different techniques and configurations impact the energy consumption \r\nduring generation? \r\n• Preferably focus on non-LLM test generation methods (EvoSuite, Pyguin, \r\nDSpot)\r\n\r\nB3. Detailed energy profiling of build pipelines\r\n• Automatic builds have become a cornerstone of quality assurance.']","['<1-hop>\r\n\r\nB6. Dataset of government-developed OS software\r\n• Governments develop & use software for supporting society\u2028\r\nOpen-source development & policies are on the rise\r\n• But studying government software is diﬃcult b/c we don’t know what is out \r\nthere\u2028\r\n→ Lack of incentive to make popular \u2028\r\n→ Language barriers\r\n• Create a comprehensive dataset, incl. data to understand state of software: \r\nbuildable?, open dev. history?, requirements documentation?\r\n• Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\r\n\r\nProject ideas\r\n• B1. Measure energy consumption of single JUnit tests \r\n• B2. Study test generation energy consumption\r\n• B3. Energy proﬁling of static analysis tools\r\n• B4. Detailed energy proﬁling of build pipelines\r\n• B5. Tool supporting SusAF workshop / process\r\n• B6. Dataset of government-developed OS software\r\n• B7. Queue - but better for the student / TA society\r\nQuality Assurance & Testing\r\nSocial & Individual \u2028\r\nSustainability', '<1-hop>\r\n\r\nB6. Dataset of government-developed OS software\r\n• Governments develop & use software for supporting society\u2028\r\nOpen-source development & policies are on the rise\r\n• But studying government software is diﬃcult b/c we don’t know what is out \r\nthere\u2028\r\n→ Lack of incentive to make popular \u2028\r\n→ Language barriers\r\n• Create a comprehensive dataset, incl. data to understand state of software: \r\nbuildable?, open dev. history?, requirements documentation?\r\n• Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\r\n\r\nProject ideas\r\n• B1. Measure energy consumption of single JUnit tests \r\n• B2. Study test generation energy consumption\r\n• B3. Energy proﬁling of static analysis tools\r\n• B4. Detailed energy proﬁling of build pipelines\r\n• B5. Tool supporting SusAF workshop / process\r\n• B6. Dataset of government-developed OS software\r\n• B7. Queue - but better for the student / TA society\r\nQuality Assurance & Testing\r\nSocial & Individual \u2028\r\nSustainability', '<1-hop>\r\n\r\nB6. Dataset of government-developed OS software\r\n• Governments develop & use software for supporting society\u2028\r\nOpen-source development & policies are on the rise\r\n• But studying government software is diﬃcult b/c we don’t know what is out \r\nthere\u2028\r\n→ Lack of incentive to make popular \u2028\r\n→ Language barriers\r\n• Create a comprehensive dataset, incl. data to understand state of software: \r\nbuildable?, open dev. history?, requirements documentation?\r\n• Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\r\n\r\nProject ideas\r\n• B1. Measure energy consumption of single JUnit tests \r\n• B2. Study test generation energy consumption\r\n• B3. Energy proﬁling of static analysis tools\r\n• B4. Detailed energy proﬁling of build pipelines\r\n• B5. Tool supporting SusAF workshop / process\r\n• B6. Dataset of government-developed OS software\r\n• B7. Queue - but better for the student / TA society\r\nQuality Assurance & Testing\r\nSocial & Individual \u2028\r\nSustainability']","The challenges in creating a dataset of government-developed open-source software include:

1. **Lack of Incentive to Make Software Popular**: There is often little motivation for governments to promote their software widely, which can lead to difficulties in discovering and cataloging such software.

2. **Language Barriers**: Government software may be developed in various languages, making it challenging to compile a comprehensive dataset that is accessible and understandable to a global audience.

3. **Understanding the State of Software**: It is crucial to include data that helps understand the state of the software, such as whether it is buildable, has an open development history, and includes requirements documentation.

These challenges make it difficult to study government software comprehensively.","Creating a dataset of government-developed OS software faces challenges such as a lack of incentive to make the software popular and language barriers, which hinder comprehensive understanding of the software's buildability, open development history, and requirements documentation.",0.99999999995,1.0,0.8181818181818182,0.77,,0.9696515751694866
"What are the practical applications of neuromorphic computing, and how does its biological inspiration influence its implementation in modern computing systems?","['McCulloch-Pitts neuron [1, 2]\r\n1940s: How do biological neurons compute basic \r\nlogic functions? (e.g. logic gates)\r\nNote: Ref. [3] gives a nice brief history on the ideas which lead to the \r\nMcCulloch-Pitts neuron.\r\n\r\nWhat is a neural network?\r\n1950s: How are neurons organized to perform \r\nsensory perception?\r\n\r\nWhat is a neural network?\r\n1950s: How are neurons organized to perform \r\nsensory perception?\r\nThe first ""neural network"": Perceptron (1958). [4]\r\nhttps://www.reddit.com/r/interestingasfuck/comments/e8a8oy/frank_rosenblatt_with_a_mark_i_perceptron/\r\nFrank Rosenblatt (Psychologist)\r\nwith a Mark I Perceptron computer in 1960\r\n\r\nWhat is a neural network?\r\n1950s: How are neurons organized to perform \r\nsensory perception?\r\nThe first ""neural network"": Perceptron (1958). [4]\r\nhttps://www.reddit.com/r/interestingasfuck/comments/e8a8oy/frank_rosenblatt_with_a_mark_i_perceptron/\r\nFrank Rosenblatt (Psychologist)\r\nwith a Mark I Perceptron computer in 1960\r\nPower-efficient\r\n\r\nNeuromorphic computing\r\nWhy neuromorphic computing?\r\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\r\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\r\n\r\nNeuromorphic computing\r\nWhy neuromorphic computing?\r\n‘Biological inspiration’ for artificial neural networks (ANNs) is not a new idea.\r\nEmulation (as opposed to simulation) of neural networks in hardware is not a new idea.\r\nQuestion: What can we gain from increasing biological realism in existing neural networks?\r\n\r\nPower-efficiency at scale\r\nModern, deep neural networks*\r\nare trained using GPUs.\r\n* It is estimated that ChatGPT was trained on \r\n10,000-20,000 GPUs and that it will require\r\n30,000 GPUs to keep running stably in the future.\r\n* It is estimated that ChatGPT has 10-20 billion \r\nparameters.\r\nhttps://www.anandtech.com/show/4008/nvidias-geforce-gtx-580/17\r\n\r\nPower-efficiency at scale\r\nModern, deep neural networks*\r\nare trained using GPUs.\r\n* It is estimated that ChatGPT was trained on \r\n10,000-20,000 GPUs and that it will require\r\n30,000 GPUs to keep running stably in the future.\r\n* It is estimated that ChatGPT has 10-20 billion \r\nparameters.\r\nSingle model with 20 billion parameters:\r\n200 Watts x 30,000 GPUs = 6M Watts\r\n\r\nPower-efficiency at scale\r\nhttps://hai.stanford.edu/ai-index/2024-ai-index-report\r\nTraining a single large language model can \r\ngenerate greater CO2 emissions than the \r\ntotal lifetime emissions of 8 cars (in 2020)!\r\n\r\nPower-efficiency at scale\r\nhttps://hai.stanford.edu/ai-index/2024-ai-index-report\r\nTraining a single model can consume \r\nmore than 1000 MWh of power!\r\n\r\nPower-efficiency at scale\r\nhttps://hai.stanford.edu/ai-index/2024-ai-index-report\r\nTraining a single model can consume \r\nmore than 1000 MWh of power!\r\n… with energy costs reaching 200M USD!\r\n\r\nPower-efficiency at scale\r\nYour brain runs on:\r\n\r\nPower-efficiency at scale\r\nYour brain runs on:\r\nHigh estimate ~3000 kcal a day\r\n   ≈145 Watts\r\n* Human brain has ~600 trillion synapses \r\n(≈parameters).', 'Different encoding strategies with spiking neurons\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\nQuestions?\r\n\r\nNeuromorphic computing\r\nWhat is the advantage for applications?\r\n-\r\nLess spikes = less energy consumption in specialized neuromorphic hardware\r\n(e.g. Intel Loihi [12])\r\nBrains are energy efficient: \r\n2. Sparse encoding\r\n\r\nMultiply-accumulate (MAC) operations:\r\nNormal neuron: Multiplies input with \r\nweights, then adds.\r\nSpiking neuron: Consider binary input (e.g. input currents are piecewise \r\nconstant and assume values {0,1}). There is no multiplication, only addition.\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\r\nNormal neuron: nin x nout multiplications,\r\n(nin– 1) x nout additions\r\nSpiking neuron: 0 multiplications, (nactive– 1) x nout \r\nadditions, with nactive ≤ nin\r\nEnergy consumption:\r\nEnormal = M nin nout + A (nin– 1) nout = 17.5 pJ\r\nEnergy consumption:\r\nEspiking = A (nactive– 1) nout = 2.7 pJ\r\nMultiply-accumulate (MAC) operations:\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\r\nNormal neuron: nin x nout multiplications,\r\n(nin– 1) x nout additions\r\nSpiking neuron: 0 multiplications, (nactive– 1) x nout \r\nadditions, with nactive ≤ nin\r\nEnergy consumption:\r\nEnormal = M nin nout + A (nin– 1) nout = 17.5 pJ\r\nEnergy consumption:\r\nEspiking = A (nactive– 1) nout = 2.7 pJ\r\nMultiply-accumulate (MAC) operations:\r\nSNN challenge: how to \r\ncompute with the least \r\namount of spikes!\r\n\r\nOften, we observe a \r\nsparsity (energy)-\r\ntask accuracy trade-off\r\n(Left: results for image \r\nclassification)\r\nSNN challenge: how to \r\ncompute with the least \r\namount of spikes!\r\nhttps://arxiv.org/pdf/2409.08290\r\n\r\nIn practice, energy \r\nconsumption computations \r\nare complex. \r\nNeed to take into account\r\n- memory access,\r\n- addressing,\r\n- auxiliary operations,\r\nin addition to MACs.\r\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \r\nand video classification respectively). FNN’s are conventional feed-forward neural networks.\r\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.\r\nhttps://arxiv.org/pdf/2210.13107\r\nComputing energy consumption\r\n\r\nIn practice, energy \r\nconsumption computations \r\nare complex. \r\nNeed to take into account\r\n- memory access,\r\n- addressing,\r\n- auxiliary operations,\r\nin addition to MACs.\r\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \r\nand video classification respectively). FNN’s are conventional feed-forward neural networks.\r\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.', 'Normal neuron: nin x nout multiplications,\r\n(nin– 1) x nout additions\r\nSpiking neuron: 0 multiplications, (nactive– 1) x nout \r\nadditions, with nactive ≤ nin\r\nEnergy consumption:\r\nEnormal = M nin nout + A (nin– 1) nout = 17.5 pJ\r\nEnergy consumption:\r\nEspiking = A (nactive– 1) nout = 2.7 pJ\r\nMultiply-accumulate (MAC) operations:\r\nSNN challenge: how to \r\ncompute with the least \r\namount of spikes!\r\n\r\nOften, we observe a \r\nsparsity (energy)-\r\ntask accuracy trade-off\r\n(Left: results for image \r\nclassification)\r\nSNN challenge: how to \r\ncompute with the least \r\namount of spikes!\r\nhttps://arxiv.org/pdf/2409.08290\r\n\r\nIn practice, energy \r\nconsumption computations \r\nare complex. \r\nNeed to take into account\r\n- memory access,\r\n- addressing,\r\n- auxiliary operations,\r\nin addition to MACs.\r\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \r\nand video classification respectively). FNN’s are conventional feed-forward neural networks.\r\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.\r\nhttps://arxiv.org/pdf/2210.13107\r\nComputing energy consumption\r\n\r\nIn practice, energy \r\nconsumption computations \r\nare complex. \r\nNeed to take into account\r\n- memory access,\r\n- addressing,\r\n- auxiliary operations,\r\nin addition to MACs.\r\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \r\nand video classification respectively). FNN’s are conventional feed-forward neural networks.\r\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.\r\nComputing energy consumption\r\nQuestions?\r\n\r\nReading materials\r\nMain reading:\r\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \r\nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\r\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\r\n- Neuromorphic computing:\r\n- Based on biology: Zenke F, Bohté SM, Clopath C, Comşa IM, Göltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \r\nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\r\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \r\noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\r\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019.']","['<1-hop>\r\n\r\nNeuromorphic computing\r\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\r\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\r\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\r\n\r\nReading materials\r\nMain reading:\r\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \r\nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\r\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\r\n- Neuromorphic computing:\r\n- Based on biology: Zenke F, Bohté SM, Clopath C, Comşa IM, Göltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \r\nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\r\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \r\noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\r\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \r\nhttps://www.nature.com/articles/s41586-019-1677-2\r\nExtra reading:\r\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \r\nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\r\n- Also an important part of neuromorphic systems and vision → Event Cameras: Gallego G, Delbrück T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \r\nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \r\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\r\nBasics of \'conventional\' neural networks:\r\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\r\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.', '<1-hop>\r\n\r\nNeuromorphic computing\r\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\r\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\r\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\r\n\r\nReading materials\r\nMain reading:\r\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \r\nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\r\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\r\n- Neuromorphic computing:\r\n- Based on biology: Zenke F, Bohté SM, Clopath C, Comşa IM, Göltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \r\nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\r\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \r\noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\r\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \r\nhttps://www.nature.com/articles/s41586-019-1677-2\r\nExtra reading:\r\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \r\nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\r\n- Also an important part of neuromorphic systems and vision → Event Cameras: Gallego G, Delbrück T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \r\nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \r\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\r\nBasics of \'conventional\' neural networks:\r\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\r\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.']","Neuromorphic computing has several practical applications, primarily due to its energy efficiency and ability to process information in a manner similar to the human brain. Some of the key applications include:

1. **Energy-efficient Computing**: Neuromorphic systems are designed to be power-efficient, making them suitable for applications where energy consumption is a critical factor. For instance, they can be used in mobile devices, IoT devices, and other battery-powered electronics where extending battery life is crucial.

2. **Real-time Processing**: Due to their ability to process information quickly and efficiently, neuromorphic systems are ideal for real-time data processing tasks, such as image and sound recognition, autonomous vehicles, and robotics.

3. **Sparse Data Processing**: Neuromorphic systems use spiking neurons, which are more efficient in handling sparse data. This makes them suitable for applications like event-based vision systems, where data is not continuously streaming but occurs in bursts.

4. **Adaptive Learning**: Neuromorphic systems can adapt and learn from their environment in a way that mimics biological learning processes. This is useful in developing adaptive control systems and intelligent agents that can operate in dynamic environments.

The biological inspiration for neuromorphic computing influences its implementation by encouraging the use of architectures and processes that mimic the brain's structure and function. This includes the use of spiking neurons, which communicate through discrete spikes rather than continuous signals, leading to reduced energy consumption and increased efficiency. Additionally, neuromorphic systems often employ sparse encoding strategies, which further enhance their energy efficiency by minimizing the number of active components at any given time. This biologically inspired approach allows neuromorphic systems to achieve a balance between computational power and energy efficiency, making them suitable for a wide range of applications.","Neuromorphic computing, inspired by the human brain, offers practical applications such as event-based vision systems and efficient processing of sensory data. Its biological inspiration influences its implementation through the use of spiking neural networks and physical artificial neurons, enabling more energy-efficient and brain-like computation methods.",0.0,0.5,0.6285714285714286,0.67,0.0,0.9819440245293172
"What are the practical applications of neuromorphic computing, and how does its biological inspiration influence its implementation?","['McCulloch-Pitts neuron [1, 2]\r\n1940s: How do biological neurons compute basic \r\nlogic functions? (e.g. logic gates)\r\nNote: Ref. [3] gives a nice brief history on the ideas which lead to the \r\nMcCulloch-Pitts neuron.\r\n\r\nWhat is a neural network?\r\n1950s: How are neurons organized to perform \r\nsensory perception?\r\n\r\nWhat is a neural network?\r\n1950s: How are neurons organized to perform \r\nsensory perception?\r\nThe first ""neural network"": Perceptron (1958). [4]\r\nhttps://www.reddit.com/r/interestingasfuck/comments/e8a8oy/frank_rosenblatt_with_a_mark_i_perceptron/\r\nFrank Rosenblatt (Psychologist)\r\nwith a Mark I Perceptron computer in 1960\r\n\r\nWhat is a neural network?\r\n1950s: How are neurons organized to perform \r\nsensory perception?\r\nThe first ""neural network"": Perceptron (1958). [4]\r\nhttps://www.reddit.com/r/interestingasfuck/comments/e8a8oy/frank_rosenblatt_with_a_mark_i_perceptron/\r\nFrank Rosenblatt (Psychologist)\r\nwith a Mark I Perceptron computer in 1960\r\nPower-efficient\r\n\r\nNeuromorphic computing\r\nWhy neuromorphic computing?\r\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\r\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\r\n\r\nNeuromorphic computing\r\nWhy neuromorphic computing?\r\n‘Biological inspiration’ for artificial neural networks (ANNs) is not a new idea.\r\nEmulation (as opposed to simulation) of neural networks in hardware is not a new idea.\r\nQuestion: What can we gain from increasing biological realism in existing neural networks?\r\n\r\nPower-efficiency at scale\r\nModern, deep neural networks*\r\nare trained using GPUs.\r\n* It is estimated that ChatGPT was trained on \r\n10,000-20,000 GPUs and that it will require\r\n30,000 GPUs to keep running stably in the future.\r\n* It is estimated that ChatGPT has 10-20 billion \r\nparameters.\r\nhttps://www.anandtech.com/show/4008/nvidias-geforce-gtx-580/17\r\n\r\nPower-efficiency at scale\r\nModern, deep neural networks*\r\nare trained using GPUs.\r\n* It is estimated that ChatGPT was trained on \r\n10,000-20,000 GPUs and that it will require\r\n30,000 GPUs to keep running stably in the future.\r\n* It is estimated that ChatGPT has 10-20 billion \r\nparameters.\r\nSingle model with 20 billion parameters:\r\n200 Watts x 30,000 GPUs = 6M Watts\r\n\r\nPower-efficiency at scale\r\nhttps://hai.stanford.edu/ai-index/2024-ai-index-report\r\nTraining a single large language model can \r\ngenerate greater CO2 emissions than the \r\ntotal lifetime emissions of 8 cars (in 2020)!\r\n\r\nPower-efficiency at scale\r\nhttps://hai.stanford.edu/ai-index/2024-ai-index-report\r\nTraining a single model can consume \r\nmore than 1000 MWh of power!\r\n\r\nPower-efficiency at scale\r\nhttps://hai.stanford.edu/ai-index/2024-ai-index-report\r\nTraining a single model can consume \r\nmore than 1000 MWh of power!\r\n… with energy costs reaching 200M USD!\r\n\r\nPower-efficiency at scale\r\nYour brain runs on:\r\n\r\nPower-efficiency at scale\r\nYour brain runs on:\r\nHigh estimate ~3000 kcal a day\r\n   ≈145 Watts\r\n* Human brain has ~600 trillion synapses \r\n(≈parameters).', 'Different encoding strategies with spiking neurons\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\nQuestions?\r\n\r\nNeuromorphic computing\r\nWhat is the advantage for applications?\r\n-\r\nLess spikes = less energy consumption in specialized neuromorphic hardware\r\n(e.g. Intel Loihi [12])\r\nBrains are energy efficient: \r\n2. Sparse encoding\r\n\r\nMultiply-accumulate (MAC) operations:\r\nNormal neuron: Multiplies input with \r\nweights, then adds.\r\nSpiking neuron: Consider binary input (e.g. input currents are piecewise \r\nconstant and assume values {0,1}). There is no multiplication, only addition.\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\r\nNormal neuron: nin x nout multiplications,\r\n(nin– 1) x nout additions\r\nSpiking neuron: 0 multiplications, (nactive– 1) x nout \r\nadditions, with nactive ≤ nin\r\nEnergy consumption:\r\nEnormal = M nin nout + A (nin– 1) nout = 17.5 pJ\r\nEnergy consumption:\r\nEspiking = A (nactive– 1) nout = 2.7 pJ\r\nMultiply-accumulate (MAC) operations:\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\r\nNormal neuron: nin x nout multiplications,\r\n(nin– 1) x nout additions\r\nSpiking neuron: 0 multiplications, (nactive– 1) x nout \r\nadditions, with nactive ≤ nin\r\nEnergy consumption:\r\nEnormal = M nin nout + A (nin– 1) nout = 17.5 pJ\r\nEnergy consumption:\r\nEspiking = A (nactive– 1) nout = 2.7 pJ\r\nMultiply-accumulate (MAC) operations:\r\nSNN challenge: how to \r\ncompute with the least \r\namount of spikes!\r\n\r\nOften, we observe a \r\nsparsity (energy)-\r\ntask accuracy trade-off\r\n(Left: results for image \r\nclassification)\r\nSNN challenge: how to \r\ncompute with the least \r\namount of spikes!\r\nhttps://arxiv.org/pdf/2409.08290\r\n\r\nIn practice, energy \r\nconsumption computations \r\nare complex. \r\nNeed to take into account\r\n- memory access,\r\n- addressing,\r\n- auxiliary operations,\r\nin addition to MACs.\r\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \r\nand video classification respectively). FNN’s are conventional feed-forward neural networks.\r\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.\r\nhttps://arxiv.org/pdf/2210.13107\r\nComputing energy consumption\r\n\r\nIn practice, energy \r\nconsumption computations \r\nare complex. \r\nNeed to take into account\r\n- memory access,\r\n- addressing,\r\n- auxiliary operations,\r\nin addition to MACs.\r\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \r\nand video classification respectively). FNN’s are conventional feed-forward neural networks.\r\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.', 'Normal neuron: nin x nout multiplications,\r\n(nin– 1) x nout additions\r\nSpiking neuron: 0 multiplications, (nactive– 1) x nout \r\nadditions, with nactive ≤ nin\r\nEnergy consumption:\r\nEnormal = M nin nout + A (nin– 1) nout = 17.5 pJ\r\nEnergy consumption:\r\nEspiking = A (nactive– 1) nout = 2.7 pJ\r\nMultiply-accumulate (MAC) operations:\r\nSNN challenge: how to \r\ncompute with the least \r\namount of spikes!\r\n\r\nOften, we observe a \r\nsparsity (energy)-\r\ntask accuracy trade-off\r\n(Left: results for image \r\nclassification)\r\nSNN challenge: how to \r\ncompute with the least \r\namount of spikes!\r\nhttps://arxiv.org/pdf/2409.08290\r\n\r\nIn practice, energy \r\nconsumption computations \r\nare complex. \r\nNeed to take into account\r\n- memory access,\r\n- addressing,\r\n- auxiliary operations,\r\nin addition to MACs.\r\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \r\nand video classification respectively). FNN’s are conventional feed-forward neural networks.\r\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.\r\nhttps://arxiv.org/pdf/2210.13107\r\nComputing energy consumption\r\n\r\nIn practice, energy \r\nconsumption computations \r\nare complex. \r\nNeed to take into account\r\n- memory access,\r\n- addressing,\r\n- auxiliary operations,\r\nin addition to MACs.\r\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \r\nand video classification respectively). FNN’s are conventional feed-forward neural networks.\r\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.\r\nComputing energy consumption\r\nQuestions?\r\n\r\nReading materials\r\nMain reading:\r\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \r\nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\r\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\r\n- Neuromorphic computing:\r\n- Based on biology: Zenke F, Bohté SM, Clopath C, Comşa IM, Göltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \r\nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\r\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \r\noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\r\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019.']","['<1-hop>\r\n\r\nNeuromorphic computing\r\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\r\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\r\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\r\n\r\nReading materials\r\nMain reading:\r\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \r\nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\r\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\r\n- Neuromorphic computing:\r\n- Based on biology: Zenke F, Bohté SM, Clopath C, Comşa IM, Göltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \r\nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\r\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \r\noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\r\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \r\nhttps://www.nature.com/articles/s41586-019-1677-2\r\nExtra reading:\r\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \r\nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\r\n- Also an important part of neuromorphic systems and vision → Event Cameras: Gallego G, Delbrück T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \r\nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \r\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\r\nBasics of \'conventional\' neural networks:\r\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\r\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.']","Neuromorphic computing is inspired by the structure and function of the human brain, and it aims to emulate neural networks in hardware rather than just simulate them. This biological inspiration influences its implementation by focusing on energy efficiency and sparse encoding, similar to how the human brain operates.

Practical applications of neuromorphic computing include:

1. **Energy-efficient Computing**: Neuromorphic systems are designed to be power-efficient, making them suitable for applications where energy consumption is a critical factor. For instance, spiking neural networks (SNNs) used in neuromorphic computing can be significantly more energy-efficient than traditional feed-forward neural networks (FNNs), as they require fewer multiply-accumulate operations and leverage sparse encoding strategies.

2. **Real-time Processing**: Neuromorphic systems can process information in real-time, which is beneficial for sensory perception tasks such as image and sound recognition. This capability is due to their ability to handle spatio-temporal data efficiently, similar to how biological neurons process information.

3. **Adaptive Learning**: Neuromorphic computing can support adaptive learning and decision-making processes, which are essential for developing intelligent systems that can operate in dynamic environments.

Overall, the biological inspiration of neuromorphic computing leads to systems that are not only more energy-efficient but also capable of handling complex, real-time data processing tasks in a manner akin to the human brain.","Neuromorphic computing, inspired by the human brain, offers practical applications through its ability to process information in a manner similar to biological neurons. This biological inspiration influences its implementation by enabling the development of systems that can efficiently handle tasks such as pattern recognition and adaptive learning, as detailed in resources like 'Computing with spiking neuron networks' and discussions on the convergence of deep learning with biological systems.",0.0,1.0,0.5,0.8,0.0,0.9742703195375054
What are the practical applications of neuromorphic systems that leverage biological inspiration and advanced training methods for efficient computation?,"['McCulloch-Pitts neuron [1, 2]\r\n1940s: How do biological neurons compute basic \r\nlogic functions? (e.g. logic gates)\r\nNote: Ref. [3] gives a nice brief history on the ideas which lead to the \r\nMcCulloch-Pitts neuron.\r\n\r\nWhat is a neural network?\r\n1950s: How are neurons organized to perform \r\nsensory perception?\r\n\r\nWhat is a neural network?\r\n1950s: How are neurons organized to perform \r\nsensory perception?\r\nThe first ""neural network"": Perceptron (1958). [4]\r\nhttps://www.reddit.com/r/interestingasfuck/comments/e8a8oy/frank_rosenblatt_with_a_mark_i_perceptron/\r\nFrank Rosenblatt (Psychologist)\r\nwith a Mark I Perceptron computer in 1960\r\n\r\nWhat is a neural network?\r\n1950s: How are neurons organized to perform \r\nsensory perception?\r\nThe first ""neural network"": Perceptron (1958). [4]\r\nhttps://www.reddit.com/r/interestingasfuck/comments/e8a8oy/frank_rosenblatt_with_a_mark_i_perceptron/\r\nFrank Rosenblatt (Psychologist)\r\nwith a Mark I Perceptron computer in 1960\r\nPower-efficient\r\n\r\nNeuromorphic computing\r\nWhy neuromorphic computing?\r\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\r\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\r\n\r\nNeuromorphic computing\r\nWhy neuromorphic computing?\r\n‘Biological inspiration’ for artificial neural networks (ANNs) is not a new idea.\r\nEmulation (as opposed to simulation) of neural networks in hardware is not a new idea.\r\nQuestion: What can we gain from increasing biological realism in existing neural networks?\r\n\r\nPower-efficiency at scale\r\nModern, deep neural networks*\r\nare trained using GPUs.\r\n* It is estimated that ChatGPT was trained on \r\n10,000-20,000 GPUs and that it will require\r\n30,000 GPUs to keep running stably in the future.\r\n* It is estimated that ChatGPT has 10-20 billion \r\nparameters.\r\nhttps://www.anandtech.com/show/4008/nvidias-geforce-gtx-580/17\r\n\r\nPower-efficiency at scale\r\nModern, deep neural networks*\r\nare trained using GPUs.\r\n* It is estimated that ChatGPT was trained on \r\n10,000-20,000 GPUs and that it will require\r\n30,000 GPUs to keep running stably in the future.\r\n* It is estimated that ChatGPT has 10-20 billion \r\nparameters.\r\nSingle model with 20 billion parameters:\r\n200 Watts x 30,000 GPUs = 6M Watts\r\n\r\nPower-efficiency at scale\r\nhttps://hai.stanford.edu/ai-index/2024-ai-index-report\r\nTraining a single large language model can \r\ngenerate greater CO2 emissions than the \r\ntotal lifetime emissions of 8 cars (in 2020)!\r\n\r\nPower-efficiency at scale\r\nhttps://hai.stanford.edu/ai-index/2024-ai-index-report\r\nTraining a single model can consume \r\nmore than 1000 MWh of power!\r\n\r\nPower-efficiency at scale\r\nhttps://hai.stanford.edu/ai-index/2024-ai-index-report\r\nTraining a single model can consume \r\nmore than 1000 MWh of power!\r\n… with energy costs reaching 200M USD!\r\n\r\nPower-efficiency at scale\r\nYour brain runs on:\r\n\r\nPower-efficiency at scale\r\nYour brain runs on:\r\nHigh estimate ~3000 kcal a day\r\n   ≈145 Watts\r\n* Human brain has ~600 trillion synapses \r\n(≈parameters).', ""• Our question: Can we save energy with alternative approaches? \r\n• We studied Grid Search, Random Search and Bayesian \r\nOptimisation.\r\n27\r\nTim's MSc Thesis, 2022 \r\nAccepted at CAIN’23\r\n\r\n\r\nResults\r\n29\r\nConclusions? \r\n• Bayesian converges faster. \r\n• No clear winner between Grid and Random\r\n\r\nDeepSeekMoE\r\n30\r\nPreprint at ArXiv, 2024 https://arxiv.org/pdf/\r\n2401.06066\r\n\r\nMixture of Experts\r\n• Llama3.1 has 405B parameters, DeepSeek V3 671B \r\n• Yet DeepSeek has quicker inference times and claims less energy \r\nconsumption (?) \r\n• Divide the model into smaller blocks of experts \r\n• Tokens get routed to certain experts based on the query \r\n• Only part of the network is active during inference \r\n• DeepSeek claims only 37B out of 671B parameters get active\r\n31\r\n\r\nDeepSeekMoE\r\n32\r\n\r\nDeepSeekMoE\r\n33\r\n• Comparable performance to \r\nLLaMA2 7B effectively using \r\nless half the parameters \r\n• Less computational power \r\n• Problems (?) \r\n• Still need to load all the \r\nparameters \r\n• High memory -> high \r\nembodied carbon\r\n\r\nGreen AI at FacebookMeta\r\n34\r\nSustainable AI: Environmental Implications, \r\nChallenges and Opportunities (2022)\r\n\r\nCarbon footprint mapped to the AI lifecycle\r\n• There are 4 main overarching stages where carbon emissions need to be isolated: data \r\ncollection, experimentation, training, inference. \r\n• At Facebook, recommendation systems split energy consumption evenly between \r\ntraining and inference; text translation models have a 35%/65% split. (Operational cost) \r\n• Operational/embodied cost split: 30%/70%\r\n35\r\n\r\nOpen issues according to Meta\r\n• A vast portion of projects only use GPUs at 30%.\u2028\r\nShould be higher to attenuate embodied carbon.\r\n36\r\nBased on 10K AI projects\r\n\r\nData Jan\r\nInformed Adaptation\r\nData Feb\r\nData Mar\r\nData Apr\r\nData May\r\n…\r\n~Data Dec\r\n~Data Dec\r\nData Jan\r\nData Feb\r\nData Mar\r\nData Apr\r\nData May\r\n…\r\nModel update Model update Model update\r\nCheck if \r\ndata change\r\nCheck if \r\ndata change\r\nCheck if \r\ndata change\r\nModel update\r\nBlind Adaptation\r\nThe AI Model will be \r\nupdated fewer times and \r\nonly when necessary.\r\nKnow when to retrain models\r\nNeither too early nor too late\r\n37\r\nAdaptation Techniques\r\n?\r\n\r\nGreen Architectural Tactics for ML-Enabled \r\nSystem\r\n38\r\nICSE-SEIS 2024\r\n\r\nArchitectural tactics\r\n39\r\n\r\nData-centric\r\n40\r\n• Reduce data size \r\n• Sampling \r\n• Dimensionality reduction \r\n• Quantization\r\n\r\nAlgorithm Design\r\n41\r\n• Carefully select your algorithm \r\n• You don’t need the fanciest \r\ntechniques\r\n\r\nModel Optimization\r\n42\r\n• Add energy to training parameters \r\n• Reduce FLOPs \r\n• Pruning, sparsity \r\n•  Take advantage of existing models\r\n\r\nModel Training\r\n43\r\n• Quantization\r\n• SAVE TRAINING PROGRESS\r\n\r\nModel Deployment\r\n44\r\n• Distributed deployment \r\n• Energy efficient hardware and \r\nconfigurations\r\n\r\nModel Management\r\n45\r\n• Reuse the model as much as possible \r\n\r\nRethinking the Architecture: Spiking Neural Networks\r\n46\r\n\r\nRethinking the Architecture: Spiking Neural Networks\r\n47\r\n\r\nSELF Lab\r\n48\r\n\r\nrecap"", 'Different encoding strategies with spiking neurons\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\nQuestions?\r\n\r\nNeuromorphic computing\r\nWhat is the advantage for applications?\r\n-\r\nLess spikes = less energy consumption in specialized neuromorphic hardware\r\n(e.g. Intel Loihi [12])\r\nBrains are energy efficient: \r\n2. Sparse encoding\r\n\r\nMultiply-accumulate (MAC) operations:\r\nNormal neuron: Multiplies input with \r\nweights, then adds.\r\nSpiking neuron: Consider binary input (e.g. input currents are piecewise \r\nconstant and assume values {0,1}). There is no multiplication, only addition.\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\r\nNormal neuron: nin x nout multiplications,\r\n(nin– 1) x nout additions\r\nSpiking neuron: 0 multiplications, (nactive– 1) x nout \r\nadditions, with nactive ≤ nin\r\nEnergy consumption:\r\nEnormal = M nin nout + A (nin– 1) nout = 17.5 pJ\r\nEnergy consumption:\r\nEspiking = A (nactive– 1) nout = 2.7 pJ\r\nMultiply-accumulate (MAC) operations:\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\r\nNormal neuron: nin x nout multiplications,\r\n(nin– 1) x nout additions\r\nSpiking neuron: 0 multiplications, (nactive– 1) x nout \r\nadditions, with nactive ≤ nin\r\nEnergy consumption:\r\nEnormal = M nin nout + A (nin– 1) nout = 17.5 pJ\r\nEnergy consumption:\r\nEspiking = A (nactive– 1) nout = 2.7 pJ\r\nMultiply-accumulate (MAC) operations:\r\nSNN challenge: how to \r\ncompute with the least \r\namount of spikes!\r\n\r\nOften, we observe a \r\nsparsity (energy)-\r\ntask accuracy trade-off\r\n(Left: results for image \r\nclassification)\r\nSNN challenge: how to \r\ncompute with the least \r\namount of spikes!\r\nhttps://arxiv.org/pdf/2409.08290\r\n\r\nIn practice, energy \r\nconsumption computations \r\nare complex. \r\nNeed to take into account\r\n- memory access,\r\n- addressing,\r\n- auxiliary operations,\r\nin addition to MACs.\r\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \r\nand video classification respectively). FNN’s are conventional feed-forward neural networks.\r\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.\r\nhttps://arxiv.org/pdf/2210.13107\r\nComputing energy consumption\r\n\r\nIn practice, energy \r\nconsumption computations \r\nare complex. \r\nNeed to take into account\r\n- memory access,\r\n- addressing,\r\n- auxiliary operations,\r\nin addition to MACs.\r\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \r\nand video classification respectively). FNN’s are conventional feed-forward neural networks.\r\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.']","['<1-hop>\r\n\r\nNeuromorphic computing\r\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\r\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\r\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\r\n\r\nReading materials\r\nMain reading:\r\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \r\nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\r\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\r\n- Neuromorphic computing:\r\n- Based on biology: Zenke F, Bohté SM, Clopath C, Comşa IM, Göltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \r\nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\r\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \r\noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\r\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \r\nhttps://www.nature.com/articles/s41586-019-1677-2\r\nExtra reading:\r\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \r\nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\r\n- Also an important part of neuromorphic systems and vision → Event Cameras: Gallego G, Delbrück T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \r\nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \r\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\r\nBasics of \'conventional\' neural networks:\r\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\r\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.', '<1-hop>\r\n\r\nNeuromorphic computing\r\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\r\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\r\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\r\n\r\nReading materials\r\nMain reading:\r\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \r\nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\r\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\r\n- Neuromorphic computing:\r\n- Based on biology: Zenke F, Bohté SM, Clopath C, Comşa IM, Göltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \r\nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\r\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \r\noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\r\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \r\nhttps://www.nature.com/articles/s41586-019-1677-2\r\nExtra reading:\r\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \r\nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\r\n- Also an important part of neuromorphic systems and vision → Event Cameras: Gallego G, Delbrück T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \r\nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \r\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\r\nBasics of \'conventional\' neural networks:\r\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\r\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.', '<1-hop>\r\n\r\nNeuromorphic computing\r\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\r\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\r\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\r\n\r\nReading materials\r\nMain reading:\r\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \r\nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\r\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\r\n- Neuromorphic computing:\r\n- Based on biology: Zenke F, Bohté SM, Clopath C, Comşa IM, Göltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \r\nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\r\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \r\noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\r\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \r\nhttps://www.nature.com/articles/s41586-019-1677-2\r\nExtra reading:\r\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \r\nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\r\n- Also an important part of neuromorphic systems and vision → Event Cameras: Gallego G, Delbrück T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \r\nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \r\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\r\nBasics of \'conventional\' neural networks:\r\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\r\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.']","Neuromorphic systems, which are inspired by the structure and function of the human brain, offer several practical applications due to their efficient computation capabilities. These systems use spiking neurons and specialized hardware to achieve energy efficiency, making them suitable for applications where power consumption is a critical concern.

1. **Energy-Efficient Computing**: Neuromorphic systems are particularly advantageous in scenarios where energy efficiency is paramount. For example, they can be used in portable and wearable devices, where battery life is limited.

2. **Real-Time Processing**: Due to their efficient computation, neuromorphic systems can be used for real-time processing tasks, such as sensory data processing in robotics and autonomous vehicles.

3. **Sparse Encoding**: Neuromorphic systems utilize sparse encoding, which reduces the number of spikes (or activations) needed for computation. This is beneficial in applications like image and video processing, where data can be large and complex.

4. **Low-Power AI**: They are suitable for deploying AI models in environments with limited power resources, such as remote sensors or edge devices.

5. **Spiking Neural Networks (SNNs)**: These networks are a key component of neuromorphic systems and are used in applications like image classification, sound recognition, and video analysis, where they have been shown to be significantly more energy-efficient than traditional feed-forward neural networks.

Overall, neuromorphic systems are ideal for applications that require efficient, low-power computation, particularly in real-time and resource-constrained environments.","Neuromorphic systems, inspired by the human brain, have practical applications such as event-based vision, which is detailed in the reading materials. These systems use advanced training methods like surrogate gradient learning to optimize spiking neural networks, enabling efficient computation and real-world applications.",0.0,0.0,0.7272727272727273,0.25,,0.9504356642689143
"What is the biological basis of neuromorphic computing, and how does it differ from conventional neural networks?","['Different encoding strategies with spiking neurons\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\nQuestions?\r\n\r\nNeuromorphic computing\r\nWhat is the advantage for applications?\r\n-\r\nLess spikes = less energy consumption in specialized neuromorphic hardware\r\n(e.g. Intel Loihi [12])\r\nBrains are energy efficient: \r\n2. Sparse encoding\r\n\r\nMultiply-accumulate (MAC) operations:\r\nNormal neuron: Multiplies input with \r\nweights, then adds.\r\nSpiking neuron: Consider binary input (e.g. input currents are piecewise \r\nconstant and assume values {0,1}). There is no multiplication, only addition.\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\r\nNormal neuron: nin x nout multiplications,\r\n(nin– 1) x nout additions\r\nSpiking neuron: 0 multiplications, (nactive– 1) x nout \r\nadditions, with nactive ≤ nin\r\nEnergy consumption:\r\nEnormal = M nin nout + A (nin– 1) nout = 17.5 pJ\r\nEnergy consumption:\r\nEspiking = A (nactive– 1) nout = 2.7 pJ\r\nMultiply-accumulate (MAC) operations:\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\r\nNormal neuron: nin x nout multiplications,\r\n(nin– 1) x nout additions\r\nSpiking neuron: 0 multiplications, (nactive– 1) x nout \r\nadditions, with nactive ≤ nin\r\nEnergy consumption:\r\nEnormal = M nin nout + A (nin– 1) nout = 17.5 pJ\r\nEnergy consumption:\r\nEspiking = A (nactive– 1) nout = 2.7 pJ\r\nMultiply-accumulate (MAC) operations:\r\nSNN challenge: how to \r\ncompute with the least \r\namount of spikes!\r\n\r\nOften, we observe a \r\nsparsity (energy)-\r\ntask accuracy trade-off\r\n(Left: results for image \r\nclassification)\r\nSNN challenge: how to \r\ncompute with the least \r\namount of spikes!\r\nhttps://arxiv.org/pdf/2409.08290\r\n\r\nIn practice, energy \r\nconsumption computations \r\nare complex. \r\nNeed to take into account\r\n- memory access,\r\n- addressing,\r\n- auxiliary operations,\r\nin addition to MACs.\r\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \r\nand video classification respectively). FNN’s are conventional feed-forward neural networks.\r\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.\r\nhttps://arxiv.org/pdf/2210.13107\r\nComputing energy consumption\r\n\r\nIn practice, energy \r\nconsumption computations \r\nare complex. \r\nNeed to take into account\r\n- memory access,\r\n- addressing,\r\n- auxiliary operations,\r\nin addition to MACs.\r\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \r\nand video classification respectively). FNN’s are conventional feed-forward neural networks.\r\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.', ""- Higher firing rate (spikes per unit time) for \r\n'selected' feature.\r\nAdapted from Hubel & Wiesel, 1959\r\n\r\nFiring rates\r\nClassical view of the brain:\r\n- Each neuron is selective for one specific \r\nfeature in the input.\r\n- Higher firing rate (spikes per unit time) for \r\n'selected' feature.\r\n- Link to modern ANNs: The scalar output of \r\nan artificial neuron is interpreted as the firing \r\nrate.\r\nAdapted from Hubel & Wiesel, 1959\r\n\r\nFiring rates\r\nClassical view of the brain:\r\n- Each neuron is selective for one specific \r\nfeature in the input.\r\n- Higher firing rate (spikes per unit time) for \r\n'selected' feature.\r\n- Link to modern ANNs: The scalar output of \r\nan artificial neuron is interpreted as the firing \r\nrate.\r\nAdapted from Hubel & Wiesel, 1959\r\nBut rate coding is inefficient and slow…\r\n(i.e. each neuron needs to fire many spikes to get good precision)\r\nboth in vivo and in silico.\r\n\r\nDifferent encoding strategies with spiking neurons [8]\r\n7 neurons\r\n7 time bins\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nn\r\n6\r\n(in the general \r\ncase a \r\nnumber \r\nbetween 0-7)\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nn\r\nTotal spike \r\ncount can \r\nencode 3 bits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nIndividual rates \r\ncan encode 7 \r\nbits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nLatency can \r\nencode ~3*7 or \r\n~19 bits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nRank order can \r\nencode ~12 bits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\nQuestions?\r\n\r\nNeuromorphic computing\r\nWhat is the advantage for applications?\r\n-\r\nLess spikes = less energy consumption in specialized neuromorphic hardware\r\n(e.g. Intel Loihi [12])\r\nBrains are energy efficient: \r\n2. Sparse encoding\r\n\r\nMultiply-accumulate (MAC) operations:\r\nNormal neuron: Multiplies input with \r\nweights, then adds.\r\nSpiking neuron: Consider binary input (e.g. input currents are piecewise \r\nconstant and assume values {0,1}). There is no multiplication, only addition.\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\r\nNormal neuron: nin x nout multiplications,\r\n(nin– 1) x nout additions\r\nSpiking neuron: 0 multiplications, (nactive– 1) x nout \r\nadditions, with nactive ≤ nin\r\nEnergy consumption:\r\nEnormal = M nin nout + A (nin– 1) nout = 17.5 pJ\r\nEnergy consumption:\r\nEspiking = A (nactive– 1) nout = 2.7 pJ\r\nMultiply-accumulate (MAC) operations:\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ)."", 'McCulloch-Pitts neuron [1, 2]\r\n1940s: How do biological neurons compute basic \r\nlogic functions? (e.g. logic gates)\r\nNote: Ref. [3] gives a nice brief history on the ideas which lead to the \r\nMcCulloch-Pitts neuron.\r\n\r\nWhat is a neural network?\r\n1950s: How are neurons organized to perform \r\nsensory perception?\r\n\r\nWhat is a neural network?\r\n1950s: How are neurons organized to perform \r\nsensory perception?\r\nThe first ""neural network"": Perceptron (1958). [4]\r\nhttps://www.reddit.com/r/interestingasfuck/comments/e8a8oy/frank_rosenblatt_with_a_mark_i_perceptron/\r\nFrank Rosenblatt (Psychologist)\r\nwith a Mark I Perceptron computer in 1960\r\n\r\nWhat is a neural network?\r\n1950s: How are neurons organized to perform \r\nsensory perception?\r\nThe first ""neural network"": Perceptron (1958). [4]\r\nhttps://www.reddit.com/r/interestingasfuck/comments/e8a8oy/frank_rosenblatt_with_a_mark_i_perceptron/\r\nFrank Rosenblatt (Psychologist)\r\nwith a Mark I Perceptron computer in 1960\r\nPower-efficient\r\n\r\nNeuromorphic computing\r\nWhy neuromorphic computing?\r\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\r\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\r\n\r\nNeuromorphic computing\r\nWhy neuromorphic computing?\r\n‘Biological inspiration’ for artificial neural networks (ANNs) is not a new idea.\r\nEmulation (as opposed to simulation) of neural networks in hardware is not a new idea.\r\nQuestion: What can we gain from increasing biological realism in existing neural networks?\r\n\r\nPower-efficiency at scale\r\nModern, deep neural networks*\r\nare trained using GPUs.\r\n* It is estimated that ChatGPT was trained on \r\n10,000-20,000 GPUs and that it will require\r\n30,000 GPUs to keep running stably in the future.\r\n* It is estimated that ChatGPT has 10-20 billion \r\nparameters.\r\nhttps://www.anandtech.com/show/4008/nvidias-geforce-gtx-580/17\r\n\r\nPower-efficiency at scale\r\nModern, deep neural networks*\r\nare trained using GPUs.\r\n* It is estimated that ChatGPT was trained on \r\n10,000-20,000 GPUs and that it will require\r\n30,000 GPUs to keep running stably in the future.\r\n* It is estimated that ChatGPT has 10-20 billion \r\nparameters.\r\nSingle model with 20 billion parameters:\r\n200 Watts x 30,000 GPUs = 6M Watts\r\n\r\nPower-efficiency at scale\r\nhttps://hai.stanford.edu/ai-index/2024-ai-index-report\r\nTraining a single large language model can \r\ngenerate greater CO2 emissions than the \r\ntotal lifetime emissions of 8 cars (in 2020)!\r\n\r\nPower-efficiency at scale\r\nhttps://hai.stanford.edu/ai-index/2024-ai-index-report\r\nTraining a single model can consume \r\nmore than 1000 MWh of power!\r\n\r\nPower-efficiency at scale\r\nhttps://hai.stanford.edu/ai-index/2024-ai-index-report\r\nTraining a single model can consume \r\nmore than 1000 MWh of power!\r\n… with energy costs reaching 200M USD!\r\n\r\nPower-efficiency at scale\r\nYour brain runs on:\r\n\r\nPower-efficiency at scale\r\nYour brain runs on:\r\nHigh estimate ~3000 kcal a day\r\n   ≈145 Watts\r\n* Human brain has ~600 trillion synapses \r\n(≈parameters).']","['<1-hop>\r\n\r\nNeuromorphic computing\r\nNeuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain.\r\nA neuromorphic computer/chip is any device that uses physical artificial neurons to do computations.\r\nhttps://en.wikipedia.org/wiki/Neuromorphic_engineering, https://www.informationweek.com/software-services/what-you-need-to-know-about-neuromorphic-computing', '<2-hop>\r\n\r\nReading materials\r\nMain reading:\r\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \r\nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\r\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\r\n- Neuromorphic computing:\r\n- Based on biology: Zenke F, Bohté SM, Clopath C, Comşa IM, Göltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \r\nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\r\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \r\noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\r\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019. 575(7784):607-17. \r\nhttps://www.nature.com/articles/s41586-019-1677-2\r\nExtra reading:\r\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \r\nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\r\n- Also an important part of neuromorphic systems and vision → Event Cameras: Gallego G, Delbrück T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \r\nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \r\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\r\nBasics of \'conventional\' neural networks:\r\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\r\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.']","The biological basis of neuromorphic computing lies in its inspiration from the structure and function of the human brain. Neuromorphic computing uses physical artificial neurons to perform computations, emulating the brain's neural networks in hardware. This approach aims to replicate the brain's energy efficiency and processing capabilities.

In contrast, conventional neural networks, such as those used in artificial neural networks (ANNs), are typically implemented in software and run on general-purpose hardware like GPUs. These networks are inspired by biological neurons but do not emulate them at the hardware level. Conventional neural networks often require significant energy and computational resources, especially when scaled to large models, whereas neuromorphic computing aims for power efficiency by minimizing energy consumption through sparse encoding and reduced spike activity.

Overall, the key difference is that neuromorphic computing seeks to emulate the brain's efficiency and structure in hardware, while conventional neural networks simulate neural processes in software on traditional computing hardware.","Neuromorphic computing is based on the biological principles of the human brain, utilizing physical artificial neurons to perform computations. Unlike conventional neural networks, which rely on traditional computing architectures, neuromorphic systems are designed to mimic the brain's structure and function, enabling more efficient and biologically inspired processing.",0.5833333333041666,1.0,0.9166666666666666,0.86,0.0,0.956395957071718
What is the difference between spiking neural networks and spiking neuron networks?,"['Normal neuron: nin x nout multiplications,\r\n(nin– 1) x nout additions\r\nSpiking neuron: 0 multiplications, (nactive– 1) x nout \r\nadditions, with nactive ≤ nin\r\nEnergy consumption:\r\nEnormal = M nin nout + A (nin– 1) nout = 17.5 pJ\r\nEnergy consumption:\r\nEspiking = A (nactive– 1) nout = 2.7 pJ\r\nMultiply-accumulate (MAC) operations:\r\nSNN challenge: how to \r\ncompute with the least \r\namount of spikes!\r\n\r\nOften, we observe a \r\nsparsity (energy)-\r\ntask accuracy trade-off\r\n(Left: results for image \r\nclassification)\r\nSNN challenge: how to \r\ncompute with the least \r\namount of spikes!\r\nhttps://arxiv.org/pdf/2409.08290\r\n\r\nIn practice, energy \r\nconsumption computations \r\nare complex. \r\nNeed to take into account\r\n- memory access,\r\n- addressing,\r\n- auxiliary operations,\r\nin addition to MACs.\r\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \r\nand video classification respectively). FNN’s are conventional feed-forward neural networks.\r\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.\r\nhttps://arxiv.org/pdf/2210.13107\r\nComputing energy consumption\r\n\r\nIn practice, energy \r\nconsumption computations \r\nare complex. \r\nNeed to take into account\r\n- memory access,\r\n- addressing,\r\n- auxiliary operations,\r\nin addition to MACs.\r\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \r\nand video classification respectively). FNN’s are conventional feed-forward neural networks.\r\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.\r\nComputing energy consumption\r\nQuestions?\r\n\r\nReading materials\r\nMain reading:\r\n- Section 1 and Section 3.1 of ""Computing with spiking neuron networks."" by Paugam-Moisy H, Bohte SM, in Handbook of natural computing (2012). \r\nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\r\n- Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. https://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\r\n- Neuromorphic computing:\r\n- Based on biology: Zenke F, Bohté SM, Clopath C, Comşa IM, Göltz J, Maass W, Masquelier T, Naud R, Neftci EO, Petrovici MA, Scherr F. Visualizing a joint future of \r\nneuroscience and neuromorphic engineering. Neuron. 2021. 109(4):571-5. https://www.sciencedirect.com/science/article/pii/S089662732100009X\r\n- How to train modern spiking networks: Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based \r\noptimization to spiking neural networks. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\r\n- Rate-based SNNs: Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with neuromorphic computing. Nature. 2019.', 'Different encoding strategies with spiking neurons\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\nQuestions?\r\n\r\nNeuromorphic computing\r\nWhat is the advantage for applications?\r\n-\r\nLess spikes = less energy consumption in specialized neuromorphic hardware\r\n(e.g. Intel Loihi [12])\r\nBrains are energy efficient: \r\n2. Sparse encoding\r\n\r\nMultiply-accumulate (MAC) operations:\r\nNormal neuron: Multiplies input with \r\nweights, then adds.\r\nSpiking neuron: Consider binary input (e.g. input currents are piecewise \r\nconstant and assume values {0,1}). There is no multiplication, only addition.\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\r\nNormal neuron: nin x nout multiplications,\r\n(nin– 1) x nout additions\r\nSpiking neuron: 0 multiplications, (nactive– 1) x nout \r\nadditions, with nactive ≤ nin\r\nEnergy consumption:\r\nEnormal = M nin nout + A (nin– 1) nout = 17.5 pJ\r\nEnergy consumption:\r\nEspiking = A (nactive– 1) nout = 2.7 pJ\r\nMultiply-accumulate (MAC) operations:\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\r\nNormal neuron: nin x nout multiplications,\r\n(nin– 1) x nout additions\r\nSpiking neuron: 0 multiplications, (nactive– 1) x nout \r\nadditions, with nactive ≤ nin\r\nEnergy consumption:\r\nEnormal = M nin nout + A (nin– 1) nout = 17.5 pJ\r\nEnergy consumption:\r\nEspiking = A (nactive– 1) nout = 2.7 pJ\r\nMultiply-accumulate (MAC) operations:\r\nSNN challenge: how to \r\ncompute with the least \r\namount of spikes!\r\n\r\nOften, we observe a \r\nsparsity (energy)-\r\ntask accuracy trade-off\r\n(Left: results for image \r\nclassification)\r\nSNN challenge: how to \r\ncompute with the least \r\namount of spikes!\r\nhttps://arxiv.org/pdf/2409.08290\r\n\r\nIn practice, energy \r\nconsumption computations \r\nare complex. \r\nNeed to take into account\r\n- memory access,\r\n- addressing,\r\n- auxiliary operations,\r\nin addition to MACs.\r\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \r\nand video classification respectively). FNN’s are conventional feed-forward neural networks.\r\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.\r\nhttps://arxiv.org/pdf/2210.13107\r\nComputing energy consumption\r\n\r\nIn practice, energy \r\nconsumption computations \r\nare complex. \r\nNeed to take into account\r\n- memory access,\r\n- addressing,\r\n- auxiliary operations,\r\nin addition to MACs.\r\nEstimated energy consumption for 3 different datasets (CIFAR10, GSC, NCARS; image, sound \r\nand video classification respectively). FNN’s are conventional feed-forward neural networks.\r\nIn this example: SNNs are 6 to 8 times more energy efficient than FNNs.', ""- Higher firing rate (spikes per unit time) for \r\n'selected' feature.\r\nAdapted from Hubel & Wiesel, 1959\r\n\r\nFiring rates\r\nClassical view of the brain:\r\n- Each neuron is selective for one specific \r\nfeature in the input.\r\n- Higher firing rate (spikes per unit time) for \r\n'selected' feature.\r\n- Link to modern ANNs: The scalar output of \r\nan artificial neuron is interpreted as the firing \r\nrate.\r\nAdapted from Hubel & Wiesel, 1959\r\n\r\nFiring rates\r\nClassical view of the brain:\r\n- Each neuron is selective for one specific \r\nfeature in the input.\r\n- Higher firing rate (spikes per unit time) for \r\n'selected' feature.\r\n- Link to modern ANNs: The scalar output of \r\nan artificial neuron is interpreted as the firing \r\nrate.\r\nAdapted from Hubel & Wiesel, 1959\r\nBut rate coding is inefficient and slow…\r\n(i.e. each neuron needs to fire many spikes to get good precision)\r\nboth in vivo and in silico.\r\n\r\nDifferent encoding strategies with spiking neurons [8]\r\n7 neurons\r\n7 time bins\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nn\r\n6\r\n(in the general \r\ncase a \r\nnumber \r\nbetween 0-7)\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nn\r\nTotal spike \r\ncount can \r\nencode 3 bits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nIndividual rates \r\ncan encode 7 \r\nbits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nLatency can \r\nencode ~3*7 or \r\n~19 bits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nRank order can \r\nencode ~12 bits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\nQuestions?\r\n\r\nNeuromorphic computing\r\nWhat is the advantage for applications?\r\n-\r\nLess spikes = less energy consumption in specialized neuromorphic hardware\r\n(e.g. Intel Loihi [12])\r\nBrains are energy efficient: \r\n2. Sparse encoding\r\n\r\nMultiply-accumulate (MAC) operations:\r\nNormal neuron: Multiplies input with \r\nweights, then adds.\r\nSpiking neuron: Consider binary input (e.g. input currents are piecewise \r\nconstant and assume values {0,1}). There is no multiplication, only addition.\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\r\nNormal neuron: nin x nout multiplications,\r\n(nin– 1) x nout additions\r\nSpiking neuron: 0 multiplications, (nactive– 1) x nout \r\nadditions, with nactive ≤ nin\r\nEnergy consumption:\r\nEnormal = M nin nout + A (nin– 1) nout = 17.5 pJ\r\nEnergy consumption:\r\nEspiking = A (nactive– 1) nout = 2.7 pJ\r\nMultiply-accumulate (MAC) operations:\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).""]","[""<1-hop>\r\n\r\nEncoding strategies\r\nWe considered single neurons with Boolean output (‘spike’=1 or ‘no spike’=0).\r\nHow should we encode information about 'features' in a large network with many spikes?\r\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\r\nElephant?\r\nhttps://www.nationalgeographic.com/anim\r\nals/mammals/facts/african-elephant"", '<2-hop>\r\n\r\nReferences\r\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \r\nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\r\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \r\n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\r\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch–Pitts neural networks. Journal of the History of the Behavioral \r\nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\r\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \r\n(not publicly available) doi:10.1037/h0042519\r\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \r\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\r\n6) https://github.com/idsc-frazzoli/retina\r\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \r\nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\r\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \r\nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\r\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \r\nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\r\n10) Gütig R, Sompolinsky H. The tempotron: a neuron that learns spike timing–based decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \r\nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\r\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \r\nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\r\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\r\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\r\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \r\n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568', ""<1-hop>\r\n\r\nEncoding strategies\r\nWe considered single neurons with Boolean output (‘spike’=1 or ‘no spike’=0).\r\nHow should we encode information about 'features' in a large network with many spikes?\r\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\r\nElephant?\r\nhttps://www.nationalgeographic.com/anim\r\nals/mammals/facts/african-elephant"", '<2-hop>\r\n\r\nReferences\r\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \r\nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\r\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \r\n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\r\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch–Pitts neural networks. Journal of the History of the Behavioral \r\nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\r\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \r\n(not publicly available) doi:10.1037/h0042519\r\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \r\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\r\n6) https://github.com/idsc-frazzoli/retina\r\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \r\nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\r\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \r\nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\r\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \r\nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\r\n10) Gütig R, Sompolinsky H. The tempotron: a neuron that learns spike timing–based decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \r\nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\r\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \r\nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\r\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\r\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\r\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \r\n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568']","The terms ""spiking neural networks"" (SNNs) and ""spiking neuron networks"" are often used interchangeably to refer to the same concept. Both refer to a type of artificial neural network that mimics the way biological neurons communicate using discrete spikes. These networks use spiking neurons, which process information in the form of spikes rather than continuous values, making them more biologically plausible compared to traditional artificial neural networks.

The main focus of SNNs is to leverage the temporal dynamics of spikes for computation, aiming for energy efficiency and potentially more powerful information processing capabilities. The challenge with SNNs is to compute with the least amount of spikes, which can lead to significant energy savings, especially when implemented on specialized neuromorphic hardware. 

In summary, there is no substantial difference between the two terms; they both describe networks that utilize spiking neurons for computation.","Spiking neural networks (SNNs) are a type of neural network model that processes information using spikes, similar to biological neurons. Spiking neuron networks refer to the individual components within these networks, focusing on how each neuron processes and communicates information through spikes.",0.0,1.0,0.6428571428571429,0.33,0.0,0.9825639032194105
What is the foundational concepts of spiking neural networks and how they are used in encoding strategies?,"['575(7784):607-17. \r\nhttps://www.nature.com/articles/s41586-019-1677-2\r\nExtra reading:\r\n- Converging history of deep networks and biological systems: Sejnowski TJ. The unreasonable effectiveness of deep learning in artificial intelligence. Proceedings of the National \r\nAcademy of Sciences. 2020. 117(48):30033-8. https://www.pnas.org/doi/full/10.1073/pnas.1907373117\r\n- Also an important part of neuromorphic systems and vision → Event Cameras: Gallego G, Delbrück T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, \r\nDaniilidis K, Scaramuzza D. Event-based vision: A survey. IEEE transactions on pattern analysis and machine intelligence. 2020. 44(1):154-80. \r\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138762\r\nBasics of \'conventional\' neural networks:\r\n- Sections 4.1 to 4.4 from the book ""Pattern Recognition"" by Theodoridis and Koutroumbas.\r\n- Subsection 4.1.7 from the book ""Pattern Recognition and Machine Learning"" by Bishop.\r\n\r\nReferences\r\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \r\nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\r\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \r\n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\r\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch–Pitts neural networks. Journal of the History of the Behavioral \r\nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\r\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \r\n(not publicly available) doi:10.1037/h0042519\r\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \r\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\r\n6) https://github.com/idsc-frazzoli/retina\r\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \r\nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\r\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \r\nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\r\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \r\nNeural Networks.', ""Spiking neural network (SNN): The 'input current' ε(t) is integrated over time.\r\nspike\r\n\r\nHow to model spiking neurons?\r\nEquivalence to perceptron: Computation at least as complex as a perceptron.\r\n\r\nNon-leaky integrate-and-fire (IF) neuron\r\nThe temporal profile of the input current ε(t) can be chosen differently, for different computations.\r\n\r\nQuestions?\r\n\r\nComputation with spiking neurons\r\nCoincidence detection\r\n\r\nComputation with spiking neurons\r\nCoincidence detection\r\nx, y ∈ {0, 1}n\r\nExample: n = 2\r\ninput = \r\nx1\r\nx2\r\ny1\r\ny2\r\n= \r\n0\r\n1\r\n1\r\n1\r\noutput? \r\n\r\nCoincidence detection\r\nComputation with spiking neurons\r\n\r\nCoincidence detection\r\nCan be trivially computed with a single spiking neuron! Requires at least n/log(n+1) hidden \r\nunits for a perceptron (proof in [7]).\r\nComputation with spiking neurons\r\n\r\nCoincidence detection\r\nCan be trivially computed with a single spiking neuron! Requires at least n/log(n+1) hidden \r\nunits for a perceptron (proof in [7]).\r\nBrains are energy efficient: \r\n1.\r\nHigh temporal resolution (more computation with less neurons)\r\nComputation with spiking neurons\r\n\r\nEncoding strategies\r\nWe considered single neurons with Boolean output (‘spike’=1 or ‘no spike’=0).\r\nHow should we encode information about 'features' in a large network with many spikes?\r\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\r\nElephant?\r\nhttps://www.nationalgeographic.com/anim\r\nals/mammals/facts/african-elephant\r\n\r\nFiring rates\r\nClassical view of the brain:\r\n- Each neuron is selective for one specific \r\nfeature in the input.\r\n- Higher firing rate (spikes per unit time) for \r\n'selected' feature.\r\nAdapted from Hubel & Wiesel, 1959\r\n\r\nFiring rates\r\nClassical view of the brain:\r\n- Each neuron is selective for one specific \r\nfeature in the input.\r\n- Higher firing rate (spikes per unit time) for \r\n'selected' feature.\r\n- Link to modern ANNs: The scalar output of \r\nan artificial neuron is interpreted as the firing \r\nrate.\r\nAdapted from Hubel & Wiesel, 1959\r\n\r\nFiring rates\r\nClassical view of the brain:\r\n- Each neuron is selective for one specific \r\nfeature in the input.\r\n- Higher firing rate (spikes per unit time) for \r\n'selected' feature.\r\n- Link to modern ANNs: The scalar output of \r\nan artificial neuron is interpreted as the firing \r\nrate.\r\nAdapted from Hubel & Wiesel, 1959\r\nBut rate coding is inefficient and slow…\r\n(i.e. each neuron needs to fire many spikes to get good precision)\r\nboth in vivo and in silico.\r\n\r\nDifferent encoding strategies with spiking neurons [8]\r\n7 neurons\r\n7 time bins\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nn\r\n6\r\n(in the general \r\ncase a \r\nnumber \r\nbetween 0-7)\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nn\r\nTotal spike \r\ncount can \r\nencode 3 bits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nIndividual rates \r\ncan encode 7 \r\nbits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nLatency can \r\nencode ~3*7 or \r\n~19 bits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nRank order can \r\nencode ~12 bits."", ""- Higher firing rate (spikes per unit time) for \r\n'selected' feature.\r\nAdapted from Hubel & Wiesel, 1959\r\n\r\nFiring rates\r\nClassical view of the brain:\r\n- Each neuron is selective for one specific \r\nfeature in the input.\r\n- Higher firing rate (spikes per unit time) for \r\n'selected' feature.\r\n- Link to modern ANNs: The scalar output of \r\nan artificial neuron is interpreted as the firing \r\nrate.\r\nAdapted from Hubel & Wiesel, 1959\r\n\r\nFiring rates\r\nClassical view of the brain:\r\n- Each neuron is selective for one specific \r\nfeature in the input.\r\n- Higher firing rate (spikes per unit time) for \r\n'selected' feature.\r\n- Link to modern ANNs: The scalar output of \r\nan artificial neuron is interpreted as the firing \r\nrate.\r\nAdapted from Hubel & Wiesel, 1959\r\nBut rate coding is inefficient and slow…\r\n(i.e. each neuron needs to fire many spikes to get good precision)\r\nboth in vivo and in silico.\r\n\r\nDifferent encoding strategies with spiking neurons [8]\r\n7 neurons\r\n7 time bins\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nn\r\n6\r\n(in the general \r\ncase a \r\nnumber \r\nbetween 0-7)\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nn\r\nTotal spike \r\ncount can \r\nencode 3 bits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nIndividual rates \r\ncan encode 7 \r\nbits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nLatency can \r\nencode ~3*7 or \r\n~19 bits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n7 neurons\r\n7 time bins\r\nRank order can \r\nencode ~12 bits.\r\n\r\nDifferent encoding strategies with spiking neurons\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\n\r\nDifferent encoding strategies with spiking neurons\r\nUsing latency or spatio-temporal codes\r\nour encoding is faster and more spike-efficient (sparser)!\r\nQuestions?\r\n\r\nNeuromorphic computing\r\nWhat is the advantage for applications?\r\n-\r\nLess spikes = less energy consumption in specialized neuromorphic hardware\r\n(e.g. Intel Loihi [12])\r\nBrains are energy efficient: \r\n2. Sparse encoding\r\n\r\nMultiply-accumulate (MAC) operations:\r\nNormal neuron: Multiplies input with \r\nweights, then adds.\r\nSpiking neuron: Consider binary input (e.g. input currents are piecewise \r\nconstant and assume values {0,1}). There is no multiplication, only addition.\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).\r\nNormal neuron: nin x nout multiplications,\r\n(nin– 1) x nout additions\r\nSpiking neuron: 0 multiplications, (nactive– 1) x nout \r\nadditions, with nactive ≤ nin\r\nEnergy consumption:\r\nEnormal = M nin nout + A (nin– 1) nout = 17.5 pJ\r\nEnergy consumption:\r\nEspiking = A (nactive– 1) nout = 2.7 pJ\r\nMultiply-accumulate (MAC) operations:\r\n\r\nAssume one multiplier and one adder circuit uses M and A energy respectively with A﹤M\r\n(e.g., for a 45nm CMOS process, standard energy usage is A = 0.9 pJ and M = 3.7 pJ).""]","[""<1-hop>\r\n\r\nEncoding strategies\r\nWe considered single neurons with Boolean output (‘spike’=1 or ‘no spike’=0).\r\nHow should we encode information about 'features' in a large network with many spikes?\r\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\r\nElephant?\r\nhttps://www.nationalgeographic.com/anim\r\nals/mammals/facts/african-elephant"", '<2-hop>\r\n\r\nReferences\r\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \r\nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\r\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \r\n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\r\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch–Pitts neural networks. Journal of the History of the Behavioral \r\nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\r\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \r\n(not publicly available) doi:10.1037/h0042519\r\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \r\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\r\n6) https://github.com/idsc-frazzoli/retina\r\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \r\nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\r\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \r\nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\r\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \r\nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\r\n10) Gütig R, Sompolinsky H. The tempotron: a neuron that learns spike timing–based decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \r\nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\r\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \r\nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\r\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\r\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\r\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \r\n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568', ""<1-hop>\r\n\r\nEncoding strategies\r\nWe considered single neurons with Boolean output (‘spike’=1 or ‘no spike’=0).\r\nHow should we encode information about 'features' in a large network with many spikes?\r\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\r\nElephant?\r\nhttps://www.nationalgeographic.com/anim\r\nals/mammals/facts/african-elephant"", '<2-hop>\r\n\r\nReferences\r\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \r\nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\r\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \r\n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\r\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch–Pitts neural networks. Journal of the History of the Behavioral \r\nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\r\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \r\n(not publicly available) doi:10.1037/h0042519\r\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \r\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\r\n6) https://github.com/idsc-frazzoli/retina\r\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \r\nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\r\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \r\nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\r\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \r\nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\r\n10) Gütig R, Sompolinsky H. The tempotron: a neuron that learns spike timing–based decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \r\nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\r\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \r\nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\r\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\r\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\r\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \r\n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568', ""<1-hop>\r\n\r\nEncoding strategies\r\nWe considered single neurons with Boolean output (‘spike’=1 or ‘no spike’=0).\r\nHow should we encode information about 'features' in a large network with many spikes?\r\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\r\nElephant?\r\nhttps://www.nationalgeographic.com/anim\r\nals/mammals/facts/african-elephant"", '<2-hop>\r\n\r\nReferences\r\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \r\nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\r\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \r\n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\r\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch–Pitts neural networks. Journal of the History of the Behavioral \r\nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\r\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \r\n(not publicly available) doi:10.1037/h0042519\r\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \r\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\r\n6) https://github.com/idsc-frazzoli/retina\r\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \r\nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\r\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \r\nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\r\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \r\nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\r\n10) Gütig R, Sompolinsky H. The tempotron: a neuron that learns spike timing–based decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \r\nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\r\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \r\nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\r\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\r\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\r\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \r\n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568', ""<1-hop>\r\n\r\nEncoding strategies\r\nWe considered single neurons with Boolean output (‘spike’=1 or ‘no spike’=0).\r\nHow should we encode information about 'features' in a large network with many spikes?\r\nhttps://analyticsindiamag.com/a-tutorial-on-spiking-neural-networks-for-beginners/\r\nElephant?\r\nhttps://www.nationalgeographic.com/anim\r\nals/mammals/facts/african-elephant"", '<2-hop>\r\n\r\nReferences\r\n1) McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics. 1943. 5(4):115-33. \r\nhttps://link.springer.com/content/pdf/10.1007/BF02478259.pdf\r\n2) Pitts W, McCulloch WS. How we know universals the perception of auditory and visual forms. The Bulletin of Mathematical Biophysics. 1947. \r\n9(3):127-47. https://link.springer.com/content/pdf/10.1007/BF02478291.pdf\r\n3) Abraham TH. (Physio) logical circuits: The intellectual origins of the McCulloch–Pitts neural networks. Journal of the History of the Behavioral \r\nSciences. 2002. 38(1):3-25. https://onlinelibrary.wiley.com/doi/pdf/10.1002/jhbs.1094\r\n4) Rosenblatt F. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review. 1958. 65(6):386. \r\n(not publicly available) doi:10.1037/h0042519\r\n5) Tappert CC. Who is the father of deep learning? International Conference on Computational Science and Computational Intelligence (CSCI) 2019. \r\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070967\r\n6) https://github.com/idsc-frazzoli/retina\r\n7) Maass W. Networks of spiking neurons: the third generation of neural network models. Neural networks. 1997. 10(9):1659-71. \r\nhttps://igi-web.tugraz.at/people/maass/psfiles/85a.pdf\r\n8) Paugam-Moisy H, Bohte SM, ""Computing with spiking neuron networks."" in Handbook of Natural Computing (2012). \r\nhttps://homepages.cwi.nl/~sbohte/publication/paugam_moisy_bohte_SNNChapter.pdf\r\n9) Kheradpisheh SR, Ganjtabesh M, Thorpe SJ, Masquelier T. STDP-based spiking deep convolutional neural networks for object recognition. \r\nNeural Networks. 2018 Mar 1;99:56-67. https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0893608017302903\r\n10) Gütig R, Sompolinsky H. The tempotron: a neuron that learns spike timing–based decisions. Nature neuroscience. 2006 Mar;9(3):420-8. \r\nhttp://mcn2016public.pbworks.com/w/file/fetch/137818197/Gutig_R_The%20tempotron_Nature%20Neuroscience.pdf\r\n11) Neftci EO, Mostafa H, Zenke F. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to \r\nSNNs. IEEE Signal Processing Magazine. 2019. 36(6):51-63. https://ieeexplore.ieee.org/abstract/document/8891809\r\n12) https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html\r\n13) https://neuronaldynamics.epfl.ch/online/Ch1.S3.html\r\n14) Kron G. Numerical solution of ordinary and partial differential equations by means of equivalent circuits. Journal of Applied Physics. 1945. \r\n16(3):172-86. https://aip.scitation.org/doi/abs/10.1063/1.1707568']","Spiking Neural Networks (SNNs) are a type of artificial neural network that more closely mimic the way biological brains operate. The foundational concept of SNNs is the use of spiking neurons, which communicate by sending discrete spikes (or pulses) rather than continuous signals. This approach allows for more biologically plausible models of neural computation.

In SNNs, the 'input current' is integrated over time, and neurons fire (or spike) when a certain threshold is reached. This spiking mechanism can be used for various computations, such as coincidence detection, which can be computed with a single spiking neuron but requires more resources in traditional perceptrons.

Encoding strategies in SNNs involve different methods to represent information using spikes. Some of these strategies include:

1. **Firing Rates**: The classical view where each neuron is selective for a specific feature, and the firing rate (spikes per unit time) indicates the presence of that feature. However, this method is considered inefficient and slow.

2. **Spike Count**: The total number of spikes in a given time can encode information, such as 3 bits in a specific setup.

3. **Latency Coding**: The timing of the first spike can encode information, which is faster and more efficient than rate coding. For example, latency can encode approximately 19 bits in a given setup.

4. **Rank Order Coding**: The order in which neurons fire can encode information, potentially encoding around 12 bits.

These encoding strategies allow SNNs to be more energy-efficient, as they require fewer spikes to transmit information, which is beneficial for applications in neuromorphic computing. This efficiency is due to the reduced need for multiply-accumulate operations, which are energy-intensive in traditional neural networks.","Spiking neural networks use spikes to encode information, with foundational concepts including the work of McCulloch and Pitts on logical calculus of nervous activity, and later developments in spiking neuron models.",0.9999999999,1.0,1.0,0.35,,0.9065174802348884
What is the project idea about government-developed OS software?,"['It’s more important that we learn how to \r\ndiscuss this topic and come up with new ideas than learning all the theory. \r\nCritical thinking over checkboxes. \r\n• Mix of content and discussion \r\n• Ultimately, the lectures aim to give you food for thought and the necessary \r\nknowledge to excel in Project 2. (We will talk about it later)\r\n30\r\n\r\nhttps://luiscruz.github.io/course_sustainableSE/\r\n31\r\nContent of the course 👇\r\n\r\n32\r\nhttps://mattermost.tudelft.nl/signup_user_complete/?id=1nj9tk6usjf8xmsws8wpq3s5uy&md=link&sbr=su\r\n\r\nProject 1\r\n• Goal: Measure the energy consumption of software applications. \r\n• Approach: energy measurement tools; use case testing. \r\n• Deliverable: blog-style report (approx. 2500 words) \r\n• Deadline: Week 3, Feb 28, 2025 \r\n• Group size: 4–5\r\n33\r\n\r\nProject 2\r\n• Goal: Create a solution/tool/technique that helps building green software. (You \r\ncan come up with your own idea or choose one from a list of suggestions). \r\n• Approach: open-source software development; literature review. \r\n• Deliverable: library/tool/app; paper; presentation. \r\n• Two deadlines:\u2028\r\n1. Paper and software: Week 8, April 4, 2025\u2028\r\n2. Presentation: Week 9, April 11, 2025 \r\n• Group size: 4–5\r\n34\r\n\r\nGuest Lectures\r\nNergis Tömen \r\nComputer Vision lab | TU Delft \r\nWeek 4, Wed, Mar 5, 2025\r\n35\r\n\r\nCommunity \r\nHow to get involved?\r\n36\r\n\r\nGreen TU\r\n• https://www.tudelft.nl/sustainability/get-\r\ninvolved/greentu \r\n• Student organisation at the TU Delft devoted \r\nto stimulating sustainability in education, \r\nresearch, university operations and \r\ncommunity engagement.\r\n37\r\n\r\nClimateAction.tech\r\n• Great community for outreach \r\n• Based on Slack  \r\n• Regular meetings, talks, social events \r\n• You can join as a volunteer or simply to \r\nconnect to other techies \r\n• Also good to for job hunting on green tech.\r\n38\r\n\r\nClimateAction.tech\r\n39\r\n\r\nBranch magazine\r\n• Stay up-to-date on sustainable tech \r\n• Creativity booster \r\n• Carbon-aware UI \r\n• https://branch.climateaction.tech 🔗\r\n40\r\n\r\nThis is the fourth edition\r\n• Any feedback is welcome! Email or DM!\r\n41\r\n\r\n42\r\n\r\nCarolin Brandt \r\nC.E.Brandt@tudelft.nl \r\nLuís Cruz \r\nL.Cruz@tudelft.nl \r\n1. Intro Class\r\nSustainable Software Engineering\u2028\r\nCS4575\r\nSustainableSE 2025\r\nEnrique Barba Roque \r\nE.BarbaRoque@tudelft.nl', 'Carbon-free giants\r\n• Google, Microsoft, Meta/Facebook want to be carbon free by 2030 \r\n• Carbon free is different from carbon neutral \r\n• Green IT experts are needed to meet these goals\r\n23\r\n\r\n\r\nRebound effect*\r\n• Energy consumption decreases ↓↓ but demand also increases ↑↑. \r\n• *a.k.a. Jevons Paradox in economics; Downs–Thomson paradox in\xa0mobility.\r\nEnergy per prompt\r\nPrompts\r\n100\r\n80\r\n30\r\n38\r\nChat \r\nGPT\r\n“Energy-efficient” \r\nChat GPT\r\n0\r\n\r\nIs sustainability an\u2028\r\nethical issue?\r\n• Climate change is more likely to affect the \r\npoorest countries. \r\n• Less financial resources to adapt \r\n• Climate-impact does not necessarily affect \r\npolluting countries. \r\n• Poorest countries have contributed less to the \r\nclimate change. \r\n• We need to figure out how to do more using \r\nless resources.\r\n26\r\n\r\nMorality ≠ Moralising\r\n• We should not use climate action as a \r\nshaming weapon \r\n• Climate action should be agnostic of political \r\nviews, ideology, social status, etc. \r\n• We need everyone to take action!\r\n27\r\n\r\nWhy?\r\n• Throughout your career you might: \r\n• Design/maintain/contract data centers \r\n• Set up operations/devops \r\n• Develop AI for IoT devices \r\n• Be the next CEO/CTO of a software company \r\n• Sustainability can be your main role: \r\n• Green Software Developer \r\n• Sustainability Consultant \r\n• Green Advocate  \r\n• Founder of a Green Tech startup (B2B?)\r\n28\r\n\r\nFormat of classes\r\n• In-person. \r\n• Collegerama recordings. \r\n• Lectures and Labs. \r\n• Guest lectures. \r\n• Steering meetings (after week 5, new schedule)\r\n29\r\n\r\nFormat of classes\r\n• There’s no exam in this course. It’s more important that we learn how to \r\ndiscuss this topic and come up with new ideas than learning all the theory. \r\nCritical thinking over checkboxes. \r\n• Mix of content and discussion \r\n• Ultimately, the lectures aim to give you food for thought and the necessary \r\nknowledge to excel in Project 2. (We will talk about it later)\r\n30\r\n\r\nhttps://luiscruz.github.io/course_sustainableSE/\r\n31\r\nContent of the course 👇\r\n\r\n32\r\nhttps://mattermost.tudelft.nl/signup_user_complete/?id=1nj9tk6usjf8xmsws8wpq3s5uy&md=link&sbr=su\r\n\r\nProject 1\r\n• Goal: Measure the energy consumption of software applications. \r\n• Approach: energy measurement tools; use case testing. \r\n• Deliverable: blog-style report (approx. 2500 words) \r\n• Deadline: Week 3, Feb 28, 2025 \r\n• Group size: 4–5\r\n33\r\n\r\nProject 2\r\n• Goal: Create a solution/tool/technique that helps building green software. (You \r\ncan come up with your own idea or choose one from a list of suggestions). \r\n• Approach: open-source software development; literature review. \r\n• Deliverable: library/tool/app; paper; presentation. \r\n• Two deadlines:\u2028\r\n1. Paper and software: Week 8, April 4, 2025\u2028\r\n2. Presentation: Week 9, April 11, 2025 \r\n• Group size: 4–5\r\n34\r\n\r\nGuest Lectures\r\nNergis Tömen \r\nComputer Vision lab | TU Delft \r\nWeek 4, Wed, Mar 5, 2025\r\n35\r\n\r\nCommunity \r\nHow to get involved?', 'Luís Cruz \r\nL.Cruz@tudelft.nl \r\n10. Project 2\r\nSustainable Software Engineering\u2028\r\nCS4575\r\nSustainableSE 2025\r\nCarolin Brandt \r\nC.E.Brandt@tudelft.nl \r\nEnrique Barba Roque \r\nE.BarbaRoque@tudelft.nl\r\n\r\n1. Goal/assignment \r\n2. Deliverables \r\n3. Strategy \r\n4. Ideas\r\n\r\nAssignment\r\n• Goal: Solve a Sustainable Software Engineering problem. \r\n• Identify 1 problem that should be fixed to help enabling sustainability in \r\nthe software engineering industry/community. \r\n• Propose a solution. A tool, framework, guidelines, etc. \r\n• Implementation. \r\n• Validation. (Depending on the idea) \r\n• Dissemination/social impact. (Solution should be open source, welcome \r\ncontributors, post on social media? Tool website? Available in a package \r\nmanager?)\r\n3\r\n\r\nDeliverables\r\n• Paper-like article. (Min 4 pages, max 10) \r\n• Online git repo with open source codebase and/or replication package. \r\n• Presentation: 5 min + 5min Q&A\r\n4\r\n\r\nArticle\r\n• Different projects will have different expectations -> Make agreements with \r\nyour coach. \r\n• Some projects are more technical and some projects more theoretical. \r\n• Common requirements: \r\n• Motivation, formulation of the problem being addressed, etc. \r\n• Description of the solution. \r\n• Validation of the solution (if applicable -> discuss with coach) \r\n• Discussion of the solution. (Including limitations)\r\n5\r\n\r\nStrategy\r\n• Starting next week, there are no lectures  \r\n• Steering meetings from week 5 till week 9 (either online or in person).  \r\n• 1 steering meeting per week. (4+1 sprints) \r\n• Every week, you need to plan different tasks and assignments. \r\n• Deadline April 4.\r\n6\r\n\r\nStrategy\r\n• Week 0 \r\n• Decide project idea (today) \r\n• Define steering meeting schedule \r\n• Create working document of the \r\narticle: Problem statement and solution \r\nproposal! \r\n• Define and assign tasks for each week. \r\n• Week 1 \r\n• Implementation \r\n• Agreements with supervisor. \r\n• Week 2 \r\n• Implementation \r\n• Week 3 \r\n• Implementation, Full draft of article, \r\ndissemination. \r\n• Week 4 \r\n• Final refinements \r\n• Prepare presentation\r\n7\r\n\r\nProject ideas\r\n• A1. Prototype cross-machine comparable benchmarks \r\n• A2. Add energy metrics to LMstudio/Ollama \r\n• A3. Visualizations built-in with EnergiBridge \r\n• A4. Service-based version of EnergiBridge\r\n\r\nProject ideas\r\n• B1. Measure energy consumption of single JUnit tests \r\n• B2. Study test generation energy consumption \r\n• B3. Energy profiling of static analysis tools \r\n• B4. Detailed energy profiling of build pipelines \r\n• B5. Tool supporting SusAF workshop / process \r\n• B6. Dataset of government-developed OS software \r\n• B7. Queue - but better for the student / TA society\r\nQuality Assurance & Testing\r\nSocial & Individual \u2028\r\nSustainability\r\n\r\nProject ideas\r\n• C1. Compare energy consumption of docker images for ML workloads. \r\n• C2. Create a plugin to visualize Hugging Face carbon emissions in detail. \r\n• C3. Plugin for ChatGPT (footprint per chat window) \r\n• C4. NutriScore for software libraries. \r\n• C5. Add energy-awareness to existing software \r\n• C6. Green Shift Left \r\n• C7. Education for Sustainable SE\r\n\r\nA1.']","['<1-hop>\r\n\r\nB6. Dataset of government-developed OS software\r\n• Governments develop & use software for supporting society\u2028\r\nOpen-source development & policies are on the rise\r\n• But studying government software is diﬃcult b/c we don’t know what is out \r\nthere\u2028\r\n→ Lack of incentive to make popular \u2028\r\n→ Language barriers\r\n• Create a comprehensive dataset, incl. data to understand state of software: \r\nbuildable?, open dev. history?, requirements documentation?\r\n• Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\r\n\r\nProject ideas\r\n• B1. Measure energy consumption of single JUnit tests \r\n• B2. Study test generation energy consumption\r\n• B3. Energy proﬁling of static analysis tools\r\n• B4. Detailed energy proﬁling of build pipelines\r\n• B5. Tool supporting SusAF workshop / process\r\n• B6. Dataset of government-developed OS software\r\n• B7. Queue - but better for the student / TA society\r\nQuality Assurance & Testing\r\nSocial & Individual \u2028\r\nSustainability']",The project idea related to government-developed OS software is to create a dataset of such software. This falls under the category of Social & Individual Sustainability.,"The project idea involves creating a comprehensive dataset of government-developed OS software to understand its state, including buildability, open development history, and requirements documentation. This addresses challenges like lack of incentives and language barriers.",0.3333333333,0.5,0.6666666666666666,0.4,0.0,0.9655522568718524
What is the project idea related to creating a dataset of goverment-developed OS software?,"['Luís Cruz \r\nL.Cruz@tudelft.nl \r\n10. Project 2\r\nSustainable Software Engineering\u2028\r\nCS4575\r\nSustainableSE 2025\r\nCarolin Brandt \r\nC.E.Brandt@tudelft.nl \r\nEnrique Barba Roque \r\nE.BarbaRoque@tudelft.nl\r\n\r\n1. Goal/assignment \r\n2. Deliverables \r\n3. Strategy \r\n4. Ideas\r\n\r\nAssignment\r\n• Goal: Solve a Sustainable Software Engineering problem. \r\n• Identify 1 problem that should be fixed to help enabling sustainability in \r\nthe software engineering industry/community. \r\n• Propose a solution. A tool, framework, guidelines, etc. \r\n• Implementation. \r\n• Validation. (Depending on the idea) \r\n• Dissemination/social impact. (Solution should be open source, welcome \r\ncontributors, post on social media? Tool website? Available in a package \r\nmanager?)\r\n3\r\n\r\nDeliverables\r\n• Paper-like article. (Min 4 pages, max 10) \r\n• Online git repo with open source codebase and/or replication package. \r\n• Presentation: 5 min + 5min Q&A\r\n4\r\n\r\nArticle\r\n• Different projects will have different expectations -> Make agreements with \r\nyour coach. \r\n• Some projects are more technical and some projects more theoretical. \r\n• Common requirements: \r\n• Motivation, formulation of the problem being addressed, etc. \r\n• Description of the solution. \r\n• Validation of the solution (if applicable -> discuss with coach) \r\n• Discussion of the solution. (Including limitations)\r\n5\r\n\r\nStrategy\r\n• Starting next week, there are no lectures  \r\n• Steering meetings from week 5 till week 9 (either online or in person).  \r\n• 1 steering meeting per week. (4+1 sprints) \r\n• Every week, you need to plan different tasks and assignments. \r\n• Deadline April 4.\r\n6\r\n\r\nStrategy\r\n• Week 0 \r\n• Decide project idea (today) \r\n• Define steering meeting schedule \r\n• Create working document of the \r\narticle: Problem statement and solution \r\nproposal! \r\n• Define and assign tasks for each week. \r\n• Week 1 \r\n• Implementation \r\n• Agreements with supervisor. \r\n• Week 2 \r\n• Implementation \r\n• Week 3 \r\n• Implementation, Full draft of article, \r\ndissemination. \r\n• Week 4 \r\n• Final refinements \r\n• Prepare presentation\r\n7\r\n\r\nProject ideas\r\n• A1. Prototype cross-machine comparable benchmarks \r\n• A2. Add energy metrics to LMstudio/Ollama \r\n• A3. Visualizations built-in with EnergiBridge \r\n• A4. Service-based version of EnergiBridge\r\n\r\nProject ideas\r\n• B1. Measure energy consumption of single JUnit tests \r\n• B2. Study test generation energy consumption \r\n• B3. Energy profiling of static analysis tools \r\n• B4. Detailed energy profiling of build pipelines \r\n• B5. Tool supporting SusAF workshop / process \r\n• B6. Dataset of government-developed OS software \r\n• B7. Queue - but better for the student / TA society\r\nQuality Assurance & Testing\r\nSocial & Individual \u2028\r\nSustainability\r\n\r\nProject ideas\r\n• C1. Compare energy consumption of docker images for ML workloads. \r\n• C2. Create a plugin to visualize Hugging Face carbon emissions in detail. \r\n• C3. Plugin for ChatGPT (footprint per chat window) \r\n• C4. NutriScore for software libraries. \r\n• C5. Add energy-awareness to existing software \r\n• C6. Green Shift Left \r\n• C7. Education for Sustainable SE\r\n\r\nA1.', '• Deadline April 4.\r\n6\r\n\r\nStrategy\r\n• Week 0 \r\n• Decide project idea (today) \r\n• Define steering meeting schedule \r\n• Create working document of the \r\narticle: Problem statement and solution \r\nproposal! \r\n• Define and assign tasks for each week. \r\n• Week 1 \r\n• Implementation \r\n• Agreements with supervisor. \r\n• Week 2 \r\n• Implementation \r\n• Week 3 \r\n• Implementation, Full draft of article, \r\ndissemination. \r\n• Week 4 \r\n• Final refinements \r\n• Prepare presentation\r\n7\r\n\r\nProject ideas\r\n• A1. Prototype cross-machine comparable benchmarks \r\n• A2. Add energy metrics to LMstudio/Ollama \r\n• A3. Visualizations built-in with EnergiBridge \r\n• A4. Service-based version of EnergiBridge\r\n\r\nProject ideas\r\n• B1. Measure energy consumption of single JUnit tests \r\n• B2. Study test generation energy consumption \r\n• B3. Energy profiling of static analysis tools \r\n• B4. Detailed energy profiling of build pipelines \r\n• B5. Tool supporting SusAF workshop / process \r\n• B6. Dataset of government-developed OS software \r\n• B7. Queue - but better for the student / TA society\r\nQuality Assurance & Testing\r\nSocial & Individual \u2028\r\nSustainability\r\n\r\nProject ideas\r\n• C1. Compare energy consumption of docker images for ML workloads. \r\n• C2. Create a plugin to visualize Hugging Face carbon emissions in detail. \r\n• C3. Plugin for ChatGPT (footprint per chat window) \r\n• C4. NutriScore for software libraries. \r\n• C5. Add energy-awareness to existing software \r\n• C6. Green Shift Left \r\n• C7. Education for Sustainable SE\r\n\r\nA1. Prototype cross-machine comparable benchmarks\r\n• Energy-usage comparisons require running both baseline + software on the \r\nsame machine → limits how extensive our experiments can be \r\n• Research community is in need of benchmarks that make energy \r\nmeasurements comparable even if executed on different machines \r\n• Focus on a single task or model (i.e. computing vision, classification)\r\n\r\nA2. Adding energy metrics to LMstudio/Ollama\r\n• Make energy consumption visible to users \r\nwithin local chat-interface for LLMs \r\n• LMstudio/Ollama are tools for easy \r\ndeployment of LLMs \r\n• Do not show energy metrics \r\n• Add energy metrics to LMstudio-python \r\nor Ollama\r\nhttps://lmstudio.ai/\r\n\r\nA3. Visualizations built-in with EnergiBridge\r\n• EnergiBridge simplifies energy measurement, but analysis & visualization is \r\nstill left to the user \r\n• Extend the tool with well-chosen visualizations and analyses directly from the \r\nprovided data\r\n\r\nA4. Service-based version of EnergiBridge\r\n• Simplify interaction & setup with EnergiBridge \r\n• Service that runs independently, start/stop signals over RPC to manage \r\nexperiments \r\n• Potential: create EnergiBridge interface for other prog. lang\r\n\r\nB1. Measure energy consumption of single JUnit tests\r\n• We’d like to identify energy anti patterns in unit tests \r\n• As a first step, we need tooling to measure and compare the energy \r\nconsumption of single unit tests \r\n• Ideally including preliminary analysis looking at potential reasons for high-\r\nenergy-consuming tests\r\n\r\nB2. Study test generation energy consumption\r\n• Automatic test generation mainly focuses on making strong test suites \r\n• Do different techniques and configurations impact the energy consumption \r\nduring generation? \r\n• Preferably focus on non-LLM test generation methods (EvoSuite, Pyguin, \r\nDSpot)\r\n\r\nB3. Detailed energy profiling of build pipelines\r\n• Automatic builds have become a cornerstone of quality assurance.', 'computing vision, classification)\r\n\r\nA2. Adding energy metrics to LMstudio/Ollama\r\n• Make energy consumption visible to users \r\nwithin local chat-interface for LLMs \r\n• LMstudio/Ollama are tools for easy \r\ndeployment of LLMs \r\n• Do not show energy metrics \r\n• Add energy metrics to LMstudio-python \r\nor Ollama\r\nhttps://lmstudio.ai/\r\n\r\nA3. Visualizations built-in with EnergiBridge\r\n• EnergiBridge simplifies energy measurement, but analysis & visualization is \r\nstill left to the user \r\n• Extend the tool with well-chosen visualizations and analyses directly from the \r\nprovided data\r\n\r\nA4. Service-based version of EnergiBridge\r\n• Simplify interaction & setup with EnergiBridge \r\n• Service that runs independently, start/stop signals over RPC to manage \r\nexperiments \r\n• Potential: create EnergiBridge interface for other prog. lang\r\n\r\nB1. Measure energy consumption of single JUnit tests\r\n• We’d like to identify energy anti patterns in unit tests \r\n• As a first step, we need tooling to measure and compare the energy \r\nconsumption of single unit tests \r\n• Ideally including preliminary analysis looking at potential reasons for high-\r\nenergy-consuming tests\r\n\r\nB2. Study test generation energy consumption\r\n• Automatic test generation mainly focuses on making strong test suites \r\n• Do different techniques and configurations impact the energy consumption \r\nduring generation? \r\n• Preferably focus on non-LLM test generation methods (EvoSuite, Pyguin, \r\nDSpot)\r\n\r\nB3. Detailed energy profiling of build pipelines\r\n• Automatic builds have become a cornerstone of quality assurance. But how \r\nmuch energy do they even consume? \r\n• Create a tool that reports on the energy consumed during the (different stages \r\nof the) whole build (compile, build, test, package, …) \r\n• Should be integrated with build system(s), making setup for developers easy \r\n• For local setup (to enable true energy measurements)\r\n\r\nB4. Energy profiling of static analysis tools\r\n• What is the energy consumption of a ""typical run"" for a few OSS projects? \r\n• Does the type of analysis matter? Are certain analysis more expensive? Does \r\nthe number of rules that are activated in a static analysis tool important for the \r\nenergy consumption? \r\n• Differences between static analysis tools [lower priority]\r\n\r\nB5. Tool supporting SusAF workshop / process\r\n• Lead engineers through process & questions \r\n• Interface to create & document the two diagrams \r\n• (!) Should be easy to start using & set up \r\n• You may also create a simplified version / \u2028\r\nyour favorite sustainability framework\r\nEnvironmental\r\nEconomic\r\nTechnical\r\nIndividual\r\nSocial\r\nImmediate\r\nEnabling\r\nStructural\r\nAirbnb\r\ngreater \r\nearnings\r\nincrease \r\nin rents\r\ngentrification\r\ngreater racial \r\ndisparities\r\nrent \r\nrooms\r\n5\r\n1\r\n3\r\n4\r\n2\r\n \r\n \r\n \r\n \r\n \r\n \r\nVery \r\nunlikely \r\nVery \r\nlikely \r\n\r\nB6. Dataset of government-developed OS software\r\n• Governments develop & use software for supporting society\u2028\r\nOpen-source development & policies are on the rise \r\n• But studying government software is difficult b/c we don’t know what is out \r\nthere\u2028\r\n→ Lack of incentive to make popular \u2028\r\n→ Language barriers \r\n• Create a comprehensive dataset, incl. data to understand state of software: \r\nbuildable?, open dev. history?, requirements documentation? \r\n• Could start with NL, but including your / other countries greatly appreciated!\r\n\r\nB7. Queue - but better for the student / TA society\r\n• Requirements analysis regarding social and individual sustainability effects \r\nof Queue \r\n• Other EIP / TUD used software also possible: e.g., Answers EWI \r\n• Non-technical project → proper process (workshops?']","['<1-hop>\r\n\r\nB6. Dataset of government-developed OS software\r\n• Governments develop & use software for supporting society\u2028\r\nOpen-source development & policies are on the rise\r\n• But studying government software is diﬃcult b/c we don’t know what is out \r\nthere\u2028\r\n→ Lack of incentive to make popular \u2028\r\n→ Language barriers\r\n• Create a comprehensive dataset, incl. data to understand state of software: \r\nbuildable?, open dev. history?, requirements documentation?\r\n• Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\r\n\r\nProject ideas\r\n• B1. Measure energy consumption of single JUnit tests \r\n• B2. Study test generation energy consumption\r\n• B3. Energy proﬁling of static analysis tools\r\n• B4. Detailed energy proﬁling of build pipelines\r\n• B5. Tool supporting SusAF workshop / process\r\n• B6. Dataset of government-developed OS software\r\n• B7. Queue - but better for the student / TA society\r\nQuality Assurance & Testing\r\nSocial & Individual \u2028\r\nSustainability', '<1-hop>\r\n\r\nB6. Dataset of government-developed OS software\r\n• Governments develop & use software for supporting society\u2028\r\nOpen-source development & policies are on the rise\r\n• But studying government software is diﬃcult b/c we don’t know what is out \r\nthere\u2028\r\n→ Lack of incentive to make popular \u2028\r\n→ Language barriers\r\n• Create a comprehensive dataset, incl. data to understand state of software: \r\nbuildable?, open dev. history?, requirements documentation?\r\n• Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\r\n\r\nProject ideas\r\n• B1. Measure energy consumption of single JUnit tests \r\n• B2. Study test generation energy consumption\r\n• B3. Energy proﬁling of static analysis tools\r\n• B4. Detailed energy proﬁling of build pipelines\r\n• B5. Tool supporting SusAF workshop / process\r\n• B6. Dataset of government-developed OS software\r\n• B7. Queue - but better for the student / TA society\r\nQuality Assurance & Testing\r\nSocial & Individual \u2028\r\nSustainability', '<1-hop>\r\n\r\nB6. Dataset of government-developed OS software\r\n• Governments develop & use software for supporting society\u2028\r\nOpen-source development & policies are on the rise\r\n• But studying government software is diﬃcult b/c we don’t know what is out \r\nthere\u2028\r\n→ Lack of incentive to make popular \u2028\r\n→ Language barriers\r\n• Create a comprehensive dataset, incl. data to understand state of software: \r\nbuildable?, open dev. history?, requirements documentation?\r\n• Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\r\n\r\nProject ideas\r\n• B1. Measure energy consumption of single JUnit tests \r\n• B2. Study test generation energy consumption\r\n• B3. Energy proﬁling of static analysis tools\r\n• B4. Detailed energy proﬁling of build pipelines\r\n• B5. Tool supporting SusAF workshop / process\r\n• B6. Dataset of government-developed OS software\r\n• B7. Queue - but better for the student / TA society\r\nQuality Assurance & Testing\r\nSocial & Individual \u2028\r\nSustainability', '<1-hop>\r\n\r\nB6. Dataset of government-developed OS software\r\n• Governments develop & use software for supporting society\u2028\r\nOpen-source development & policies are on the rise\r\n• But studying government software is diﬃcult b/c we don’t know what is out \r\nthere\u2028\r\n→ Lack of incentive to make popular \u2028\r\n→ Language barriers\r\n• Create a comprehensive dataset, incl. data to understand state of software: \r\nbuildable?, open dev. history?, requirements documentation?\r\n• Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\r\n\r\nProject ideas\r\n• B1. Measure energy consumption of single JUnit tests \r\n• B2. Study test generation energy consumption\r\n• B3. Energy proﬁling of static analysis tools\r\n• B4. Detailed energy proﬁling of build pipelines\r\n• B5. Tool supporting SusAF workshop / process\r\n• B6. Dataset of government-developed OS software\r\n• B7. Queue - but better for the student / TA society\r\nQuality Assurance & Testing\r\nSocial & Individual \u2028\r\nSustainability', '<1-hop>\r\n\r\nB6. Dataset of government-developed OS software\r\n• Governments develop & use software for supporting society\u2028\r\nOpen-source development & policies are on the rise\r\n• But studying government software is diﬃcult b/c we don’t know what is out \r\nthere\u2028\r\n→ Lack of incentive to make popular \u2028\r\n→ Language barriers\r\n• Create a comprehensive dataset, incl. data to understand state of software: \r\nbuildable?, open dev. history?, requirements documentation?\r\n• Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\r\n\r\nProject ideas\r\n• B1. Measure energy consumption of single JUnit tests \r\n• B2. Study test generation energy consumption\r\n• B3. Energy proﬁling of static analysis tools\r\n• B4. Detailed energy proﬁling of build pipelines\r\n• B5. Tool supporting SusAF workshop / process\r\n• B6. Dataset of government-developed OS software\r\n• B7. Queue - but better for the student / TA society\r\nQuality Assurance & Testing\r\nSocial & Individual \u2028\r\nSustainability']","The project idea related to creating a dataset of government-developed open-source (OS) software involves compiling a comprehensive dataset that includes data to understand the state of such software. This includes information on whether the software is buildable, has an open development history, and includes requirements documentation. The project acknowledges the challenges in studying government software due to a lack of incentive to make it popular and language barriers. It suggests starting with the Netherlands but encourages including other countries as well.","The project idea is to create a dataset of government-developed OS software, which would help in understanding the state of such software, including whether it is buildable, has open development history, and has requirements documentation.",0.9999999999666668,1.0,1.0,0.67,0.0,0.9728212501702852
What is the project idea related to creating a dataset of government-developed OS software?,"['Luís Cruz \r\nL.Cruz@tudelft.nl \r\n10. Project 2\r\nSustainable Software Engineering\u2028\r\nCS4575\r\nSustainableSE 2025\r\nCarolin Brandt \r\nC.E.Brandt@tudelft.nl \r\nEnrique Barba Roque \r\nE.BarbaRoque@tudelft.nl\r\n\r\n1. Goal/assignment \r\n2. Deliverables \r\n3. Strategy \r\n4. Ideas\r\n\r\nAssignment\r\n• Goal: Solve a Sustainable Software Engineering problem. \r\n• Identify 1 problem that should be fixed to help enabling sustainability in \r\nthe software engineering industry/community. \r\n• Propose a solution. A tool, framework, guidelines, etc. \r\n• Implementation. \r\n• Validation. (Depending on the idea) \r\n• Dissemination/social impact. (Solution should be open source, welcome \r\ncontributors, post on social media? Tool website? Available in a package \r\nmanager?)\r\n3\r\n\r\nDeliverables\r\n• Paper-like article. (Min 4 pages, max 10) \r\n• Online git repo with open source codebase and/or replication package. \r\n• Presentation: 5 min + 5min Q&A\r\n4\r\n\r\nArticle\r\n• Different projects will have different expectations -> Make agreements with \r\nyour coach. \r\n• Some projects are more technical and some projects more theoretical. \r\n• Common requirements: \r\n• Motivation, formulation of the problem being addressed, etc. \r\n• Description of the solution. \r\n• Validation of the solution (if applicable -> discuss with coach) \r\n• Discussion of the solution. (Including limitations)\r\n5\r\n\r\nStrategy\r\n• Starting next week, there are no lectures  \r\n• Steering meetings from week 5 till week 9 (either online or in person).  \r\n• 1 steering meeting per week. (4+1 sprints) \r\n• Every week, you need to plan different tasks and assignments. \r\n• Deadline April 4.\r\n6\r\n\r\nStrategy\r\n• Week 0 \r\n• Decide project idea (today) \r\n• Define steering meeting schedule \r\n• Create working document of the \r\narticle: Problem statement and solution \r\nproposal! \r\n• Define and assign tasks for each week. \r\n• Week 1 \r\n• Implementation \r\n• Agreements with supervisor. \r\n• Week 2 \r\n• Implementation \r\n• Week 3 \r\n• Implementation, Full draft of article, \r\ndissemination. \r\n• Week 4 \r\n• Final refinements \r\n• Prepare presentation\r\n7\r\n\r\nProject ideas\r\n• A1. Prototype cross-machine comparable benchmarks \r\n• A2. Add energy metrics to LMstudio/Ollama \r\n• A3. Visualizations built-in with EnergiBridge \r\n• A4. Service-based version of EnergiBridge\r\n\r\nProject ideas\r\n• B1. Measure energy consumption of single JUnit tests \r\n• B2. Study test generation energy consumption \r\n• B3. Energy profiling of static analysis tools \r\n• B4. Detailed energy profiling of build pipelines \r\n• B5. Tool supporting SusAF workshop / process \r\n• B6. Dataset of government-developed OS software \r\n• B7. Queue - but better for the student / TA society\r\nQuality Assurance & Testing\r\nSocial & Individual \u2028\r\nSustainability\r\n\r\nProject ideas\r\n• C1. Compare energy consumption of docker images for ML workloads. \r\n• C2. Create a plugin to visualize Hugging Face carbon emissions in detail. \r\n• C3. Plugin for ChatGPT (footprint per chat window) \r\n• C4. NutriScore for software libraries. \r\n• C5. Add energy-awareness to existing software \r\n• C6. Green Shift Left \r\n• C7. Education for Sustainable SE\r\n\r\nA1.', '• Deadline April 4.\r\n6\r\n\r\nStrategy\r\n• Week 0 \r\n• Decide project idea (today) \r\n• Define steering meeting schedule \r\n• Create working document of the \r\narticle: Problem statement and solution \r\nproposal! \r\n• Define and assign tasks for each week. \r\n• Week 1 \r\n• Implementation \r\n• Agreements with supervisor. \r\n• Week 2 \r\n• Implementation \r\n• Week 3 \r\n• Implementation, Full draft of article, \r\ndissemination. \r\n• Week 4 \r\n• Final refinements \r\n• Prepare presentation\r\n7\r\n\r\nProject ideas\r\n• A1. Prototype cross-machine comparable benchmarks \r\n• A2. Add energy metrics to LMstudio/Ollama \r\n• A3. Visualizations built-in with EnergiBridge \r\n• A4. Service-based version of EnergiBridge\r\n\r\nProject ideas\r\n• B1. Measure energy consumption of single JUnit tests \r\n• B2. Study test generation energy consumption \r\n• B3. Energy profiling of static analysis tools \r\n• B4. Detailed energy profiling of build pipelines \r\n• B5. Tool supporting SusAF workshop / process \r\n• B6. Dataset of government-developed OS software \r\n• B7. Queue - but better for the student / TA society\r\nQuality Assurance & Testing\r\nSocial & Individual \u2028\r\nSustainability\r\n\r\nProject ideas\r\n• C1. Compare energy consumption of docker images for ML workloads. \r\n• C2. Create a plugin to visualize Hugging Face carbon emissions in detail. \r\n• C3. Plugin for ChatGPT (footprint per chat window) \r\n• C4. NutriScore for software libraries. \r\n• C5. Add energy-awareness to existing software \r\n• C6. Green Shift Left \r\n• C7. Education for Sustainable SE\r\n\r\nA1. Prototype cross-machine comparable benchmarks\r\n• Energy-usage comparisons require running both baseline + software on the \r\nsame machine → limits how extensive our experiments can be \r\n• Research community is in need of benchmarks that make energy \r\nmeasurements comparable even if executed on different machines \r\n• Focus on a single task or model (i.e. computing vision, classification)\r\n\r\nA2. Adding energy metrics to LMstudio/Ollama\r\n• Make energy consumption visible to users \r\nwithin local chat-interface for LLMs \r\n• LMstudio/Ollama are tools for easy \r\ndeployment of LLMs \r\n• Do not show energy metrics \r\n• Add energy metrics to LMstudio-python \r\nor Ollama\r\nhttps://lmstudio.ai/\r\n\r\nA3. Visualizations built-in with EnergiBridge\r\n• EnergiBridge simplifies energy measurement, but analysis & visualization is \r\nstill left to the user \r\n• Extend the tool with well-chosen visualizations and analyses directly from the \r\nprovided data\r\n\r\nA4. Service-based version of EnergiBridge\r\n• Simplify interaction & setup with EnergiBridge \r\n• Service that runs independently, start/stop signals over RPC to manage \r\nexperiments \r\n• Potential: create EnergiBridge interface for other prog. lang\r\n\r\nB1. Measure energy consumption of single JUnit tests\r\n• We’d like to identify energy anti patterns in unit tests \r\n• As a first step, we need tooling to measure and compare the energy \r\nconsumption of single unit tests \r\n• Ideally including preliminary analysis looking at potential reasons for high-\r\nenergy-consuming tests\r\n\r\nB2. Study test generation energy consumption\r\n• Automatic test generation mainly focuses on making strong test suites \r\n• Do different techniques and configurations impact the energy consumption \r\nduring generation? \r\n• Preferably focus on non-LLM test generation methods (EvoSuite, Pyguin, \r\nDSpot)\r\n\r\nB3. Detailed energy profiling of build pipelines\r\n• Automatic builds have become a cornerstone of quality assurance.', 'computing vision, classification)\r\n\r\nA2. Adding energy metrics to LMstudio/Ollama\r\n• Make energy consumption visible to users \r\nwithin local chat-interface for LLMs \r\n• LMstudio/Ollama are tools for easy \r\ndeployment of LLMs \r\n• Do not show energy metrics \r\n• Add energy metrics to LMstudio-python \r\nor Ollama\r\nhttps://lmstudio.ai/\r\n\r\nA3. Visualizations built-in with EnergiBridge\r\n• EnergiBridge simplifies energy measurement, but analysis & visualization is \r\nstill left to the user \r\n• Extend the tool with well-chosen visualizations and analyses directly from the \r\nprovided data\r\n\r\nA4. Service-based version of EnergiBridge\r\n• Simplify interaction & setup with EnergiBridge \r\n• Service that runs independently, start/stop signals over RPC to manage \r\nexperiments \r\n• Potential: create EnergiBridge interface for other prog. lang\r\n\r\nB1. Measure energy consumption of single JUnit tests\r\n• We’d like to identify energy anti patterns in unit tests \r\n• As a first step, we need tooling to measure and compare the energy \r\nconsumption of single unit tests \r\n• Ideally including preliminary analysis looking at potential reasons for high-\r\nenergy-consuming tests\r\n\r\nB2. Study test generation energy consumption\r\n• Automatic test generation mainly focuses on making strong test suites \r\n• Do different techniques and configurations impact the energy consumption \r\nduring generation? \r\n• Preferably focus on non-LLM test generation methods (EvoSuite, Pyguin, \r\nDSpot)\r\n\r\nB3. Detailed energy profiling of build pipelines\r\n• Automatic builds have become a cornerstone of quality assurance. But how \r\nmuch energy do they even consume? \r\n• Create a tool that reports on the energy consumed during the (different stages \r\nof the) whole build (compile, build, test, package, …) \r\n• Should be integrated with build system(s), making setup for developers easy \r\n• For local setup (to enable true energy measurements)\r\n\r\nB4. Energy profiling of static analysis tools\r\n• What is the energy consumption of a ""typical run"" for a few OSS projects? \r\n• Does the type of analysis matter? Are certain analysis more expensive? Does \r\nthe number of rules that are activated in a static analysis tool important for the \r\nenergy consumption? \r\n• Differences between static analysis tools [lower priority]\r\n\r\nB5. Tool supporting SusAF workshop / process\r\n• Lead engineers through process & questions \r\n• Interface to create & document the two diagrams \r\n• (!) Should be easy to start using & set up \r\n• You may also create a simplified version / \u2028\r\nyour favorite sustainability framework\r\nEnvironmental\r\nEconomic\r\nTechnical\r\nIndividual\r\nSocial\r\nImmediate\r\nEnabling\r\nStructural\r\nAirbnb\r\ngreater \r\nearnings\r\nincrease \r\nin rents\r\ngentrification\r\ngreater racial \r\ndisparities\r\nrent \r\nrooms\r\n5\r\n1\r\n3\r\n4\r\n2\r\n \r\n \r\n \r\n \r\n \r\n \r\nVery \r\nunlikely \r\nVery \r\nlikely \r\n\r\nB6. Dataset of government-developed OS software\r\n• Governments develop & use software for supporting society\u2028\r\nOpen-source development & policies are on the rise \r\n• But studying government software is difficult b/c we don’t know what is out \r\nthere\u2028\r\n→ Lack of incentive to make popular \u2028\r\n→ Language barriers \r\n• Create a comprehensive dataset, incl. data to understand state of software: \r\nbuildable?, open dev. history?, requirements documentation? \r\n• Could start with NL, but including your / other countries greatly appreciated!\r\n\r\nB7. Queue - but better for the student / TA society\r\n• Requirements analysis regarding social and individual sustainability effects \r\nof Queue \r\n• Other EIP / TUD used software also possible: e.g., Answers EWI \r\n• Non-technical project → proper process (workshops?']","['<1-hop>\r\n\r\nB6. Dataset of government-developed OS software\r\n• Governments develop & use software for supporting society\u2028\r\nOpen-source development & policies are on the rise\r\n• But studying government software is diﬃcult b/c we don’t know what is out \r\nthere\u2028\r\n→ Lack of incentive to make popular \u2028\r\n→ Language barriers\r\n• Create a comprehensive dataset, incl. data to understand state of software: \r\nbuildable?, open dev. history?, requirements documentation?\r\n• Could start with NL, but including your / other countries greatly appreciated!', '<2-hop>\r\n\r\nProject ideas\r\n• B1. Measure energy consumption of single JUnit tests \r\n• B2. Study test generation energy consumption\r\n• B3. Energy proﬁling of static analysis tools\r\n• B4. Detailed energy proﬁling of build pipelines\r\n• B5. Tool supporting SusAF workshop / process\r\n• B6. Dataset of government-developed OS software\r\n• B7. Queue - but better for the student / TA society\r\nQuality Assurance & Testing\r\nSocial & Individual \u2028\r\nSustainability']","The project idea related to creating a dataset of government-developed OS software involves compiling a comprehensive dataset that includes data to understand the state of such software. This includes information on whether the software is buildable, has an open development history, and includes requirements documentation. The project acknowledges the challenges in studying government software due to a lack of incentive to make it popular and language barriers. It suggests starting with the Netherlands but encourages including other countries as well.","The project idea is to create a comprehensive dataset of government-developed OS software to understand the state of such software, including its buildability, open development history, and requirements documentation. This addresses challenges like the lack of incentive to make software popular and language barriers.",0.9999999999666668,1.0,1.0,0.89,0.0,0.99085318718479
